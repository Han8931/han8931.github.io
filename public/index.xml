<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Han&#39;s XYZ</title>
        <link>http://localhost:1313/</link>
        <description>Han&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 14 Feb 2025 00:00:00 &#43;0000</lastBuildDate>
            <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Understanding DeepSeek Part 1</title>
    <link>http://localhost:1313/20250214_deepseek_componenet/</link>
    <pubDate>Fri, 14 Feb 2025 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20250214_deepseek_componenet/</guid>
    <description><![CDATA[<p><a href="https://www.deepseek.com/" target="_blank" rel="noopener noreffer">DeepSeek</a>
&rsquo;s latest moves have sent ripples through the AI community. Not only has it marked the beginning of a new era in artificial intelligence, but it has also made significant contributions to the open-source AI landscape. Their engineering techniques behind DeepSeek are truly impressive, and their reports are quite enjoyable. However, understanding their core ideas can be challenging and demands a substantial amount of effort.</p>
<p>At the forefront of this innovation is DeepSeek-R1, a model that built upon the foundation established by preceding projects such as DeepSeek Coder, Math, MoE, and notably, the DeepSeek-V3 model. While DeepSeek-R1 is the center of the DeepSeek&rsquo;s frenzy, its success is rooted on these past works.</p>]]></description>
</item>
<item>
    <title>Abstract Classes or Protocols</title>
    <link>http://localhost:1313/20250128_python_protocol_abstract_classes/</link>
    <pubDate>Tue, 28 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20250128_python_protocol_abstract_classes/</guid>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>When it comes to writing clean, maintainable, and scalable Python code, design matters. As your projects grow, you&rsquo;ll often find yourself needing to enforce structure, ensure consistency, and promote reusability. This is where Python&rsquo;s <a href="https://docs.python.org/3/library/abc.html" target="_blank" rel="noopener noreffer">Abstract Base Classes (ABCs)</a>
 and <a href="https://www.python.org/dev/peps/pep-0544/" target="_blank" rel="noopener noreffer">Protocols</a>
 come into play—two powerful features that help you design better software.</p>
<p>Abstract classes act as <strong>blueprints for other classes, allowing you to define methods that must be implemented by any subclass</strong>. They&rsquo;re typically used for creating a shared foundation while enforcing a specific structure. Protocols, on the other hand, take a more flexible approach. Instead of relying on inheritance, they <strong>let you define interfaces based on behavior</strong>, making them ideal for <em>duck typing</em> (or <em>structural subtyping</em>) and runtime flexibility.</p>]]></description>
</item>
<item>
    <title>Run Pytorch Container in Arch Linux</title>
    <link>http://localhost:1313/20250111_pytorch_container/</link>
    <pubDate>Sat, 11 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20250111_pytorch_container/</guid>
    <description><![CDATA[<h1 id="setting-up-dl-experiment-environments">Setting Up DL Experiment Environments</h1>
<h2 id="a-challenge-for-arch-linux-users">A Challenge for Arch Linux Users</h2>
<p>If you&rsquo;ve ever tried to set up a new experiment environment for deep learning on Arch Linux, you&rsquo;re probably familiar with the challenges involved. Arch Linux, renowned for its rolling-release model and cutting-edge updates, provides unparalleled flexibility and control over your system. However, this same flexibility can often lead to headaches when setting up complex environments for machine learning or deep learning experiments. Dependency conflicts, missing libraries, and version mismatches are all too common.</p>]]></description>
</item>
<item>
    <title>Asyncio in Python: A Deep Dive into Asynchronous I/O</title>
    <link>http://localhost:1313/20250105_asyncio/</link>
    <pubDate>Sun, 05 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20250105_asyncio/</guid>
    <description><![CDATA[<p>For the past few months, I&rsquo;ve been working on an exciting internal project at my company: taking users&rsquo; documents and running them through LLM APIs to translate and summarize their content, somewhat similar to <a href="https://www.deepl.com/" target="_blank" rel="noopener noreffer">DeepL</a>
. The output is a collection of translated documents, each overlaid with the newly translated text. Our goal is to provide a stable service that can handle large files efficiently for thousands of employees at Samsung—no small task! To achieve this, we needed a concurrency strategy that supports high throughput while remaining responsive. <strong>That&rsquo;s where Asyncio comes in.</strong></p>]]></description>
</item>
<item>
    <title>Install Arch Linux</title>
    <link>http://localhost:1313/20250105_arch/</link>
    <pubDate>Sun, 05 Jan 2025 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20250105_arch/</guid>
    <description><![CDATA[<p>I recently bought a mini PC because I wanted a lightweight machine that I can easily carry anywhere. Arch Linux&rsquo;s minimalistic, rolling-release approach aligns perfectly with my love for a Vim-based workflow and a highly customizable setup. While the process can seem intimidating at first, it’s an incredibly rewarding experience that offers complete control over your system.</p>
<hr>
<h1 id="installing-arch-linux-uefi-or-bios">Installing Arch Linux (UEFI or BIOS)</h1>
<p>Arch Linux is well-known for giving users full control over their system. This guide walks you through a fresh Arch Linux installation. While it is detailed, always refer to the <a href="https://wiki.archlinux.org/" target="_blank" rel="noopener noreffer">official Arch Wiki</a>
 for up-to-date information.</p>]]></description>
</item>
<item>
    <title>Introduction to SVM Part 3. Asymmetric Kernels</title>
    <link>http://localhost:1313/20240902_svm3/</link>
    <pubDate>Sun, 01 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240902_svm3/</guid>
    <description><![CDATA[<h1 id="introduction-to-asymmetric-kernels">Introduction to Asymmetric Kernels</h1>
<p>Recall that the dual form of LS-SVM is given by
\begin{align*}
\begin{bmatrix}
0 &amp; y^T \\
y &amp; \Omega + \frac{1}{\gamma} I
\end{bmatrix}
\begin{bmatrix}
b \\
\alpha
\end{bmatrix}
=
\begin{bmatrix}
0 \\
e
\end{bmatrix}
\end{align*}
An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way.</p>]]></description>
</item>
<item>
    <title>Introduction to SVM Part 2. LS-SVM</title>
    <link>http://localhost:1313/20240825_svm2/</link>
    <pubDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240825_svm2/</guid>
    <description><![CDATA[<h1 id="introduction-to-least-square-svm">Introduction to Least-Square SVM</h1>
<h2 id="introduction">Introduction</h2>
<p>Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a <em>least squares cost function</em>. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve.</p>
<h3 id="optimization-problem-primal-problem">Optimization Problem (Primal Problem)</h3>
<p>\begin{align*}
&amp;\min_{w, b, e} \frac{1}{2} \lVert w\rVert^2 + \frac{\gamma}{2} \sum_{i=1}^N e_i^2,\\
&amp;\text{subject to } y_i (w^T \phi(x_i) + b) = 1 - e_i, \ \forall i
\end{align*}
where:</p>]]></description>
</item>
<item>
    <title>Introduction to SVM Part 1. Basics</title>
    <link>http://localhost:1313/20240825_svm1/</link>
    <pubDate>Sun, 25 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240825_svm1/</guid>
    <description><![CDATA[<h1 id="support-vector-machine">Support Vector Machine</h1>
<h2 id="introduction">Introduction</h2>
<p>Support Vector Machines (SVMs) are among the most effective and versatile tools in machine learning, widely used for various tasks. SVMs work by finding the optimal boundary, or hyperplane, that separates different classes of data with the maximum margin, making them highly reliable for classification, especially with complex datasets.</p>
<p>What truly sets SVMs apart is their ability to handle both linear and non-linear data through the <em>kernel trick</em>, allowing them to adapt to a wide range of problems with impressive accuracy. In this blog post, we&rsquo;ll delve into how SVMs work and gently explore the mathematical foundations behind their powerful performance.</p>]]></description>
</item>
<item>
    <title>Direction of Gradient Descent Update</title>
    <link>http://localhost:1313/20240819_gradient_descent/</link>
    <pubDate>Mon, 19 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240819_gradient_descent/</guid>
    <description><![CDATA[<h1 id="on-gradient-descent">On Gradient Descent</h1>
<p>Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the function&rsquo;s minimum value. It is a fundamental concept in machine learning, particularly in training models such as neural networks. The gradient is a vector that represents the direction of the steepest increase of the function at a given point. For example, for a convex function $z = ax^2 + by^2$, the gradient is $[2ax, 2by]$, which points in the direction of the steepest ascent.</p>]]></description>
</item>
<item>
    <title>Introduction to Latent Variable Modeling (Part 1)</title>
    <link>http://localhost:1313/20240818_latent_variable_part1/</link>
    <pubDate>Sun, 18 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240818_latent_variable_part1/</guid>
    <description><![CDATA[<h1 id="latent-variable-modeling">Latent Variable Modeling</h1>
<h2 id="motivation-of-latent-variable-modeling">Motivation of Latent Variable Modeling</h2>
<p>Let&rsquo;s say we want to classify some data. If we had access to a corresponding latent variable for each observation $ \mathbf{x}_i $, modeling would be more straightforward. To illustrate this, consider the challenge of finding the latent variable (i.e., the true class of $ \mathbf{x} $). It can be expressed like $ z^* = \argmax_{z} p(\mathbf{x} | z) $. It is hard to identify the true clusters without prior knowledge about them. For example, we can cluster like Fig. (b) or (c).</p>]]></description>
</item>
</channel>
</rss>
