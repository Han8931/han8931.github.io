<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Causal Inference Part 1: Causation and Correlation - Han&#39;s XYZ</title><meta name="Description" content="Causal Inference"><meta property="og:url" content="https://han8931.github.io/causality-1/">
  <meta property="og:site_name" content="Han&#39;s XYZ">
  <meta property="og:title" content="Causal Inference Part 1: Causation and Correlation">
  <meta property="og:description" content="Causal Inference">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-22T00:00:00+00:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Causality">
    <meta property="article:tag" content="Causal Inference">
    <meta property="og:image" content="https://han8931.github.io/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://han8931.github.io/logo.png">
  <meta name="twitter:title" content="Causal Inference Part 1: Causation and Correlation">
  <meta name="twitter:description" content="Causal Inference">
<meta name="application-name" content="KeepIt">
<meta name="apple-mobile-web-app-title" content="KeepIt">
<meta name="referrer" content="no-referrer" /><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://han8931.github.io/causality-1/" /><link rel="prev" href="https://han8931.github.io/litellm/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><meta name="google-site-verification" content="B7mVm-DfAgGVs4ghQkEqrmeZIv8D26A-1C7dx6Ajeh0" /><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Causal Inference Part 1: Causation and Correlation",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/han8931.github.io\/causality-1\/"
        },"genre": "posts","keywords": "machine learning, causality, causal inference","wordcount":  3115 ,
        "url": "https:\/\/han8931.github.io\/causality-1\/","datePublished": "2025-11-22T00:00:00+00:00","dateModified": "2025-11-22T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Han"
            },"description": "Causal Inference"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/studynotes/"> StudyNotes </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/studynotes/" title="">StudyNotes</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Causal Inference Part 1: Causation and Correlation</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Han</a></span>&nbsp;<span class="post-category">included in <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Machine Learning</a>&nbsp;<a href="/categories/causality/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Causality</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-11-22">2025-11-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;3115 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;15 minutes&nbsp;<span id="/causality-1/" class="leancloud_visitors" data-flag-title="Causal Inference Part 1: Causation and Correlation">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;views
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#correlation-vs-causation">Correlation vs Causation</a>
      <ul>
        <li><a href="#correlation-how-things-move-together">Correlation: how things move together</a></li>
        <li><a href="#causation-how-one-variable-affects-another">Causation: how one variable affects another</a></li>
        <li><a href="#what-most-machine-learning-actually-learns">What most machine learning actually learns</a></li>
        <li><a href="#from-correlation-to-intervention">From correlation to intervention</a></li>
      </ul>
    </li>
    <li><a href="#observations-vs-actions">Observations vs Actions</a>
      <ul>
        <li>
          <ul>
            <li><a href="#example">Example</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#the-limitations-of-observation">The limitations of observation</a></li>
    <li><a href="#causal-models-as-a-bridge-between-data-and-assumptions">Causal models as a bridge between data and assumptions</a>
      <ul>
        <li><a href="#observation-vs-intervention-in-notation">Observation vs intervention in notation</a></li>
        <li><a href="#causal-diagrams-informally">Causal diagrams (informally)</a></li>
        <li><a href="#designing-better-studies">Designing better studies</a></li>
        <li><a href="#reasoning-with-incomplete-data">Reasoning with incomplete data</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Most of the time when we say &ldquo;my model learned something,&rdquo; what it actually learned is a bunch of very smart correlations. If users who click A also tend to click B, or if certain pixels tend to appear together in cat photos, our models will happily latch onto those patterns and exploit them. That&rsquo;s powerful—and often enough for prediction—but it&rsquo;s not the same as understanding what would <em>happen</em> if we actually changed something in the world: raised a price, changed a policy, or shipped a new feature.</p>
<p>This blog post is about that gap. We&rsquo;ll look at how correlation-based learning differs from causal reasoning, why most ML lives firmly on the correlation side, and how ideas from causal inference help us talk more clearly about actions, interventions, and &ldquo;what-if&rdquo; questions.</p>
<p style="text-align:center;"> 
  <img src="/posts/machine_learning/images/causation2.png" alt="causation and correlation" height="400">
</p>
<h2 id="correlation-vs-causation">Correlation vs Causation</h2>
<p>Before we talk about causal inference, it is useful to clarify a basic but crucial distinction: <em>correlation</em> versus <em>causation</em>.</p>
<h3 id="correlation-how-things-move-together">Correlation: how things move together</h3>
<p>Two variables are said to be <em>correlated</em> if they tend to move (or change) together in the data:</p>
<ul>
<li>When one variable is high, the other also tends to be high, or</li>
<li>When one is high, the other tends to be low.</li>
</ul>
<p>Formally, we can measure this with quantities such as <em>covariance</em> or <em>correlation coefficients</em>. Intuitively, correlation captures how much two variables <strong>vary</strong> in the data we observe.</p>
<p>A key point is that correlation is purely about <strong>patterns in the data</strong>, not about underlying mechanisms. If ice-cream sales and drowning accidents go up together in summer, they are correlated; but it would be absurd to say that buying ice cream <em>causes</em> drowning. Both are driven by a third factor: the weather.</p>
<h3 id="causation-how-one-variable-affects-another">Causation: how one variable affects another</h3>
<p>By contrast, <em>causation</em> is about what happens to one variable if we <em>change</em> another variable:</p>
<blockquote>
<p><em>If we force $X$ to change, how does $Y$ respond (on average)?</em></p>
</blockquote>
<p>Here we are interested in the <strong>exact impact</strong> of one variable on another under <em>interventions</em>, not just how they happened to move together in the past. Causal questions are of the form:</p>
<ul>
<li>What is the effect of raising the legal driving age on traffic accidents?</li>
<li>What is the effect of a new drug on blood pressure?</li>
<li>What is the effect of a new recommendation algorithm on user engagement?</li>
</ul>
<p>These questions all ask about the consequences of a hypothetical action, not just about existing correlations.</p>
<h3 id="what-most-machine-learning-actually-learns">What most machine learning actually learns</h3>
<p>Most standard machine learning algorithms (e.g., linear regression, logistic regression, random forests, neural networks, transformers, and so on) are trained to learn patterns like</p>
<p>$$P(Y \mid X) \quad \text{or} \quad f(x) \approx \mathbb{E}[Y \mid X = x],$$</p>
<p>that is, they learn to predict $Y$ from $X$ based on <em>observed co-variation</em> in the data. In other words:</p>
<ul>
<li>They learn <strong>correlations</strong> (or more generally, statistical dependencies) between inputs and outputs.</li>
<li>They do not, by default, distinguish whether those dependencies are <em>causal</em> or simply due to confounding factors.</li>
</ul>
<p>We can think of standard ML as <strong>correlation-based learning</strong>:</p>
<ul>
<li>It focuses on learning <em>how variables change together</em> in the data rather than understanding the underlying &ldquo;why&rdquo;,</li>
<li>It focuses not on what would happen if we actively intervened and changed one variable while holding everything else fixed.</li>
</ul>
<p>This distinction is crucial:</p>
<ul>
<li>A model that is excellent at exploiting correlations may be very good at prediction in environments similar to its training data,</li>
<li>but it can fail badly when we change the environment or policy, because it does not know the <em>underlying causal structure</em>.</li>
</ul>
<p>Causal inference, by contrast, aims precisely at learning (or using assumptions about) the <em>causal impact</em> of one variable on another.</p>
<p>In short, correlation-based learning is about capturing <em>how variables change together</em> in the observed world, while causal inference aims to characterize the <em>direct impact</em> of changing one variable on another under a specified intervention.</p>
<h3 id="from-correlation-to-intervention">From correlation to intervention</h3>
<p>In the rest of this post, we will move from correlation-based questions to truly causal ones. To do this, we will introduce the distinction between:</p>
<ul>
<li><strong>Observations</strong>: passively watching the world as it is, and</li>
<li><strong>Actions (interventions)</strong>: asking what would happen if we changed something in the world.</li>
</ul>
<p>This will lead naturally to the language of <em>causal reasoning</em>, and to examples — such as driving-age policies — that illustrate why correlation alone is not enough.</p>
<h2 id="observations-vs-actions">Observations vs Actions</h2>
<p>Let&rsquo;s start with the difference between just watching the world and actually changing something in it.</p>
<ul>
<li>When we passively observe, we simply watch how people normally behave, following their habits, routines, and preferences.</li>
<li>The data we collect in this way are like a snapshot of the world as it currently is, summarized in whatever features we decided to record (e.g., age, income, accident history).</li>
</ul>
<p>This kind of data is called <em>observational data</em>. So what can we answer with observational data? Some questions fit perfectly into this observational world. For example:</p>
<blockquote>
<h4 id="example">Example</h4>
<p><em>Do 16-year-old drivers have more traffic accidents than 18-year-old drivers?</em></p>
<p>This is a question about how often something happens in the world as it is.</p>
<p>Mathematically, we can compute:</p>
<ul>
<li>the (conditional) probability of an accident given that the driver is 16.</li>
<li>the (conditional) probability of an accident given that the driver is 18.</li>
</ul>
<p>Then compare those two numbers (e.g., subtract them).</p>
<p>If we have a large enough sample with both 16- and 18-year-old drivers, we can estimate these probabilities directly from the data using standard observational statistics.</p>
</blockquote>
<p>Now consider a slightly different question:</p>
<blockquote>
<p><em>What would happen to traffic fatalities if we raised the legal driving age by two years?</em></p>
</blockquote>
<p>This sounds similar, but it&rsquo;s actually a different type of question.</p>
<ul>
<li>The previous question was about <strong>how often something happens in the current world</strong>.</li>
<li>This new question is about <strong>what would happen if we changed the rules of the world.</strong></li>
</ul>
<p>That is, it&rsquo;s asking about the effect of a <em>hypothetical action (an intervention)</em>: &ldquo;Raise the driving age -&gt; what changes?&rdquo;</p>
<p>You can&rsquo;t answer that reliably just by looking at current accident rates by age, because:</p>
<ul>
<li>Maybe older drivers crash less simply because they have more experience, not because they&rsquo;re older.</li>
<li>An 18-year-old with only 2 months of driving experience might be no safer than a 16-year-old with 2 months of experience.</li>
</ul>
<p>We could try to control for experience; for example, we can compare accident rates for people with the same number of months of driving, but different ages.</p>
<p>However, even then, we run into complications:</p>
<ul>
<li>What if 18-year-olds with two months of driving experience tend to be exceptionally cautious, but become less cautious as they gain more experience?</li>
<li>Maybe they tend to live in areas where people do not need to start driving until later in life.</li>
</ul>
<p>So even when you try to adjust for obvious factors like months of experience, other hidden differences can still mess up your conclusions.</p>
<p>We might try another strategy: compare countries with different legal driving ages, like the US and Germany. These countries differ in many ways besides driving age:</p>
<ul>
<li>Public transport, culture, enforcement, and even laws, like the legal drinking age.</li>
</ul>
<p>So differences in accident rates could be caused by any of those factors, not just the driving age.</p>
<p>This is where causal reasoning comes in. Causal reasoning is a framework — both conceptual and technical — for answering questions like:</p>
<ul>
<li>What is the effect of doing $X$? (e.g., raising the driving age)</li>
<li>What action caused $Y$? (e.g., what policy likely reduced accidents?)</li>
</ul>
<p>It focuses on <em>interventions</em>:</p>
<ul>
<li>Not just what is the world like?</li>
<li>But what would the world look like if we changed something?</li>
</ul>
<p>Once we understand how to define and estimate <em>the effect of an action</em>, we can turn questions around and ask:</p>
<blockquote>
<p><em>Given that we observed this outcome, what actions or causes are likely responsible?</em></p>
</blockquote>
<p>To formalize the concept, let&rsquo;s see how the limits of pure observation show up in real data.</p>
<h2 id="the-limitations-of-observation">The limitations of observation</h2>
<p>In 1973, researchers looked at graduate school admissions at the University of California, Berkeley. They had data on 12,763 applicants across 101 departments and programs.</p>
<ul>
<li>About 4,321 of these applicants were women, and roughly 35% of them were admitted.</li>
<li>About 8,442 were men, and around 44% of them were admitted.</li>
</ul>
<p>Just looking at these totals, it seems like <strong>men were more likely to be admitted than women</strong> (44% vs 35%). Standard statistical tests say this difference is too big to be explained by random chance alone, so it looks like a real gap.</p>
<p>The same pattern appeared when the researchers focused on the six largest departments:</p>
<ul>
<li>Across these six, men again had an overall acceptance rate of about 44%,</li>
<li>while women had an overall rate of about 30%.</li>
</ul>
<p>Again, this suggests that men are doing better than women when you look at the data in aggregate (all combined).</p>
<p>However, each department decides who to admit by itself, and departments can be very different — different fields, different standards, different competitiveness. So the researchers drilled down and looked at the acceptance rates within each of those six big departments.</p>
<p>What they found was surprising:</p>
<ul>
<li>In four of the six departments, women actually had a higher acceptance rate than men.</li>
<li>In the other two departments, men had the higher acceptance rate.</li>
</ul>
<p>But those two departments weren&rsquo;t big enough or different enough to explain the large overall gap in the combined data.</p>
<p>We can find a <em>reversal</em>:</p>
<ul>
<li>Overall across departments: men seem to be favored.</li>
<li>Inside most departments: women do as well or better than men.</li>
</ul>
<p style="text-align:center;"> 
  <img src="/posts/machine_learning/images/berkeley.png" alt="Simpson's Paradox" height="300">
</p>
<p>This is an example of what&rsquo;s often called <em>Simpson&rsquo;s paradox</em>: a situation where a pattern that appears in overall (aggregate) data reverses when you break the data into subgroups.</p>
<p>In this case:</p>
<ul>
<li>Event $Y$: the applicant is accepted.</li>
<li>Event $A$: the applicant is female (gender treated as a binary variable).</li>
<li>Variable $Z$: which department the applicant applied to.</li>
</ul>
<p>Simpson&rsquo;s paradox means it can happen that:</p>
<ul>
<li>For each department ($Z$), women might do as well as or better than men:</li>
</ul>
<p>$$P(\text{accepted} \mid \text{female}, Z) \ge P(\text{accepted} \mid \text{male}, Z),$$
but</p>
<ul>
<li>Overall, women still have a lower acceptance rate than men:</li>
</ul>
<p>$$P(\text{accepted} \mid \text{female}) &lt; P(\text{accepted} \mid \text{male}).$$</p>
<p>This happens because men and women apply to different departments in different proportions, and those departments have different levels of competitiveness.</p>
<p>From the data, one thing is very clear: <strong>Gender affects which departments people apply to</strong>. Men and women have <strong>different patterns of department choice</strong>.</p>
<p>We also know that <strong>departments differ in how hard it is to get in</strong>. Some have low acceptance rates (very competitive), others have higher acceptance rates.</p>
<p>So one plausible explanation is:</p>
<ul>
<li>Women tended to apply more to highly competitive departments, while men applied more to less competitive ones.</li>
<li>As a result, women were rejected more often overall, even though departments themselves may have treated individual male and female applicants fairly.</li>
</ul>
<p>This was essentially the conclusion of the original study. They argued:</p>
<ul>
<li>The bias seen in the combined statistics does not come from admissions committees systematically discriminating against women.</li>
<li>Instead, it comes from earlier stages in the pipeline: the way society and the education system have steered women toward certain fields.</li>
</ul>
<p>They suggested that women were <strong>shunted</strong> by their upbringing and education into fields that:</p>
<ul>
<li>are more crowded,</li>
<li>have fewer resources and funding,</li>
<li>have lower completion rates,</li>
<li>and often lead to poorer job prospects.</li>
</ul>
<p>In other words, they said the gender bias was mainly a <em>pipeline problem</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>: by the time women reached graduate applications, they were already concentrated in less favorable, more competitive fields, through no fault of the departments themselves. However, it&rsquo;s hard to fully defend or criticize that conclusion using this data alone, because key information is missing.</p>
<p>For example, we don&rsquo;t know why women chose those more competitive departments:</p>
<ul>
<li>Maybe some less competitive departments (like certain engineering programs) were unwelcoming to women.</li>
<li>Maybe some departments had a bad reputation for how they treated women, so women avoided them.</li>
<li>Maybe the way departments advertised or described themselves discouraged women from applying.</li>
</ul>
<p>We also don&rsquo;t know anything about the qualifications of applicants:</p>
<ul>
<li>It could be that, because of social barriers, women who applied to engineering in 1973 were on average more qualified than the men who applied.</li>
<li>In that case, if the acceptance rate for men and women is equal, it might actually mean women are being held to a higher bar, which would be discrimination.</li>
</ul>
<p>So, the observed acceptance rates alone cannot tell us whether there was discrimination or not. They leave us with many plausible stories, and we can&rsquo;t distinguish between them without more information.</p>
<p>Given this uncertainty, there are two possible options:</p>
<ol>
<li>
<p><strong>Design a new study and collect better data.</strong></p>
<ul>
<li>Measure more variables (like applicant qualifications, department culture, prior experiences, etc.).</li>
<li>This might allow a more conclusive answer about discrimination.</li>
</ul>
</li>
<li>
<p><strong>Stay with the current data and argue using assumptions and background knowledge.</strong></p>
<ul>
<li>Use what we know about the social context of the 1970s, academic culture, and gender norms.</li>
<li>Then argue about which explanation is more likely:
<ul>
<li>Is it mostly a neutral pipeline effect?</li>
<li>Or a mix of pipeline factors and discrimination at various stages?</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="causal-models-as-a-bridge-between-data-and-assumptions">Causal models as a bridge between data and assumptions</h2>
<p>Up to this point, we have seen two recurring themes:</p>
<ul>
<li>Observational data alone can be ambiguous: the same pattern can be explained by multiple causal stories.</li>
<li>Policy questions (&ldquo;what if we changed $X$?&rdquo;) are fundamentally about interventions, not just about correlations.</li>
</ul>
<p>Causal models provide a way to organize both of these issues. They give us:</p>
<ul>
<li>A way to distinguish between <em>observing</em> and <em>doing</em>,</li>
<li>A structured way to encode assumptions about how variables influence one another,</li>
<li>A toolkit to derive the effect of hypothetical actions from observational data when possible.</li>
</ul>
<h3 id="observation-vs-intervention-in-notation">Observation vs intervention in notation</h3>
<p>In ordinary statistics, we are used to working with conditional probabilities of the form</p>
<p>\begin{align*}
P(Y \mid X = x),
\end{align*}
which answer questions of the form:</p>
<blockquote>
<p><em>Among the individuals for whom we</em> see <em>$X = x$, how often do we see $Y$?</em></p>
</blockquote>
<p>For example, $P(\text{accident} \mid \text{age}=16)$ is the accident rate among all drivers who happen to be 16 in our data.</p>
<p>Causal questions, by contrast, refer to the effect of an <em>action</em>. For this we introduce the notation</p>
<p>\begin{align*}
P(Y \mid \operatorname{do}(X = x)),
\end{align*}
which answers:</p>
<blockquote>
<p><em>If we were to</em> force <em>$X$ to take the value $x$ (by intervention), how often would we see $Y$?</em></p>
</blockquote>
<p>In the driving-age example, $P(\text{accident} \mid \operatorname{do}(\text{min.\ age}=18))$ describes the accident rate we would expect under a policy that sets the legal driving age to 18, taking into account all downstream changes (e.g., who drives, when they start, how much experience they accumulate).</p>
<p>In general,
\begin{align*}
P(Y \mid X = x) \neq P(Y \mid \operatorname{do}(X = x)),
\end{align*}
since individuals who <em>happen</em> to have $X = x$ in the observational world may differ in systematic ways from those who would end up with $X = x$ under an intervention. This discrepancy is precisely what we saw in the Berkeley example and the driving-age discussion: simple conditioning on (X) mixes together many other influences.</p>
<h3 id="causal-diagrams-informally">Causal diagrams (informally)</h3>
<p>One convenient way to encode assumptions about how variables influence each other is via <em>causal diagrams</em>, also called directed acyclic graphs (DAGs). For the driving-age example, a highly simplified diagram might look like:</p>
<div class="code-block code-line-numbers open" style="counter-reset: code-block 0">
    <div class="code-header language-text">
        <span class="code-title"><i class="arrow fas fa-angle-right fa-fw" aria-hidden="true"></i></span>
        <span class="ellipses"><i class="fas fa-ellipsis-h fa-fw" aria-hidden="true"></i></span>
        <span class="copy" title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden="true"></i></span>
    </div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Law → Age at start → Experience → Accidents
</span></span><span class="line"><span class="cl">                         |       /   |
</span></span><span class="line"><span class="cl">                         |      /    |
</span></span><span class="line"><span class="cl">                         |     /     |
</span></span><span class="line"><span class="cl">                    Personality  Region</span></span></code></pre></div></div>
<p>Read informally, this says:</p>
<ul>
<li>The <strong>Law</strong> constrains the age at which people can start driving.</li>
<li><strong>Age at start</strong> influences how much <strong>Experience</strong> they can accumulate.</li>
<li><strong>Experience</strong> influences the risk of <strong>Accidents</strong>.</li>
<li><strong>Personality</strong> (cautious vs reckless) affects both how much <strong>Experience</strong> people seek and their risk of <strong>Accidents</strong>.</li>
<li><strong>Region</strong> (urban vs rural, road conditions, etc.) also affects <strong>Accidents</strong>.</li>
</ul>
<p>This kind of diagram helps us see why simply comparing $P(\text{Accidents} \mid \text{Age})$ can be misleading:</p>
<ul>
<li>Age is entangled with Experience, Personality, and Region.</li>
<li>Changing the Law changes Age, which in turn changes Experience and who drives at all.</li>
</ul>
<p>Causal calculus (Pearl&rsquo;s do-calculus) tells us when and how we can recover $P(Y \mid \operatorname{do}(X=x))$ from purely observational data given such a diagram, and when we cannot.</p>
<h3 id="designing-better-studies">Designing better studies</h3>
<p>Causal models are not only a tool for interpreting existing data; they also guide the <em>design</em> of new studies. Given a causal diagram, we can ask:</p>
<ul>
<li>Which variables must we measure in order to block spurious associations (confounding paths)?</li>
<li>Which variables should we <em>not</em> condition on, because they may introduce new biases (colliders)?</li>
<li>Under what conditions can we estimate causal effects from purely observational data?</li>
</ul>
<p>In the Berkeley example, a causal model could tell us that we are missing key variables, such as:</p>
<ul>
<li>Applicant qualifications (grades, test scores, research experience),</li>
<li>Department climate or reputation for treating women,</li>
<li>Broader social forces shaping field choice.</li>
</ul>
<p>The model would then make explicit that, without additional measurements or assumptions, certain causal questions (e.g., &ldquo;Was there discrimination at the department level?&rdquo;) cannot be answered definitively from the existing data.</p>
<h3 id="reasoning-with-incomplete-data">Reasoning with incomplete data</h3>
<p>Even when we cannot collect more data, causal models still have value. They allow us to:</p>
<ul>
<li>Enumerate different plausible causal stories consistent with the observed data,</li>
<li>Explore the implications of each story (&ldquo;If the world worked this way, what would the effect of changing (X) be?&rdquo;),</li>
<li>Identify which assumptions are doing the real work in our conclusions.</li>
</ul>
<p>In other words, a causal model acts as a <em>logic engine</em> for cause-and-effect reasoning: given explicit assumptions, it yields explicit conclusions, and it clarifies which parts of our reasoning depend on empirical evidence and which parts depend on judgment and prior knowledge.</p>
<h2 id="summary">Summary</h2>
<p>We can now summarize the main points:</p>
<ul>
<li>Observational data show us how the world <em>is</em>, not automatically how it would be if we <em>changed</em> something.</li>
<li>Questions about policies or interventions (raising the driving age, changing admissions rules, etc.) are inherently <em>causal</em> and require reasoning about $\operatorname{do}(\cdot)$, not just conditional probabilities.</li>
<li>The Berkeley admissions example illustrates how aggregated patterns can reverse when we condition on a relevant variable (department), a phenomenon known as Simpson&rsquo;s paradox.</li>
<li>Observational patterns alone can be compatible with multiple causal explanations (pipeline effects, discrimination at various stages, or both); data without a causal model cannot fully resolve such ambiguities.</li>
<li>Causal models provide a structured way to:
<ul>
<li>Distinguish observation from intervention,</li>
<li>Design better studies and decide which variables to measure or control,</li>
<li>Connect domain knowledge and assumptions to concrete, testable implications.</li>
</ul>
</li>
</ul>
<p>In this sense, causal inference is not just an add-on to statistics but a complementary framework that lets us talk rigorously about actions, policies, and the mechanisms that generate the data we see.</p>
<h1 id="reference">Reference</h1>
<ul>
<li><em>Patterns, predictions, and actions: A story about machine learning</em>, Moritz Hardt, Benjamin Recht, 2021</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This term describes a situation where the number of people, particularly women, decreases at each successive stage of a career pipeline, leading to a significant underrepresentation at the top levels.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-11-22</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/causality-1/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/causality/">Causality</a>,&nbsp;<a href="/tags/causal-inference/">Causal Inference</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/litellm/" class="prev" rel="prev" title="LiteLLM: LLM Proxy Server"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>LiteLLM: LLM Proxy Server</a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.152.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.3.1-DEV"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2024 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Han</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a>
        </div>

        <div id="fixed-buttons-hidden"><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"><script src="https://han-6.disqus.com/embed.js" defer></script><script src="https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@5.20.2/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js"></script><script>window.config={"comment":{"valine":{"appId":"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI","appKey":"WBmoGyJtbqUswvfLh6L8iEBr","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"en","pageSize":10,"placeholder":"Your comment ...","recordIP":true,"serverURLs":"https://leancloud.xxxxx.com","visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"A9NUSQZEO5","algoliaIndex":"github","algoliaSearchKey":"e255482bc340762a0da27f50eddd2765","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script src="/js/theme.min.js"></script></body>
</html>
