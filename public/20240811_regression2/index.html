<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Getting Started with Regression (Part 2) - Han&#39;s XYZ</title><meta name="Description" content="Introduction to Regression: Understanding the Basics (Part 2)"><meta property="og:url" content="http://localhost:1313/20240811_regression2/">
  <meta property="og:site_name" content="Han&#39;s XYZ">
  <meta property="og:title" content="Getting Started with Regression (Part 2)">
  <meta property="og:description" content="Introduction to Regression: Understanding the Basics (Part 2)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-08-11T18:14:02+09:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Regression">
    <meta property="article:tag" content="Least Square">
    <meta property="og:image" content="http://localhost:1313/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/logo.png">
  <meta name="twitter:title" content="Getting Started with Regression (Part 2)">
  <meta name="twitter:description" content="Introduction to Regression: Understanding the Basics (Part 2)">
<meta name="application-name" content="KeepIt">
<meta name="apple-mobile-web-app-title" content="KeepIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/20240811_regression2/" /><link rel="prev" href="http://localhost:1313/20240810_regression1/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Getting Started with Regression (Part 2)",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/20240811_regression2\/"
        },"genre": "posts","keywords": "machine learning, regression, least square","wordcount":  1197 ,
        "url": "http:\/\/localhost:1313\/20240811_regression2\/","datePublished": "2024-08-11T00:00:00+00:00","dateModified": "2024-08-11T18:14:02+09:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Han"
            },"description": "Introduction to Regression: Understanding the Basics (Part 2)"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Getting Started with Regression (Part 2)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Han</a>
</span>&nbsp;<span class="post-category">included in <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Machine Learning</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-11">2024-08-11</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1197 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;6 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#understanding-ridge-regression">Understanding Ridge Regression</a></li>
    <li><a href="#overdetermined-and-underdetermined-problems">Overdetermined and Underdetermined Problems</a>
      <ul>
        <li><a href="#overdetermined-systems">Overdetermined Systems</a></li>
        <li><a href="#underdetermined-systems">Underdetermined Systems</a></li>
      </ul>
    </li>
    <li><a href="#regularization-and-ridge-regression">Regularization and Ridge Regression</a>
      <ul>
        <li><a href="#understanding-the-role-of-lambda">Understanding the Role of $\lambda$</a></li>
        <li><a href="#spectral-decomposition-and-invertibility">Spectral Decomposition and Invertibility</a></li>
        <li><a href="#dual-form-of-ridge-regression">Dual Form of Ridge Regression</a></li>
      </ul>
    </li>
    <li><a href="#considering-varying-confidence-in-measurements-weighted-regression">Considering Varying Confidence in Measurements: Weighted Regression</a>
      <ul>
        <li>
          <ul>
            <li><a href="#references">References:</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="an-introductory-guide-part-2">An Introductory Guide (Part 2)</h1>
<h2 id="understanding-ridge-regression">Understanding Ridge Regression</h2>
<p>In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model.</p>
<ul>
<li>
<p><strong>Overfitting</strong> occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data.</p>
</li>
<li>
<p><strong>Underfitting</strong>, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data.</p>
</li>
</ul>
<p>To address these issues, regularization techniques are often used. Regularization involves <strong>adding a penalty term to the model&rsquo;s objective function, which helps control the complexity of the model and prevents it from overfitting</strong>.</p>
<h2 id="overdetermined-and-underdetermined-problems">Overdetermined and Underdetermined Problems</h2>
<p>Recall that linear regression is fundamentally an optimization problem where we aim to find the optimal parameter vector $\boldsymbol{\theta}_{opt}$ that minimizes the residual sum of squares between the observed data and the model&rsquo;s predictions. Mathematically, this is expressed as:</p>
<p>$$\boldsymbol{\theta}_{opt} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d}\lVert y-\mathbf{X}\boldsymbol{\theta}\rVert^2.$$</p>
<h3 id="overdetermined-systems">Overdetermined Systems</h3>
<p>An optimization problem is termed <em>overdetermined</em> when the design matrix (or data matrix) $\mathbf{X}\in \mathbb{R}^{m\times d}$ has more rows than columns, i.e., $m&gt;d$. This configuration means that there are more equations than unknowns, typically leading to a unique solution. In other words, <em>there is more information available than the number of unknowns</em>. The unique solution can be found using the formula:
$$\boldsymbol{\theta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$</p>
<p>The solution exists if and only if $\mathbf{X}^T\mathbf{X}$ is invertible, which is true when the columns of $\mathbf{X}$ are linearly independent, meaning $\mathbf{X}$ is full rank.</p>
<h3 id="underdetermined-systems">Underdetermined Systems</h3>
<p>In contrast, when $\mathbf{X}$ is fat and short (i.e., $m&lt;d$), the problem is called <em>underdetermined</em>. In this scenario, there are more unknowns than equations, leading to infinitely many solutions. This occurs because <em>the system has less information than the number of unknowns</em>. Among these, the solution that minimizes the squared norm of the parameters is preferred. This solution is known as the minimum-norm least-squares solution.</p>
<p>For an underdetermined linear regression problem, the objective can be written as:</p>
<p>\begin{align*}
\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d} \lVert \boldsymbol{\theta}\rVert^2, \quad \textrm{subject to}\ \mathbf{y} = \mathbf{X}\boldsymbol{\theta}.
\end{align*}</p>
<p>Here, $\mathbf{X}\in \mathbb{R}^{m\times d}, \boldsymbol{\theta}\in \mathbb{R}^d,$ and $\mathbf{y}\in \mathbb{R}^m$. If the matrix has full rank, meaning rank$(\mathbf{X})=m$, then the linear regression problem will have a unique global minimum</p>
<p>\begin{align*}
\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{y}.
\end{align*}</p>
<p>This solution is called the minimum-norm least-squares solution. The proof of this solution is given by:
\begin{align*}
\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \lVert\boldsymbol{\theta}\rVert^2 + \boldsymbol{\lambda}^T(\mathbf{X}\boldsymbol{\theta}-\mathbf{y}),
\end{align*}</p>
<p>where $\boldsymbol{\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives with respec to $\boldsymbol{\lambda}$ and $\boldsymbol{\theta}$ and setting them to zero:</p>
<p>\begin{align*}
\nabla_{\boldsymbol{\theta}} &amp;= 2 \boldsymbol{\theta} + \mathbf{X}^T\boldsymbol{\lambda} = 0\\
\nabla_{\boldsymbol{\lambda}} &amp;= \mathbf{X}\boldsymbol{\theta} - \mathbf{y} = 0
\end{align*}</p>
<p>The first equation gives us $\boldsymbol{\theta} = -\mathbf{X}^T\boldsymbol{\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\mathbf{X})=N$ so that $\mathbf{X}^T\mathbf{X}$ is invertible, we have $\boldsymbol{\lambda} = -2 (\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{y}.$ Thus, we have</p>
<p>\begin{align*}
\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{y}.
\end{align*}</p>
<p>Note that $\mathbf{X}\mathbf{X}^T$ is often called a <em>Gram matrix</em>, $\mathbf{G}$.</p>
<h2 id="regularization-and-ridge-regression">Regularization and Ridge Regression</h2>
<p>Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term that encourages the parameters to behave better, effectively controlling their magnitude. Ridge regression is a widely-used regularization technique that adds a penalty proportional to the square of the magnitude of the model parameters.</p>
<p>The objective function for ridge regression is formulated as:</p>
<p>\begin{align*}
J(\boldsymbol{\theta}) = \lVert\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\rVert^2_2 + \lambda \lVert\boldsymbol{\theta}\rVert^2_2
\end{align*}</p>
<p>This can be expanded as:</p>
<p>\begin{align*}
J(\boldsymbol{\theta}) = (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta}) + \lambda\boldsymbol{\theta}^T\boldsymbol{\theta}
\end{align*}</p>
<p>Breaking it down further:</p>
<p>\begin{align*}
J(\boldsymbol{\theta}) = \mathbf{y}^T\mathbf{y} - \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\theta} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} + \lambda\boldsymbol{\theta}^T\mathbf{I}\boldsymbol{\theta}
\end{align*}</p>
<p>To minimize the objective function $J(\boldsymbol{\theta})$, we take the derivative with respect to $\boldsymbol{\theta}$ and set it to zero:</p>
<p>\begin{align*}
\frac{\partial J}{\partial \boldsymbol{\theta}} = -\mathbf{X}^T\mathbf{y} - \mathbf{X}^T\mathbf{y} + \mathbf{X}^T\mathbf{X}\boldsymbol{\theta} + \mathbf{X}^T\mathbf{X}\boldsymbol{\theta} + 2\lambda\mathbf{I}\boldsymbol{\theta} = 0
\end{align*}</p>
<p>Solving for $\boldsymbol{\theta}$, we obtain the ridge regression solution:</p>
<p>\begin{align*}
\boldsymbol{\theta} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{align*}</p>
<h3 id="understanding-the-role-of-lambda">Understanding the Role of $\lambda$</h3>
<ul>
<li>When $\lambda$ approaches 0, the regularization term $\lambda \lVert\boldsymbol{\theta}\rVert^2_2$ becomes negligible, making ridge regression equivalent to ordinary least squares (OLS), which can lead to overfitting if the model is too complex.
<ul>
<li>If $\lambda\to 0$, then $\lVert\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\rVert^2_2 + \underbrace{\lambda \lVert\boldsymbol{\theta}\rVert^2_2}_{=0}$</li>
</ul>
</li>
<li>As $\lambda$ increases towards infinity, the regularization term dominates, forcing $\boldsymbol{\theta}$ towards zero. In this case, the solution becomes overly simplistic, effectively shrinking the model parameters to zero, which may result in underfitting.
<ul>
<li>$\lambda\to \infty$, then $\underbrace{\frac{1}{\lambda}\lVert\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\rVert^2_2}_{=0} + \lVert\boldsymbol{\theta}\rVert^2_2$</li>
<li>Since what we want to do is to minimize the objective function, we can divide it by $\lambda$. Therefore, the solution will be $\boldsymbol{\theta}=0$, because it is the smallest value the squared function can achieve.</li>
</ul>
</li>
</ul>
<h3 id="spectral-decomposition-and-invertibility">Spectral Decomposition and Invertibility</h3>
<p>It&rsquo;s important to note that $\mathbf{X}^T\mathbf{X}$ is always symmetric. According to the Spectral theorem, this matrix can be decomposed as $\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$, where $\mathbf{Q}$ is the eigenvector matrix, and $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues. This allows us to express the inverse operation in ridge regression as:</p>
<p>\begin{align*}
\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I} &amp;= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{I}\\
&amp;= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{Q}\mathbf{Q}^T\\
&amp;= \mathbf{Q}(\mathbf{\Lambda}+\lambda\mathbf{I})\mathbf{Q}^T.
\end{align*}</p>
<p>Even if $\mathbf{X}^T\mathbf{X}$ is not invertible (or is close to being non-invertible), the regularization constant $\lambda$ ensures invertibility by making the matrix full-rank.</p>
<h3 id="dual-form-of-ridge-regression">Dual Form of Ridge Regression</h3>
<p>Ridge regression can also be expressed in its dual form, which is particularly useful for solving underdetermined problems:
\begin{align*}
(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta}	&amp;= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\\
(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta} &amp;= \mathbf{X}^T\mathbf{y}\\
\boldsymbol{\theta} &amp;= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\mathbf{y}-\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})\\
&amp;= \mathbf{X}^T\lambda^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
&amp;= \mathbf{X}^T\alpha\\
\lambda\alpha &amp;= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
&amp;= (\mathbf{y}-\mathbf{X}\mathbf{X}^T\alpha)\\
\mathbf{y} &amp;= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
\alpha &amp;= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\mathbf{y}\\
\alpha &amp;= (\mathbf{G}+\lambda)^{-1}\mathbf{y}.
\end{align*}
Here, $\alpha$ represents the dual coefficients, and $\mathbf{G}$ is the Gram matrix. This formulation is especially powerful when dealing with high-dimensional data, where the number of features exceeds the number of samples.</p>
<h2 id="considering-varying-confidence-in-measurements-weighted-regression">Considering Varying Confidence in Measurements: Weighted Regression</h2>
<p>Up to this point, we have assumed that all measurements are equally reliable. However, in practice, the confidence in different measurements may vary. To account for this, we can consider the situation where the noise associated with each measurement has a zero mean and is independent across measurements. Under these conditions, the covariance matrix for the measurement noise can be expressed as follows:</p>
<p>\begin{align*}
R &amp;= \mathbb{E}(\eta\eta^T)\\
&amp;= \begin{bmatrix}
\sigma_1^2 &amp; \dots &amp; 0\\
\vdots &amp; \ddots &amp; \vdots\\
0 &amp; \dots &amp; \sigma_l^2
\end{bmatrix}
\end{align*}</p>
<p>By denoting the error vector $\mathbf{y}-\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\epsilon} = (\epsilon_1, \dots, \epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements:
\begin{align*}
J(\tilde{\mathbf{x}}) &amp;= \boldsymbol{\epsilon}^TR^{-1}\boldsymbol{\epsilon}=\frac{\boldsymbol{\epsilon}_1^2}{\sigma_1^2}+\dots+\frac{\boldsymbol{\epsilon}_l^2}{\sigma_l^2}\\
&amp;= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^TR^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})
\end{align*}
Note that by dividing each residual by its variance, we effectively <em>equalize the influence of each data point on the overall fitting process</em>. Subsequently, by taking the partial derivative of $J$ with respect to $\boldsymbol{\theta}$, we get the best estimate of the parameter, which is given by
$$\boldsymbol{\theta} = (\mathbf{X}^TR^{-1}\mathbf{X})^{-1}\mathbf{X}^TR^{-1}\mathbf{y}.$$
Note that the measurement noise matrix $R$ must be non-singular for a solution to exist.</p>
<p>To learn more, please take a look at this <a href="https://github.com/Han8931/deep_statistical_learning" target="_blank" rel="noopener noreffer">note</a>
!</p>
<p>This article continues in Part 3.</p>
<h4 id="references">References:</h4>
<ol>
<li>H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014</li>
<li>Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006</li>
</ol>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-08-11&nbsp;<a class="git-hash" href="https://github.com/Han8931/commit/908c1d34875eafe3a0ba65022da09d5356a8215f" target="_blank" title="commit by Han(tabularasa8931@gmail.com) 908c1d34875eafe3a0ba65022da09d5356a8215f: Update">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>908c1d3</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/20240811_regression2/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/regression/">Regression</a>,&nbsp;<a href="/tags/least-square/">Least Square</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/20240810_regression1/" class="prev" rel="prev" title="Getting Started with Regression (Part 1)"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Getting Started with Regression (Part 1)</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.129.0">Hugo</a> | Theme - <a href="https://github.com/Fastbyte01/KeepIt" target="_blank" rel="noopener noreffer" title="KeepIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> KeepIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Han</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
