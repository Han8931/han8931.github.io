<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Least Square - Tag - Han&#39;s XYZ</title>
        <link>http://localhost:1313/tags/least-square/</link>
        <description>Least Square - Tag - Han&#39;s XYZ</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 12 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/least-square/" rel="self" type="application/rss+xml" /><item>
    <title>Getting Started with Regression Part 3. RLS</title>
    <link>http://localhost:1313/20240812_recursive_least_square/</link>
    <pubDate>Mon, 12 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240812_recursive_least_square/</guid>
    <description><![CDATA[<h1 style="line-height: 1.3;">Deep Dive into Regression: Recursive Least Squares Explained (Part 3)</h1>
<h2 id="introduction-to-recursive-least-squares">Introduction to Recursive Least Squares</h2>
<p>Ordinary least squares assumes that all data is available at once, but in practice, this isn&rsquo;t always the case. <strong>Often, measurements are obtained sequentially</strong>, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where <strong><em>Recursive Least Squares</em></strong> (RLS) comes into play.</p>]]></description>
</item>
<item>
    <title>Getting Started with Regression Part 2. Ridge Regression</title>
    <link>http://localhost:1313/20240811_regression2/</link>
    <pubDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240811_regression2/</guid>
    <description><![CDATA[<h1 id="an-introductory-guide-part-2">An Introductory Guide (Part 2)</h1>
<h2 id="understanding-ridge-regression">Understanding Ridge Regression</h2>
<p>In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model.</p>
<ul>
<li>
<p><strong>Overfitting</strong> occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data.</p>
</li>
<li>
<p><strong>Underfitting</strong>, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data.</p>]]></description>
</item>
<item>
    <title>Getting Started with Regression Part 1. Basics</title>
    <link>http://localhost:1313/20240810_regression1/</link>
    <pubDate>Sat, 10 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240810_regression1/</guid>
    <description><![CDATA[<h1 id="an-introductory-guide-part-1">An Introductory Guide (Part 1)</h1>
<p>Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That&rsquo;s why I&rsquo;ve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction.</p>]]></description>
</item>
</channel>
</rss>
