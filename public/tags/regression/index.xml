<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression on Han&#39;s XYZ</title>
    <link>http://localhost:1313/tags/regression/</link>
    <description>Recent content in Regression on Han&#39;s XYZ</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
    <webMaster>tabularasa8931@gmail.com (Han)</webMaster>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Sat, 31 Aug 2024 10:02:57 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting Started with Regression Part 3. RLS</title>
      <link>http://localhost:1313/20240812_recursive_least_square/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate><author>tabularasa8931@gmail.com (Han)</author>
      <guid>http://localhost:1313/20240812_recursive_least_square/</guid>
      <description>&lt;h1 style=&#34;line-height: 1.3;&#34;&gt;Deep Dive into Regression: Recursive Least Squares Explained (Part 3)&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction-to-recursive-least-squares&#34;&gt;Introduction to Recursive Least Squares&lt;/h2&gt;&#xA;&lt;p&gt;Ordinary least squares assumes that all data is available at once, but in practice, this isn&amp;rsquo;t always the case. &lt;strong&gt;Often, measurements are obtained sequentially&lt;/strong&gt;, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where &lt;strong&gt;&lt;em&gt;Recursive Least Squares&lt;/em&gt;&lt;/strong&gt; (RLS) comes into play.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting Started with Regression Part 2. Ridge Regression</title>
      <link>http://localhost:1313/20240811_regression2/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><author>tabularasa8931@gmail.com (Han)</author>
      <guid>http://localhost:1313/20240811_regression2/</guid>
      <description>&lt;h1 id=&#34;an-introductory-guide-part-2&#34;&gt;An Introductory Guide (Part 2)&lt;/h1&gt;&#xA;&lt;h2 id=&#34;understanding-ridge-regression&#34;&gt;Understanding Ridge Regression&lt;/h2&gt;&#xA;&lt;p&gt;In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Underfitting&lt;/strong&gt;, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting Started with Regression Part 1. Basics</title>
      <link>http://localhost:1313/20240810_regression1/</link>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate><author>tabularasa8931@gmail.com (Han)</author>
      <guid>http://localhost:1313/20240810_regression1/</guid>
      <description>&lt;h1 id=&#34;an-introductory-guide-part-1&#34;&gt;An Introductory Guide (Part 1)&lt;/h1&gt;&#xA;&lt;p&gt;Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That&amp;rsquo;s why I&amp;rsquo;ve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
