<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine Learning - Tag - Han&#39;s XYZ</title>
        <link>http://localhost:1313/tags/machine-learning/</link>
        <description>Machine Learning - Tag - Han&#39;s XYZ</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 12 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Getting Started with Regression (Part 3)</title>
    <link>http://localhost:1313/20240812_recursive_least_square/</link>
    <pubDate>Mon, 12 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240812_recursive_least_square/</guid>
    <description><![CDATA[Deep Dive into Regression: Recursive Least Squares Explained (Part 3) Introduction to Recursive Least Squares Ordinary least squares assumes that all data is available at once, but in practice, this isn&rsquo;t always the case. Often, measurements are obtained sequentially, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements.]]></description>
</item>
<item>
    <title>Getting Started with Regression (Part 2)</title>
    <link>http://localhost:1313/20240811_regression2/</link>
    <pubDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240811_regression2/</guid>
    <description><![CDATA[An Introductory Guide (Part 2) Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model.
Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data.
Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data.]]></description>
</item>
<item>
    <title>Getting Started with Regression (Part 1)</title>
    <link>http://localhost:1313/20240810_regression1/</link>
    <pubDate>Sat, 10 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240810_regression1/</guid>
    <description><![CDATA[An Introductory Guide (Part 1) Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That&rsquo;s why I&rsquo;ve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction.]]></description>
</item>
</channel>
</rss>
