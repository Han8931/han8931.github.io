<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Least-Square SVM - Tag - Han&#39;s XYZ</title>
        <link>http://localhost:1313/tags/least-square-svm/</link>
        <description>Least-Square SVM - Tag - Han&#39;s XYZ</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 01 Sep 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/least-square-svm/" rel="self" type="application/rss+xml" /><item>
    <title>Introduction to SVM Part 3. Asymmetric Kernels</title>
    <link>http://localhost:1313/20240902_svm3/</link>
    <pubDate>Sun, 01 Sep 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240902_svm3/</guid>
    <description><![CDATA[<h1 id="introduction-to-asymmetric-kernels">Introduction to Asymmetric Kernels</h1>
<p>Recall that the dual form of LS-SVM is given by
\begin{align*}
\begin{bmatrix}
0 &amp; y^T \\
y &amp; \Omega + \frac{1}{\gamma} I
\end{bmatrix}
\begin{bmatrix}
b \\
\alpha
\end{bmatrix}
=
\begin{bmatrix}
0 \\
e
\end{bmatrix}
\end{align*}
An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way.</p>]]></description>
</item>
<item>
    <title>Introduction to SVM Part 2. LS-SVM</title>
    <link>http://localhost:1313/20240825_svm2/</link>
    <pubDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240825_svm2/</guid>
    <description><![CDATA[<h1 id="introduction-to-least-square-svm">Introduction to Least-Square SVM</h1>
<h2 id="introduction">Introduction</h2>
<p>Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a <em>least squares cost function</em>. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve.</p>
<h3 id="optimization-problem-primal-problem">Optimization Problem (Primal Problem)</h3>
<p>\begin{align*}
&amp;\min_{w, b, e} \frac{1}{2} \lVert w\rVert^2 + \frac{\gamma}{2} \sum_{i=1}^N e_i^2,\\
&amp;\text{subject to } y_i (w^T \phi(x_i) + b) = 1 - e_i, \ \forall i
\end{align*}
where:</p>]]></description>
</item>
</channel>
</rss>
