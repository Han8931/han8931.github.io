[{"categories":["uv","python"],"content":"uv workspace tutorial","date":"2025-12-08","objectID":"/uv-packages/","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Understanding uv Workspaces The official uv website explains workspaces very clearly: Inspired by Cargo, a uv workspace is a collection of one or more Python packages (workspace members) managed together in a single repo. Each package has its own pyproject.toml, but the workspace shares one lockfile, keeping dependencies consistent across apps and libraries. Commands like uv lock operate on the whole workspace, while uv run and uv sync default to the workspace root but can target a specific member via --package 1. In practice, a uv workspace is just a directory that contains multiple Python projects (apps and/or libraries) that are managed together with one top-level configuration. Workspaces are especially useful when: You have multiple related apps or services (e.g. API, worker, CLI) You maintain one or more internal libraries used across those apps You want shared dependencies and a single lockfile for consistent environments For example, you can have: llm-platform/ pyproject.toml # workspace root api/ pyproject.toml # FastAPI app worker/ pyproject.toml # queue consumers, tasks common/ pyproject.toml # shared utilities, models And uv understands that my-workspace is a workspace root and each project is a member. ","date":"2025-12-08","objectID":"/uv-packages/:1:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Basic structures: ","date":"2025-12-08","objectID":"/uv-packages/:2:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Root pyproject.toml At the root, you don‚Äôt define a Python package (you can, but usually it‚Äôs just a config container). Most important is the [tool.uv.workspace] section: [tool.uv.workspace] members = [ \"api\", \"worker\", \"common\", ] This tells uv: ‚Äúthese subdirs are workspace members‚Äù. ","date":"2025-12-08","objectID":"/uv-packages/:2:1","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Members Inside apl/pyproject.toml: [project] name = \"llm-api\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"fastapi\", \"uvicorn[standard]\" ] [project.optional-dependencies] dev = [ \"pytest\", \"httpx\", ] Inside common/pyproject.toml: [project] name = \"llm-common\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"pydantic\", ] Now api can depend on common as a normal package: [project] name = \"llm-api\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"fastapi\", \"uvicorn[standard]\", \"llm-common\", # just use the name ] uv will notice that llm-common is another workspace member. ","date":"2025-12-08","objectID":"/uv-packages/:2:2","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Typical commands: To run a command inside a specific member: # Run a FastAPI server defined in api/ uv run --package api uvicorn llm_api.main:app --reload To add a dependency to a member, cd into the member package, then: # Add 'redis' to worker/ only uv add redis You can lock dependencies for whole workspace by uv lock You can create a member project like this: mkdir api common cd api uv init --package llm_api cd ../common uv init --package llm_common cd .. ","date":"2025-12-08","objectID":"/uv-packages/:2:3","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"uv sources A workspace is about project structure: It says: ‚ÄúThese directories are all part of one big project/monorepo.‚Äù Defined in [tool.uv.workspace] in the root pyproject.toml. All members share: One uv.lock One virtualenv (by default) Shared tool.uv config [project] name = \"my-app\" version = \"0.1.0\" dependencies = [\"hello-common\"] [tool.uv.workspace] members = [\"libs/*\"] my-app is the workspace root. Everything under libs/ (like libs/hello_common) is a workspace member. uv will install all members as editable when you sync the workspace. On the other hand, [tool.uv.sources] is about where a dependency comes from: It enriches project.dependencies with alternative sources: [project] name = \"my-app\" version = \"0.1.0\" dependencies = [\"hello-common\"] [tool.uv.workspace] members = [\"libs/*\"] [tool.uv.sources] hello-common = { workspace = true } It says When you see dependency hello-common, don‚Äôt fetch from PyPI ‚Äî resolve it from my workspace member instead. You can think of sources as: Given a dependency name, where do we install it from? This is especially useful in: Company environments with internal PyPI mirrors Air-gapped / proxy-heavy setups When you want a local wheel dir for speed For example, you want my-internal-lib to come from your company index, not public PyPI. [tool.uv.sources] my-internal-lib = { index = \"https://pypi.mycompany.com/simple\" } Then in [project.dependencies] you just write: [project] name = \"my-app\" version = \"0.1.0\" dependencies = [ \"my-internal-lib\", ] In your uv config file (not pyproject.toml), you can set a global index (e.g. corporate mirror): [package-index] default = \"https://pypi.mycompany.com/simple\" ","date":"2025-12-08","objectID":"/uv-packages/:3:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Hands-on Example Let‚Äôs use a minimal setup, starting from what uv init gives you. ","date":"2025-12-08","objectID":"/uv-packages/:4:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Starting point You run: uv init uv-ws-practice cd uv-ws-practice You get: uv-ws-practice/ ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main.py ‚îî‚îÄ‚îÄ pyproject.toml main.py is a simple script. Now let‚Äôs add a library project hello_common under libs/. ","date":"2025-12-08","objectID":"/uv-packages/:4:1","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Create the library project From uv-ws-practice: mkdir -p libs/hello_common cd libs/hello_common uv init --package . or identically mkdir libs uv init --package hello_common Now, the project directory looks like this uv-ws-practice/ ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main.py ‚îú‚îÄ‚îÄ pyproject.toml # root ‚îî‚îÄ‚îÄ libs/ ‚îî‚îÄ‚îÄ hello_common/ # library project ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ pyproject.toml ‚îî‚îÄ‚îÄ src/ ‚îî‚îÄ‚îÄ hello_common/ ‚îî‚îÄ‚îÄ __init__.py Edit libs/hello_common/src/hello_common/__init__.py: def greet(name: str) -\u003e str: return f\"Hello, {name} from hello_common!\" def main() -\u003e None: # For a console script print(greet(\"world\")) Edit libs/hello_common/pyproject.toml so it would look like: [project] name = \"hello-common\" # distribution name version = \"0.1.0\" description = \"A small shared library for uv workspace practice.\" readme = \"README.md\" requires-python = \"\u003e=3.10\" dependencies = [] [project.scripts] hello-common = \"hello_common:main\" # console script name [build-system] ... Now we have: Project/distribution: hello-common Python package: hello_common (under src/hello_common) Script: hello-common running hello_common.main(). ","date":"2025-12-08","objectID":"/uv-packages/:4:2","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Turn the root into a workspace and declare the dependency Open the root pyproject.toml and make it: [project] name = \"uv-ws-practice\" version = \"0.1.0\" description = \"Root of a uv workspace to practice workspace and sources.\" readme = \"README.md\" requires-python = \"\u003e=3.10\" dependencies = [ \"hello-common\", # ‚úÖ root project depends on the library ] [build-system] ... # 1) Workspace members [tool.uv.workspace] members = [ \"libs/hello_common\", ] # 2) Source override: \"hello-common\" comes from *this* workspace [tool.uv.sources] hello-common = { workspace = true } Under [project] we say: ‚ÄúThis root project needs hello-common to run.‚Äù Under [tool.uv.workspace] we say: ‚ÄúThe project at libs/hello_common is part of this workspace.‚Äù Under [tool.uv.sources] we say: ‚ÄúWhen resolving the dependency hello-common, use the workspace project instead of PyPI.‚Äù ","date":"2025-12-08","objectID":"/uv-packages/:4:3","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"Use the library in main.py In the root main.py: from hello_common import greet def main() -\u003e None: print(greet(\"Workspace\")) if __name__ == \"__main__\": main() Now main.py imports the hello_common package. From uv-ws-practice: uv sync This command Reads the root pyproject.toml Sees dependencies = [\"hello-common\"] Sees in [tool.uv.sources] that hello-common is from the workspace Looks in [tool.uv.workspace].members for a matching project (libs/hello_common) Resolves dependencies and sets up an environment containing: the root project the hello-common library uv run python main.py You see: Hello, Workspace from hello_common! If you had forgotten to add \"hello-common\" to dependencies, you‚Äôd get: ModuleNotFoundError: No module named 'hello_common' Even though the library exists in libs/hello_common. That‚Äôs the important lesson: workspace + sources does not automatically make everything importable; you still must depend on it. Now try: uv run hello-common Because: The root env was synced Root depends on hello-common hello-common exposes a script called hello-common ‚Ä¶uv will run the console script from the root project‚Äôs environment: Hello, world from hello_common! Alternatively, inside libs/hello_common: cd libs/hello_common uv run hello-common Now uv uses the library project itself as the current project and still runs the same script. https://docs.astral.sh/uv/concepts/projects/workspaces/¬†‚Ü©Ô∏é ","date":"2025-12-08","objectID":"/uv-packages/:4:4","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/uv-packages/"},{"categories":["uv","python"],"content":"uv workspace tutorial","date":"2025-12-07","objectID":"/drafts/uv-packages/","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"uv workspace A uv workspace is a folder that contains multiple Python projects (packages/apps) that are managed together with one top-level configuration. Workspaces are useful when you have: Several related services/packages (e.g. API, worker, common-lib) Shared dependencies / shared internal libraries Want to develop and test them together For example, you can have: llm-platform/ pyproject.toml # workspace root api/ pyproject.toml # FastAPI app worker/ pyproject.toml # queue consumers, tasks common/ pyproject.toml # shared utilities, models And uv understands that my-workspace is a workspace root and each project is a member. ","date":"2025-12-07","objectID":"/drafts/uv-packages/:1:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"Basic structures: Root pyproject.toml At the root, you don‚Äôt define a Python package (you can, but usually it‚Äôs just a config container). Most important is the [tool.uv.workspace] section: [tool.uv.workspace] members = [ \"api\", \"worker\", \"common\", ] This tells uv: ‚Äúthese subdirs are workspace members‚Äù. Members Inside apl/pyproject.toml: [project] name = \"llm-api\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"fastapi\", \"uvicorn[standard]\" ] [project.optional-dependencies] dev = [ \"pytest\", \"httpx\", ] Inside common/pyproject.toml: [project] name = \"llm-common\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"pydantic\", ] Now api can depend on common as a normal package: [project] name = \"llm-api\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"fastapi\", \"uvicorn[standard]\", \"llm-common\", # just use the name ] uv will notice that llm-common is another workspace member. ","date":"2025-12-07","objectID":"/drafts/uv-packages/:1:1","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"Typical commands: To run a command inside a specific member: # Run a FastAPI server defined in api/ uv run --package api uvicorn llm_api.main:app --reload To add a dependency: # Add 'redis' to worker/ only uv add -p worker redis You can lock dependencies for whole workspace by uv lock You can create a member project like this: mkdir api common cd api uv init --package llm_api cd ../common uv init --package llm_common cd .. ","date":"2025-12-07","objectID":"/drafts/uv-packages/:1:2","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"uv sources You‚Äôll mainly see them in pyproject.toml under: [tool.uv.sources] ... That section lets you say: ‚ÄúFor this package, install it from here, not from public PyPI.‚Äù Or ‚ÄúUse this private index URL instead of PyPI.‚Äù This is especially useful in: Company environments with internal PyPI mirrors Air-gapped / proxy-heavy setups When you want a local wheel dir for speed For example, you want my-internal-lib to come from your company index, not public PyPI. [tool.uv.sources] my-internal-lib = { index = \"https://pypi.mycompany.com/simple\" } Then in [project.dependencies] you just write: [project] name = \"my-app\" version = \"0.1.0\" dependencies = [ \"my-internal-lib\", ] In your uv config file (not pyproject.toml), you can set a global index (e.g. corporate mirror): [package-index] default = \"https://pypi.mycompany.com/simple\" ","date":"2025-12-07","objectID":"/drafts/uv-packages/:2:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"with workspace Inside a workspace: [tool.uv.workspace] defines which sub-projects belong to the workspace. [tool.uv.sources] at the root defines how all members should resolve certain dependencies (PyPI vs git vs local path vs another workspace member). [tool.uv.sources] inside a member can override the root rules for that member only. In a workspace you typically have internal packages that you want to import by name, but you do not want to fetch them from PyPI. [project] name = \"albatross\" version = \"0.1.0\" requires-python = \"\u003e=3.12\" dependencies = [\"bird-feeder\", \"tqdm\u003e=4,\u003c5\"] [tool.uv.sources] bird-feeder = { workspace = true } [tool.uv.workspace] members = [\"packages/*\"] albatross depends on a package called bird-feeder. bird-feeder is another workspace member. bird-feeder = { workspace = true } tells uv: ‚ÄúSatisfy this dependency from the workspace, not from PyPI.‚Äù For example: file-translate/ pyproject.toml # workspace root libs/ translate_common/ pyproject.toml services/ api/ pyproject.toml worker/ pyproject.toml We can set pyproject.toml: # file-translate/pyproject.toml (root) [project] name = \"file-translate\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"translate-common\", ] [tool.uv.workspace] members = [\"libs/*\", \"services/*\"] [tool.uv.sources] translate-common = { workspace = true } members = [\"libs/*\", \"services/*\"]: This just tells uv: ‚Äúthere are other projects inside libs/ and services/ ‚Äî manage them together with me as one workspace‚Äù. In services/api/pyproject.toml: [project] name = \"file-translate-api\" version = \"0.1.0\" requires-python = \"\u003e=3.10\" dependencies = [ \"fastapi\", \"uvicorn[standard]\", \"translate-common\", # just normal dependency name ] ","date":"2025-12-07","objectID":"/drafts/uv-packages/:2:1","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["uv","python"],"content":"Hands-on Example ","date":"2025-12-07","objectID":"/drafts/uv-packages/:3:0","tags":["python","workspace","uv","project management","uv workspace","uv tutorial"],"title":"uv workspace: effective management of Python apps","uri":"/drafts/uv-packages/"},{"categories":["machine learning","causality"],"content":"Causal Inference","date":"2025-11-22","objectID":"/causality-1/","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Most of the time when we say ‚Äúmy model learned something,‚Äù what it actually learned is a bunch of very smart correlations. If users who click A also tend to click B, or if certain pixels tend to appear together in cat photos, our models will happily latch onto those patterns and exploit them. That‚Äôs powerful‚Äîand often enough for prediction‚Äîbut it‚Äôs not the same as understanding what would happen if we actually changed something in the world: raised a price, changed a policy, or shipped a new feature. This blog post is about that gap. We‚Äôll look at how correlation-based learning differs from causal reasoning, why most ML lives firmly on the correlation side, and how ideas from causal inference help us talk more clearly about actions, interventions, and ‚Äúwhat-if‚Äù questions. ","date":"2025-11-22","objectID":"/causality-1/:0:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Correlation vs Causation Before we talk about causal inference, it is useful to clarify a basic but crucial distinction: correlation versus causation. ","date":"2025-11-22","objectID":"/causality-1/:1:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Correlation: how things move together Two variables are said to be correlated if they tend to move (or change) together in the data: When one variable is high, the other also tends to be high, or When one is high, the other tends to be low. Formally, we can measure this with quantities such as covariance or correlation coefficients. Intuitively, correlation captures how much two variables vary in the data we observe. A key point is that correlation is purely about patterns in the data, not about underlying mechanisms. If ice-cream sales and drowning accidents go up together in summer, they are correlated; but it would be absurd to say that buying ice cream causes drowning. Both are driven by a third factor: the weather. ","date":"2025-11-22","objectID":"/causality-1/:1:1","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Causation: how one variable affects another By contrast, causation is about what happens to one variable if we change another variable: If we force $X$ to change, how does $Y$ respond (on average)? Here we are interested in the exact impact of one variable on another under interventions, not just how they happened to move together in the past. Causal questions are of the form: What is the effect of raising the legal driving age on traffic accidents? What is the effect of a new drug on blood pressure? What is the effect of a new recommendation algorithm on user engagement? These questions all ask about the consequences of a hypothetical action, not just about existing correlations. ","date":"2025-11-22","objectID":"/causality-1/:1:2","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"What most machine learning actually learns Most standard machine learning algorithms (e.g., linear regression, logistic regression, random forests, neural networks, transformers, and so on) are trained to learn patterns like $$P(Y \\mid X) \\quad \\text{or} \\quad f(x) \\approx \\mathbb{E}[Y \\mid X = x],$$ that is, they learn to predict $Y$ from $X$ based on observed co-variation in the data. In other words: They learn correlations (or more generally, statistical dependencies) between inputs and outputs. They do not, by default, distinguish whether those dependencies are causal or simply due to confounding factors. We can think of standard ML as correlation-based learning: It focuses on learning how variables change together in the data rather than understanding the underlying ‚Äúwhy‚Äù, It focuses not on what would happen if we actively intervened and changed one variable while holding everything else fixed. This distinction is crucial: A model that is excellent at exploiting correlations may be very good at prediction in environments similar to its training data, but it can fail badly when we change the environment or policy, because it does not know the underlying causal structure. Causal inference, by contrast, aims precisely at learning (or using assumptions about) the causal impact of one variable on another. In short, correlation-based learning is about capturing how variables change together in the observed world, while causal inference aims to characterize the direct impact of changing one variable on another under a specified intervention. ","date":"2025-11-22","objectID":"/causality-1/:1:3","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"From correlation to intervention In the rest of this post, we will move from correlation-based questions to truly causal ones. To do this, we will introduce the distinction between: Observations: passively watching the world as it is, and Actions (interventions): asking what would happen if we changed something in the world. This will lead naturally to the language of causal reasoning, and to examples ‚Äî such as driving-age policies ‚Äî that illustrate why correlation alone is not enough. ","date":"2025-11-22","objectID":"/causality-1/:1:4","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Observations vs Actions Let‚Äôs start with the difference between just watching the world and actually changing something in it. When we passively observe, we simply watch how people normally behave, following their habits, routines, and preferences. The data we collect in this way are like a snapshot of the world as it currently is, summarized in whatever features we decided to record (e.g., age, income, accident history). This kind of data is called observational data. So what can we answer with observational data? Some questions fit perfectly into this observational world. For example: Example Do 16-year-old drivers have more traffic accidents than 18-year-old drivers? This is a question about how often something happens in the world as it is. Mathematically, we can compute: the (conditional) probability of an accident given that the driver is 16. the (conditional) probability of an accident given that the driver is 18. Then compare those two numbers (e.g., subtract them). If we have a large enough sample with both 16- and 18-year-old drivers, we can estimate these probabilities directly from the data using standard observational statistics. Now consider a slightly different question: What would happen to traffic fatalities if we raised the legal driving age by two years? This sounds similar, but it‚Äôs actually a different type of question. The previous question was about how often something happens in the current world. This new question is about what would happen if we changed the rules of the world. That is, it‚Äôs asking about the effect of a hypothetical action (an intervention): ‚ÄúRaise the driving age -\u003e what changes?‚Äù You can‚Äôt answer that reliably just by looking at current accident rates by age, because: Maybe older drivers crash less simply because they have more experience, not because they‚Äôre older. An 18-year-old with only 2 months of driving experience might be no safer than a 16-year-old with 2 months of experience. We could try to control for experience; for example, we can compare accident rates for people with the same number of months of driving, but different ages. However, even then, we run into complications: What if 18-year-olds with two months of driving experience tend to be exceptionally cautious, but become less cautious as they gain more experience? Maybe they tend to live in areas where people do not need to start driving until later in life. So even when you try to adjust for obvious factors like months of experience, other hidden differences can still mess up your conclusions. We might try another strategy: compare countries with different legal driving ages, like the US and Germany. These countries differ in many ways besides driving age: Public transport, culture, enforcement, and even laws, like the legal drinking age. So differences in accident rates could be caused by any of those factors, not just the driving age. This is where causal reasoning comes in. Causal reasoning is a framework ‚Äî both conceptual and technical ‚Äî for answering questions like: What is the effect of doing $X$? (e.g., raising the driving age) What action caused $Y$? (e.g., what policy likely reduced accidents?) It focuses on interventions: Not just what is the world like? But what would the world look like if we changed something? Once we understand how to define and estimate the effect of an action, we can turn questions around and ask: Given that we observed this outcome, what actions or causes are likely responsible? To formalize the concept, let‚Äôs see how the limits of pure observation show up in real data. ","date":"2025-11-22","objectID":"/causality-1/:2:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"The limitations of observation In 1973, researchers looked at graduate school admissions at the University of California, Berkeley. They had data on 12,763 applicants across 101 departments and programs. About 4,321 of these applicants were women, and roughly 35% of them were admitted. About 8,442 were men, and around 44% of them were admitted. Just looking at these totals, it seems like men were more likely to be admitted than women (44% vs 35%). Standard statistical tests say this difference is too big to be explained by random chance alone, so it looks like a real gap. The same pattern appeared when the researchers focused on the six largest departments: Across these six, men again had an overall acceptance rate of about 44%, while women had an overall rate of about 30%. Again, this suggests that men are doing better than women when you look at the data in aggregate (all combined). However, each department decides who to admit by itself, and departments can be very different ‚Äî different fields, different standards, different competitiveness. So the researchers drilled down and looked at the acceptance rates within each of those six big departments. What they found was surprising: In four of the six departments, women actually had a higher acceptance rate than men. In the other two departments, men had the higher acceptance rate. But those two departments weren‚Äôt big enough or different enough to explain the large overall gap in the combined data. We can find a reversal: Overall across departments: men seem to be favored. Inside most departments: women do as well or better than men. This is an example of what‚Äôs often called Simpson‚Äôs paradox: a situation where a pattern that appears in overall (aggregate) data reverses when you break the data into subgroups. In this case: Event $Y$: the applicant is accepted. Event $A$: the applicant is female (gender treated as a binary variable). Variable $Z$: which department the applicant applied to. Simpson‚Äôs paradox means it can happen that: For each department ($Z$), women might do as well as or better than men: $$P(\\text{accepted} \\mid \\text{female}, Z) \\ge P(\\text{accepted} \\mid \\text{male}, Z),$$ but Overall, women still have a lower acceptance rate than men: $$P(\\text{accepted} \\mid \\text{female}) \u003c P(\\text{accepted} \\mid \\text{male}).$$ This happens because men and women apply to different departments in different proportions, and those departments have different levels of competitiveness. From the data, one thing is very clear: Gender affects which departments people apply to. Men and women have different patterns of department choice. We also know that departments differ in how hard it is to get in. Some have low acceptance rates (very competitive), others have higher acceptance rates. So one plausible explanation is: Women tended to apply more to highly competitive departments, while men applied more to less competitive ones. As a result, women were rejected more often overall, even though departments themselves may have treated individual male and female applicants fairly. This was essentially the conclusion of the original study. They argued: The bias seen in the combined statistics does not come from admissions committees systematically discriminating against women. Instead, it comes from earlier stages in the pipeline: the way society and the education system have steered women toward certain fields. They suggested that women were shunted by their upbringing and education into fields that: are more crowded, have fewer resources and funding, have lower completion rates, and often lead to poorer job prospects. In other words, they said the gender bias was mainly a pipeline problem1: by the time women reached graduate applications, they were already concentrated in less favorable, more competitive fields, through no fault of the departments themselves. However, it‚Äôs hard to fully defend or criticize that conclusion using this data alone, because key information is missing. Fo","date":"2025-11-22","objectID":"/causality-1/:3:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Causal models as a bridge between data and assumptions Up to this point, we have seen two recurring themes: Observational data alone can be ambiguous: the same pattern can be explained by multiple causal stories. Policy questions (‚Äúwhat if we changed $X$?‚Äù) are fundamentally about interventions, not just about correlations. Causal models provide a way to organize both of these issues. They give us: A way to distinguish between observing and doing, A structured way to encode assumptions about how variables influence one another, A toolkit to derive the effect of hypothetical actions from observational data when possible. ","date":"2025-11-22","objectID":"/causality-1/:4:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Observation vs intervention in notation In ordinary statistics, we are used to working with conditional probabilities of the form \\begin{align*} P(Y \\mid X = x), \\end{align*} which answer questions of the form: Among the individuals for whom we see $X = x$, how often do we see $Y$? For example, $P(\\text{accident} \\mid \\text{age}=16)$ is the accident rate among all drivers who happen to be 16 in our data. Causal questions, by contrast, refer to the effect of an action. For this we introduce the notation \\begin{align*} P(Y \\mid \\operatorname{do}(X = x)), \\end{align*} which answers: If we were to force $X$ to take the value $x$ (by intervention), how often would we see $Y$? In the driving-age example, $P(\\text{accident} \\mid \\operatorname{do}(\\text{min.\\ age}=18))$ describes the accident rate we would expect under a policy that sets the legal driving age to 18, taking into account all downstream changes (e.g., who drives, when they start, how much experience they accumulate). In general, \\begin{align*} P(Y \\mid X = x) \\neq P(Y \\mid \\operatorname{do}(X = x)), \\end{align*} since individuals who happen to have $X = x$ in the observational world may differ in systematic ways from those who would end up with $X = x$ under an intervention. This discrepancy is precisely what we saw in the Berkeley example and the driving-age discussion: simple conditioning on (X) mixes together many other influences. ","date":"2025-11-22","objectID":"/causality-1/:4:1","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Causal diagrams (informally) One convenient way to encode assumptions about how variables influence each other is via causal diagrams, also called directed acyclic graphs (DAGs). For the driving-age example, a highly simplified diagram might look like: Law ‚Üí Age at start ‚Üí Experience ‚Üí Accidents | / | | / | | / | Personality Region Read informally, this says: The Law constrains the age at which people can start driving. Age at start influences how much Experience they can accumulate. Experience influences the risk of Accidents. Personality (cautious vs reckless) affects both how much Experience people seek and their risk of Accidents. Region (urban vs rural, road conditions, etc.) also affects Accidents. This kind of diagram helps us see why simply comparing $P(\\text{Accidents} \\mid \\text{Age})$ can be misleading: Age is entangled with Experience, Personality, and Region. Changing the Law changes Age, which in turn changes Experience and who drives at all. Causal calculus (Pearl‚Äôs do-calculus) tells us when and how we can recover $P(Y \\mid \\operatorname{do}(X=x))$ from purely observational data given such a diagram, and when we cannot. ","date":"2025-11-22","objectID":"/causality-1/:4:2","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Designing better studies Causal models are not only a tool for interpreting existing data; they also guide the design of new studies. Given a causal diagram, we can ask: Which variables must we measure in order to block spurious associations (confounding paths)? Which variables should we not condition on, because they may introduce new biases (colliders)? Under what conditions can we estimate causal effects from purely observational data? In the Berkeley example, a causal model could tell us that we are missing key variables, such as: Applicant qualifications (grades, test scores, research experience), Department climate or reputation for treating women, Broader social forces shaping field choice. The model would then make explicit that, without additional measurements or assumptions, certain causal questions (e.g., ‚ÄúWas there discrimination at the department level?‚Äù) cannot be answered definitively from the existing data. ","date":"2025-11-22","objectID":"/causality-1/:4:3","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Reasoning with incomplete data Even when we cannot collect more data, causal models still have value. They allow us to: Enumerate different plausible causal stories consistent with the observed data, Explore the implications of each story (‚ÄúIf the world worked this way, what would the effect of changing (X) be?‚Äù), Identify which assumptions are doing the real work in our conclusions. In other words, a causal model acts as a logic engine for cause-and-effect reasoning: given explicit assumptions, it yields explicit conclusions, and it clarifies which parts of our reasoning depend on empirical evidence and which parts depend on judgment and prior knowledge. ","date":"2025-11-22","objectID":"/causality-1/:4:4","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["machine learning","causality"],"content":"Summary We can now summarize the main points: Observational data show us how the world is, not automatically how it would be if we changed something. Questions about policies or interventions (raising the driving age, changing admissions rules, etc.) are inherently causal and require reasoning about $\\operatorname{do}(\\cdot)$, not just conditional probabilities. The Berkeley admissions example illustrates how aggregated patterns can reverse when we condition on a relevant variable (department), a phenomenon known as Simpson‚Äôs paradox. Observational patterns alone can be compatible with multiple causal explanations (pipeline effects, discrimination at various stages, or both); data without a causal model cannot fully resolve such ambiguities. Causal models provide a structured way to: Distinguish observation from intervention, Design better studies and decide which variables to measure or control, Connect domain knowledge and assumptions to concrete, testable implications. In this sense, causal inference is not just an add-on to statistics but a complementary framework that lets us talk rigorously about actions, policies, and the mechanisms that generate the data we see. Reference Patterns, predictions, and actions: A story about machine learning, Moritz Hardt, Benjamin Recht, 2021 This term describes a situation where the number of people, particularly women, decreases at each successive stage of a career pipeline, leading to a significant underrepresentation at the top levels.¬†‚Ü©Ô∏é ","date":"2025-11-22","objectID":"/causality-1/:5:0","tags":["machine learning","causality","causal inference"],"title":"Causal Inference Part 1: Causation and Correlation","uri":"/causality-1/"},{"categories":["study notes"],"content":"My NLP and LLM study note","date":"2025-10-07","objectID":"/studynotes/llm-notes/","tags":["deep learning","LLM","NLP"],"title":"NLP and LLM Study Note","uri":"/studynotes/llm-notes/"},{"categories":["study notes"],"content":"NLP and LLM üëâ Repository: NLP Note These are my working notes on NLP and large language models: intuitive math, minimal proofs, and practical recipes. I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. ","date":"2025-10-07","objectID":"/studynotes/llm-notes/:0:0","tags":["deep learning","LLM","NLP"],"title":"NLP and LLM Study Note","uri":"/studynotes/llm-notes/"},{"categories":["LLM Proxy server","LiteLLM","LLM"],"content":"A tutorial for llm proxy server and litellm","date":"2025-09-18","objectID":"/litellm/","tags":["LLM Proxy server","LiteLLM","AI","LLM"],"title":"LiteLLM: LLM Proxy Server","uri":"/litellm/"},{"categories":["LLM Proxy server","LiteLLM","LLM"],"content":"Why an LLM proxy at all? An LLM proxy sits between your app and model providers (OpenAI, Anthropic, Google, Ollama, etc.). It gives you a unified API (usually OpenAI-compatible), centralized auth, usage controls (budgets / rate-limits), routing and fallbacks, and caching‚Äîwithout changing your application code for each vendor. ","date":"2025-09-18","objectID":"/litellm/:1:0","tags":["LLM Proxy server","LiteLLM","AI","LLM"],"title":"LiteLLM: LLM Proxy Server","uri":"/litellm/"},{"categories":["LLM Proxy server","LiteLLM","LLM"],"content":"What is LiteLLM? LiteLLM is an OpenAI-compatible LLM Gateway that lets you call 100+ providers behind one API, plus adds budgets/rate-limits, model access control, caching, routing, admin UI, and more. You can run it as a single Docker container with a YAML config. An LLM Proxy is a service that sits between your application and the LLM provider‚Äôs API. ","date":"2025-09-18","objectID":"/litellm/:2:0","tags":["LLM Proxy server","LiteLLM","AI","LLM"],"title":"LiteLLM: LLM Proxy Server","uri":"/litellm/"},{"categories":["Docker","DevOps","Programming"],"content":"A Docker tutorial.","date":"2025-09-13","objectID":"/docker-basics/","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"This is part of my Docker Basics series ‚Äî introductory guides to help you get started with Docker, learn key concepts, and build your skills step by step. Part 1: Understanding Container Part 2: Basic Commands Part 3: Dockerfile Part 4: Networks Docker Fundamentals (Part 1) Software systems frequently exhibit environment-dependent behavior: dependency versions drift, filesystem paths diverge, and minor operating-system differences produce major failures. Containerization addresses this by packaging an application together with its runtime dependencies so that a single artifact executes consistently across development laptops, continuous-integration pipelines, and production clusters. Formally: same package $\\rightarrow$ same behavior across environments. ","date":"2025-09-13","objectID":"/docker-basics/:0:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"A Minimal Example: To build intuition, consider the following command: docker run hello-world At high level, docker run performs the following steps: Pull the image: If the specified image is not available locally, Docker pulls it from the configured container registry (e.g., Docker Hub). Create a container: Docker creates a new container from the specified image. Allocate resources: Docker allocates necessary resources for the container (e.g., network, storage). Start the container: Docker starts the container, running the specified command within it. Attach the container: If specified, Docker attaches your terminal to the container‚Äôs input, output, and error streams. Key terms: Docker images are the blueprints, then containers are the actual buildings ","date":"2025-09-13","objectID":"/docker-basics/:0:1","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Virtualization (the classic approach) Before diving into Docker, let‚Äôs first contrast it with the traditional isolation model: virtual machines (VMs). Virtualization is a technique of running a complete simulated computer within another computer. The simulated computer‚Äîcalled a virtual machine (VM)‚Äîemulates the components of a physical system: CPU, memory, motherboard/firmware, disks, and peripheral buses. A hypervisor (the software) manages one or more VMs on a host (the real machine). Complete: It mimics everything a physical computer would have: CPU, memory, motherboard, BIOS, disks, USB ports, and so on. Simulated: The machine exists entirely in software. It doesn‚Äôt physically exist, which is why it‚Äôs called virtual. Virtualization is a great fit when you need: A full operating system distinct from the host (e.g., Windows on a Linux workstation); Strong isolation for testing in a controlled environment; Execution of applications unavailable on the host OS; or Faithful reproduction of production-like environments for development and debugging. ","date":"2025-09-13","objectID":"/docker-basics/:1:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Containerization (the modern approach) A Docker container is like a shipping container for software‚Äîa self-contained ‚Äúbox‚Äù that carries an application and just what it needs to run. From inside, the app feels as if it has a whole computer to itself: its own hostname/IP, its own filesystem ‚Äúdrive‚Äù, and its own process space. Those are virtual resources that Docker creates and manages. Processes inside the box can‚Äôt see beyond it, while the box runs on a real machine that can host many boxes at once. Each container has its own isolated environment, yet all containers share the host‚Äôs CPU, memory, and kernel (on macOS/Windows, Docker supplies a lightweight Linux VM). A container isolates an application and its immediate runtime; it does not simulate a complete computer. Processes execute on the host kernel with restricted views of the filesystem, process table, and network, and the image includes only the user-space libraries the application requires. Key properties: Lightweight, single-process by design: Containers start fast (no firmware checks or OS boot) and images stay compact by excluding extras. Minimal dependencies: Each container bundles only the libraries it needs; treat containers as disposable‚Äîrebuild and redeploy instead of patching in place. Host-managed hardware: The host OS handles CPU, memory, devices, and peripherals. Not a full OS: Containers don‚Äôt pretend to be virtual machines; processes can detect they‚Äôre containerized. Limitations No hardware emulation (e.g., you can‚Äôt test a device driver). Container filesystems are ephemeral; persistent data should live in volumes or external storage attached at runtime. Benefits Extremely small footprint (often just a few MB) and only the memory the process needs. Reproducible environments, frequent safe releases, and rapid horizontal scaling. ","date":"2025-09-13","objectID":"/docker-basics/:2:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Docker and its Ecosystem Docker Hub is a cloud-based registry where you can find and share Docker images_. It contains a vast collection of images, ranging from official base images like Ubuntu and Node.js to custom images created by the community. Docker is the most popular container platform, largely because of the ecosystem it created. A Docker image is essentially: A compact archive with all binaries, libraries, and minimal configuration needed to run an application. Stateless and easily shareable. Docker Hub provides: A public registry for storing and discovering images. Command-line and web tools for uploading, searching, rating, and giving feedback. This ecosystem made it easy to share and reuse container images, accelerating container adoption across the industry. Anatomy of Docker Docker is a packaging and runtime system for applications. Docker is made up of several core components: Command-line utility (docker) Host machine Objects (images, containers, volumes, networks) Registries (where images are stored and shared) Let‚Äôs take a look one by one ","date":"2025-09-13","objectID":"/docker-basics/:2:1","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Docker The Docker CLI tool ‚Äì docker The Docker CLI is your front door to the platform. The CLI communicates with the Docker host through an API exposed by the daemon (dockerd) to manage containers and images. The docker command is the primary way to manage containers and images. With docker, you can Build images. Pull images from registries. Push images to registries. Run and manage containers. Set runtime options. Remove containers and images. ","date":"2025-09-13","objectID":"/docker-basics/:3:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Docker Host \u0026 Daemon (dockerd) dockerd is the long-running engine that does the real work: it stores images, creates and supervises containers, wires up networks and volumes, and serves the Docker API. Internally it leans on standards-based runtimes (containerd/runc) to execute containers reliably. The host machine runs the Docker daemon (dockerd). Responsibilities of dockerd: Stores images locally. Creates and manages containers. Provides resources such as networking and volumes. Offers an API for interacting with containers and images. Maintains metadata (container state, logs, configs). Handles communication in Docker Swarm mode. Think of dockerd (daemon) as the engine that powers all Docker operations. ","date":"2025-09-13","objectID":"/docker-basics/:4:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"What is an image? An image is an immutable, layered snapshot of an app‚Äôs user-space: binaries, libs, and config, plus metadata like entrypoint and exposed ports (everything needed to run an application). Built from a Dockerfile and identified by a content hash and tags, one image can spawn many identical containers. Characteristics: Immutable: once built, it doesn‚Äôt change. Reusable: can spawn multiple containers. Identified by a unique hash (SHA-256) and often tagged with human-readable labels. Built using a Dockerfile, which specifies: Base image. Packages to install. Files to copy. Commands to run. Workflow: docker build: reads instructions and creates image layers. docker run: starts a container from the image with a chosen process. If the primary process ends, the container stops. ","date":"2025-09-13","objectID":"/docker-basics/:5:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"cgroups cgroups (control groups) are a Linux kernel feature that lets you manage and isolate system resources for groups of processes. With cgroups, an administrator can: Allocate CPU, memory, and network bandwidth. Monitor resource usage. Enforce limits to prevent one workload from starving others. This makes cgroups ideal for isolating different workloads on a shared system. ","date":"2025-09-13","objectID":"/docker-basics/:6:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"Docker and cgroups By default, Docker does not restrict CPU or memory usage for containers ‚Äî a process can consume as much as the host allows. However, Docker integrates directly with cgroups, making it easy to apply resource limits without manually configuring kernel settings. Limits can be applied at container startup with flags in docker run. You can limit the amount of memory a Docker container can use by using the --memory or -m option with the docker run command. For example, use the following to run the alpine image with a memory limit of 500 MB: docker run --memory 500m alpine /bin/sh You can specify the memory limit in bytes, kilobytes, megabytes, or gigabytes by using the appropriate suffix (b, k, m, or g). ","date":"2025-09-13","objectID":"/docker-basics/:6:1","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"References The Linux DevOps Handbook, Damian Wojs≈Çaw and Grzegorz Adamowicz ","date":"2025-09-13","objectID":"/docker-basics/:7:0","tags":["Docker","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 1: Basics","uri":"/docker-basics/"},{"categories":["Docker","DevOps","Programming"],"content":"A Docker tutorial. ","date":"2025-09-13","objectID":"/docker-commands/","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"This is part of my Docker Basics series ‚Äî introductory guides to help you get started with Docker, learn key concepts, and build your skills step by step. Part 1: Understanding Container Part 2: Basic Commands Part 3: Dockerfile Part 4: Networks ","date":"2025-09-13","objectID":"/docker-commands/:0:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Common Commands This is a no-frills cheat sheet for the Docker commands you‚Äôll reach for most of the time, with tiny runnable examples you can copy/paste. The most common commands you can use are the following: build: This allows you to build a new Docker image using a Dockerfile run: This starts a new container start: This restarts one or more stopped containers stop: This will stop one or more running containers login: This is used to gain access to private registries pull: This downloads an image or a repository from a registry push: This uploads an image or a repository to a registry images: This lists all images on your machine ps: This lists all running containers exec: This executes a command in a running container logs: This shows the logs of a container network: This is used to manage Docker networks volume: This is used to manage volumes ","date":"2025-09-13","objectID":"/docker-commands/:1:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker build docker build turns a Dockerfile + build context into an image. docker build [OPTIONS] PATH | URL | - PATH is the path to the directory containing the Dockerfile. URL is the URL to a Git repository containing the Dockerfile. - (a dash) is used to build an image from the contents of stdin, so you could pipe Dockerfile content to it from the output of some previous command that would build a Dockerfile, for example, generate it from a template. To build from the current directory: docker build . You can also use a specific tag for the build image: docker build -t my-image:1.0 . -t my-image:1.0: This assigns the tag my-image:1.0 to the image you‚Äôre building. You can pass build-time variables by --build-arg docker build --build-arg VAR1=value1 -t my-image:1.0 . ","date":"2025-09-13","objectID":"/docker-commands/:2:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker run When you run a container, you‚Äôre taking a Docker image (a snapshot blueprint with all the software and configuration baked in) and launching a process inside it. An image is read-only. Think of it as the recipe. A container is a running instance of that image, with its own writable layer and runtime state. The docker run command starts a new container from an image. For example: docker run myimage /bin/bash This starts a container based on myimage. Once inside, you‚Äôre running a bash shell inside the container‚Äôs isolated environment. You can pass additional options to control how the container behaves. Example: docker run -d -p 8080:80 --name mynginx nginx Docker downloads the official nginx image if not present. Starts it in the background (in detached mode) (-d). Maps port 80 in the container to port 8080 on the host (-p 8080:80), You can now visit http://localhost:8080 in your browser. The container has the name mynginx, so you can later run: docker logs mynginx # see what's happening inside docker stop mynginx # stop it docker start mynginx # start it again docker exec -it mynginx bash # Run a shell inside the container # Run a shell inside the container ","date":"2025-09-13","objectID":"/docker-commands/:3:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"environment variables You can also pass environment variables to the container: docker run -e VAR1=value1 -e VAR2=value2 myimage:latest Set environment variables inside the container. Inside the container, any process can read them like: echo $VAR1 # prints value1 echo $VAR2 # prints value2 ","date":"2025-09-13","objectID":"/docker-commands/:3:1","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker volume By default, Docker containers are ephemeral ‚Äî once a container is stopped and removed, all data stored inside its writable layer is lost. To persist data beyond the lifecycle of a container, you need to store it outside the container‚Äôs filesystem. Docker provides two main ways to do this: Volumes: storage fully managed by Docker. Bind mounts: directly map a host directory into a container. ","date":"2025-09-13","objectID":"/docker-commands/:4:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Docker Volume A volume is storage created and managed by Docker. Volumes live under Docker‚Äôs data directory, usually: /var/lib/docker/volumes/ Volumes are independent of containers, meaning you can delete and recreate containers without losing data. Multiple containers can share the same volume. You can create a volume by docker volume create myvolume To start a container with that volume mounted: docker run -it --name test1 -v myvolume:/data busybox myvolume: the name of the Docker-managed volume /data: mount point inside the container Inside the container: echo \"hello world\" \u003e /data/hello.txt Exit and remove the container: exit docker rm -f test1 Now run another container with the same volume: docker run -it --name test2 -v myvolume:/data busybox cat /data/hello.txt You‚Äôll see hello world persisted across containers. ","date":"2025-09-13","objectID":"/docker-commands/:4:1","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Bind Mount (using a host directory) A bind mount links a folder on your host machine directly into the container. Useful for development or when you need full control over where data lives. This is very useful for development (live-editing code on your host and seeing changes inside the container). Unlike volumes, you control the exact path where the data is stored. docker run -it --name test3 -v /home/admin/data:/app/data busybox /home/admin/data: directory on your host /app/data: directory inside the container Anything written to /app/data inside the container appears in /home/admin/data on the host. ","date":"2025-09-13","objectID":"/docker-commands/:4:2","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Inspecting Volumes List volumes: docker volume ls Inspect a specific volume: docker volume inspect myvolume Remove a volume (when no container uses it): docker volume rm myvolume ","date":"2025-09-13","objectID":"/docker-commands/:4:3","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker start The docker start command is used to restart containers that were previously created but are currently stopped. Unlike docker run, which creates a new container from an image, docker start simply restarts an existing container. This means the container‚Äôs state (its filesystem, volumes, and configuration) is preserved. docker start mycontainer docker start mycontainer othercontainer lastcontainer ## To stop multiple containers Here, mycontainer, othercontainer, and lastcontainer can be either container names or IDs. You can check all running and stopped containers using the docker ps or docker ps -a (all running + stopped containers) command. By default, docker start runs containers in the background (detached). If you want to see the logs and interact with the process, add the -a flag: docker start -a mycontainer ","date":"2025-09-13","objectID":"/docker-commands/:5:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker stop This command is used to stop containers running in the background. docker stop mycontainer docker stop mycontainer othercontainer lastcontainer By default, Docker sends a SIGTERM signal to allow the process inside the container to exit gracefully. If the process does not exit within 10 seconds, Docker sends a SIGKILL to force termination. You can change the timeout with -t: docker stop -t 10 mycontainer This waits up to 10 seconds before forcing a kill. ","date":"2025-09-13","objectID":"/docker-commands/:6:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker ps This command is used to list the running or stopped containers. When you run the docker ps command without any options, it will show you the list of running containers along with their container ID, names, image, command, created time, and status: docker ps docker ps -a # To view all containers (running + stopped) You can use the --quiet or -q option to display only the container IDs, which might be useful for scripting: docker ps --quiet There are some useful options: -q, --quiet: show only container IDs (handy in scripts) -f, --filter: filter by conditions (e.g., name, status, ancestor image) docker ps --filter \"name=nginx\" -s: show container disk usage (size of writable layer) docker ps -s ","date":"2025-09-13","objectID":"/docker-commands/:7:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker login The docker login command is used to log in to a Docker registry. A registry is a place where you can store and distribute Docker images. The default registry is Docker Hub, but you can specify a custom registry. docker login quay.io To avoid typing passwords interactively (safer for automation), you can use the --password-stdin or -P option to pass your password via stdin: echo \"mypassword\" | docker login --username myusername --password-stdin ","date":"2025-09-13","objectID":"/docker-commands/:8:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker pull To pull a Docker image, you can use the docker pull \u003cimage-name\u003e:\u003ctag\u003e. By default, pull will pull an image with a tag latest. For example, docker pull alpine To pull a specific version of the alpine image, docker pull alpine:3.12 You can also pull an image from a different registry by specifying the registry URL in the image name. docker pull myregistry.com/myimage:latest ","date":"2025-09-13","objectID":"/docker-commands/:9:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker push By default, push will try to upload an image to the Docker Hub registry: docker push myimage Typically, you tag your image first: docker tag myimage:latest myimage:1.0 docker push myimage:1.0 docker tag: Creates an alias (new tag) for an existing image. myimage:latest: The source image. This is the local image you already built (with tag latest). myimage:1.0: The new tag (alias). ","date":"2025-09-13","objectID":"/docker-commands/:10:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker image To list available images on your machine, you can use the docker image ls command: docker image ls To pull an image from the Docker registry, docker image pull ubuntu To build an image: docker image build -t \u003cimage_name\u003e . To create a tag for an image docker image tag \u003cimage\u003e \u003cnew_img_name\u003e To remove, docker image rm \u003cimage\u003e ","date":"2025-09-13","objectID":"/docker-commands/:11:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Save Docker Image as a File To save, docker save -o \u003cfile-name\u003e.tar \u003cdocker-image\u003e To load, docker load -i \u003cfile-name\u003e.tar ","date":"2025-09-13","objectID":"/docker-commands/:11:1","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"Dangling images A dangling image is an image that: Has no tag (shows up as \u003cnone\u003e in docker images). Is not referenced by any container. They‚Äôre usually created when: You build a new image with the same tag ‚Äî the old image becomes untagged. A build fails or is interrupted, leaving partial image layers. For instance, $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE myapp latest 1a2b3c4d5e6f 2 minutes ago 130MB \u003cnone\u003e \u003cnone\u003e 7g8h9i0j1k2l 3 minutes ago 129MB # dangling image They waste disk space but are safe to delete. You can list dangling images by docker images -f dangling=true This shows all \u003cnone\u003e images. To remove dangling images (unused images): docker image prune This command will remove all unused images (dangling images). Docker will ask for confirmation. Add -f to skip the prompt: docker image prune -f Remove all unused images (not just dangling ones): docker image prune -a If you want to remove everything unused (e.g., containers, networks, images, build cache) docker system prune ","date":"2025-09-13","objectID":"/docker-commands/:11:2","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker exec docker exec allows you to run a command in a running Docker container. For example, docker exec mycontainer ls You can run a interactive shell: docker exec -it mycontainer bash ","date":"2025-09-13","objectID":"/docker-commands/:12:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"docker logs docker logs is used to view logs generated by a Docker container: docker logs CONTAINER_NAME_OR_ID Additional options you can pass to the command are as follows: --details, -a: Show extra details provided to logs --follow, -f: Follow log output --since, -t: Only display logs since a certain date (e.g., 2013-01-02T13:23:37) --tail, -t: Number of lines to show from the end of the logs (default all) For example, docker logs -f --tail 50 mycontainer ","date":"2025-09-13","objectID":"/docker-commands/:13:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"References The Linux DevOps Handbook, Damian Wojs≈Çaw and Grzegorz Adamowicz ","date":"2025-09-13","objectID":"/docker-commands/:14:0","tags":["Docker","Containers","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 2: Basic Commands","uri":"/docker-commands/"},{"categories":["Docker","DevOps","Programming"],"content":"A Docker tutorial.","date":"2025-09-13","objectID":"/docker-dockerfile/","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"This is part of my Docker Basics series ‚Äî introductory guides to help you get started with Docker, learn key concepts, and build your skills step by step. Part 1: Understanding Container Part 2: Basic Commands Part 3: Dockerfile Part 4: Networks ","date":"2025-09-13","objectID":"/docker-dockerfile/:0:0","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Basic Commands A Dockerfile is essentially a text file with a predetermined structure that contains a set of instructions for building a Docker image. The instructions in the Dockerfile specify what base image to start with (for example, Ubuntu 20.04), what software to install, and how to configure the image. The purpose of a Dockerfile is to automate the process of building a Docker image so that the image can be easily reproduced and distributed. The structure of a Dockerfile is a list of commands (one per line) that Docker (containerd to be exact) uses to build an image. Each command creates a new layer in the image in UnionFS, and the resulting image is the union of all the layers. The fewer layers we manage to create, the smaller the resulting image. The most frequently used commands in a Dockerfile are the following: FROM COPY ADD EXPOSE CMD ENTRYPOINT RUN LABEL ENV ARG VOLUME USER WORKDIR ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:0","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"FROM A Dockerfile starts with a FROM command, which specifies the base image to start with: FROM ubuntu:20.04 You can also name this build using as keyword followed by a custom name: FROM ubuntu:20.04 as builder1 docker build will try to download Docker images from the public Docker Hub registry, but it‚Äôs also possible to use other registries out there, or a private one. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:1","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"COPY and ADD The COPY command is used to copy files or directories from the host machine to the container file system. Take the following example: COPY . /var/www/html You can also use the ADD command to add files or directories to your Docker image. ADD has additional functionality beyond COPY. It can extract a TAR archive file automatically and check for the presence of a URL in the source field, and if it finds one, it will download the file from the URL. Finally, the ADD command has a --chown option to set the ownership of the files in the destination. In general, it is recommended to use COPY in most cases, and only use ADD when the additional functionality it provides is needed. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:2","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"EXPOSE The EXPOSE command in a Dockerfile informs Docker that the container listens on the specified network ports at runtime. It does not actually publish the ports. It is used to provide information to the user about which ports are intended to be published by the container. For example, if a container runs a web server on port 80, you would include the following line in your Dockerfile: EXPOSE 80 You can specify whether the port listens on TCP or UDP ‚Äì after specifying the port number, add a slash and a TCP or UDP keyword (for example, EXPOSE 80/udp). The default is TCP if you specify only a port number. The EXPOSE command does not publish the ports. To make ports available, you will need to publish them with the use of the -p or --publish option when running the docker run command: docker run -p 8080:80 thedockerimagename:tag This will map port 8080 on the host machine to port 80 in the container so that any incoming traffic on port 8080 will be forwarded to the web server running in the container on port 80. Regardless of the EXPOSE command, you can publish different ports when running a container. EXPOSE is used to inform the user about which ports are intended to be published by the container. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:3","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"ENTRYPOINT and CMD The ENTRYPOINT instruction defines the command that will always be executed when the container starts. It essentially turns the container into an executable that behaves like a binary or script. Unlike CMD, which can be fully replaced by arguments at runtime, ENTRYPOINT is fixed, and arguments you pass with docker run are simply appended to it. You usually use ENTRYPOINT when you want your container to act like a single-purpose tool. ENTRYPOINT [\"curl\"] Running docker run mycurl https://example.com executescurl https://example.com docker run: start a new container. mycurl: the image name. https://example.com: arguments passed to the container. Docker does the following internally: Looks at the image mycurl. Sees that the ENTRYPOINT is set to [\"curl\"]. Appends your command-line arguments (https://example.com) to the ENTRYPOINT. It become curl https://example.com If you want to provide default arguments that can be overridden, you combine it with CMD. ENTRYPOINT [\"python\"] CMD [\"app.py\", \"--debug\"] Running docker run myapp:python app.py --debug Running docker run myapp server.py: python server.py Meanwhile, if you only use CMD, it‚Äôs fully replaceable at runtime. CMD [\"nginx\", \"-g\", \"daemon off;\"] Running docker run webserver: runs nginx Running docker run webserver bash: runs bash instead (overrides CMD) Rule of thumb: Use ENTRYPOINT if the container should always execute a specific binary. Use CMD for providing defaults that the user might want to override. Combine both if you want an executable with flexible arguments. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:4","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"RUN The RUN instruction is executed at build time and creates a new layer in the image. It‚Äôs used to install software, configure the environment, or set up files. Once executed, its results are part of the final image. For example, you can use the RUN command to install system dependencies and clean up: RUN apt-get update \u0026\u0026 \\ apt-get install -y git curl \u0026\u0026 \\ # install required tools rm -rf /var/lib/apt/lists/* # Cleanup reduces image size. You can use the RUN command to create a directory: RUN mkdir -p /data/logs or prepare a non-root user: RUN useradd -ms /bin/bash appuser It‚Äôs worth noting that the order of the RUN commands in the Dockerfile is important, as each command creates a new layer in the image, and the resulting image is the union of all the layers. So, if you‚Äôre expecting some packages to be installed later in the process, you need to do it before using them. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:5","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"LABEL The LABEL instruction attaches metadata to an image in the form of key-value pairs. This can include maintainer info, version, licensing, or anything meaningful to your workflow. LABEL maintainer=\"Alice Lee \u003calice@example.com\u003e\" \\ version=\"1.4\" \\ description=\"Lightweight API server image\" This metadata can later be queried: docker inspect myimage | grep version Labeling images makes them easier to manage, track, and audit in CI/CD pipelines. ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:6","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"ENV and ARG Both define variables, but their lifetimes differ: ARG: available only at build time. Used to pass values when running docker build. ENV: It creates an environment variable that is accessible to all processes running inside the container. ARG APP_VERSION=latest RUN echo \"Building version $APP_VERSION\" You can override it at build: docker build --build-arg APP_VERSION=2.0 . The ENV command is used to set environment variables: ENV PORT=8080 EXPOSE $PORT Now any process inside the container can access $PORT. Rule of thumb: Use ARG for build-time options (e.g., base image tag, dependency version). Use ENV for runtime configuration (e.g., service port, API key). ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:7","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"VOLUME The VOLUME instruction in a Dockerfile defines a mount point where data can be stored outside the container‚Äôs writable layer. This means that the data will persist even if the container is removed or rebuilt. Volumes are managed by Docker, or you can bind them to host directories. They‚Äôre especially important for databases, logs, or user-generated content. When you run a container without volumes, everything is stored in its writable layer. That writable layer is destroyed when you remove the container (docker rm). That‚Äôs why data disappears. When you declare a VOLUME in the Dockerfile (or with -v at docker run), Docker does something special: It says: ‚ÄúDon‚Äôt keep this folder (/var/lib/mysql) inside the container‚Äôs temporary writable layer.‚Äù Instead, it mounts an external storage location (on the host) to that folder. The container sees /var/lib/mysql as normal, but under the hood, all files actually live in: /var/lib/docker/volumes/\u003cvolume_id\u003e/_data (if Docker-managed volume) or /my/local/db (if you used -v /my/local/db:/var/lib/mysql) Imagine you‚Äôre building a Docker image for MySQL. You don‚Äôt want your database data to disappear every time the container restarts. FROM mysql:8.0 ## Define where MySQL stores its data VOLUME [\"/var/lib/mysql\"] EXPOSE 3306 VOLUME [\"/var/lib/mysql\"] tells Docker that /var/lib/mysql should be stored outside the container layer system. You can see the volume with docker volume ls docker volume inspect \u003cvolume_name\u003e ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:8","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"USER By default, containers run as root, which is risky. The USER instruction sets a non-root user for better security. If there‚Äôs a Docker or kernel vulnerability, a process running as root inside the container could potentially escape and gain root access on the host. This is the main worry in multi-tenant environments (like shared servers or Kubernetes clusters). RUN useradd -ms /bin/bash appuser USER appuser Now every process inside the container runs as appuser. -m: create the user‚Äôs home directory (e.g., /home/appuser). -s /bin/bash: set the user‚Äôs login shell to /bin/bash. You can still override: docker run --user root myimage ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:9","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"WORKDIR The WORKDIR instruction sets the current working directory inside the container. Every subsequent RUN, CMD, ENTRYPOINT, COPY, or ADD will be executed relative to this directory. You can use the WORKDIR command to set the working directory to /usr/local/app: WORKDIR /usr/local/app WORKDIR /usr/src/app COPY . . RUN pip install -r requirements.txt COPY . .: copies files into/usr/src/app RUN pip install -r requirements.txt: runs inside /usr/src/app The WORKDIR can be changed multiple times during an image build. FROM ubuntu:22.04 # First working directory WORKDIR /app RUN echo \"Hello\" \u003e hello.txt # Change to another directory WORKDIR /app/subdir RUN echo \"World\" \u003e world.txt # Copy from build context relative to new WORKDIR COPY main.py . ","date":"2025-09-13","objectID":"/docker-dockerfile/:1:10","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Writing Efficient Dockerfiles ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:0","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Small base images The full python image ships lots of build tools you usually don‚Äôt need in production. Using a smaller base cuts image size. Recommended for most apps (good balance of size \u0026 compatibility): FROM python:3.11-slim # Debian (Bookworm) base Smallest footprint, but riskier (musl libc; manylinux wheels may not work and native builds can be painful): FROM python:3.11-alpine Tip: Alpine is great for pure-Python deps. If you need C extensions (NumPy, psycopg2, etc.), stick with a Debian-based slim image for painless wheel installs. ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:1","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Run as a non-root user (security best practice) Don‚Äôt run your app as root. Running your application as a non-root user reduces the potential impact if the container is ever compromised. Create a dedicated user and run the app under it. Debian/Ubuntu (slim) variant: FROM python:3.11-slim # Create non-root user RUN useradd -m appuser WORKDIR /app # 1) Copy only dependency file first (cache-friendly) COPY requirements.txt . # 2) Install deps (small image; faster rebuilds) RUN pip install --no-cache-dir -r requirements.txt # 3) Copy the rest of the app with correct ownership COPY --chown=appuser:appuser . . # Run as non-root USER appuser CMD [\"python3\", \"app.py\"] useradd creates a system user with no login shell or home directory. COPY --chown=... sets ownership as files are copied, avoiding an extra chown layer. USER appuser ensures your process runs without root privileges. ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:2","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Reuses the cached layer Docker builds image layers from each instruction in your Dockerfile. If the input to a layer hasn‚Äôt changed, Docker reuses the cached layer instead of re-running it. Your application code changes often, but your dependencies (requirements) change rarely. So if you install deps in an earlier layer and copy your code later, most builds can reuse the heavy ‚Äúinstall deps‚Äù layer and only re-run the quick ‚Äúcopy code‚Äù step. Bad Example: FROM python:3.11-slim WORKDIR /app # ‚ùå Copies everything (your changing code!) first COPY . . # ‚ùå Now this runs every time your code changed above RUN pip install --no-cache-dir -r requirements.txt CMD [\"python3\", \"app.py\"] Any code change triggers the cache before pip install, so dependencies reinstall every time. Good Example: FROM python:3.11-slim WORKDIR /app # ‚úÖ Copy only the dependency file first (rarely changes) COPY requirements.txt . # ‚úÖ Install deps now; this layer is cached until requirements.txt changes RUN pip install --no-cache-dir -r requirements.txt # ‚úÖ Copy your frequently changing app code last COPY . . CMD [\"python3\", \"app.py\"] Dependency install is isolated in its own layer and only re-runs when requirements change. --no-cache-dir is a pip option that tells pip not to write a local download/cache when installing packages. When you run pip install -r requirements.txt, pip: downloads wheels/source archives to a local cache (usually ~/.cache/pip), installs the packages from that cache. That cache can speed up the next install because files are already downloaded. ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:3","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Multi-Stage Builds Some Python packages need extra tools to compile, but your app won‚Äôt use those tools after it‚Äôs built. With multi-stage builds, you compile everything in a builder image, then move just the finished packages into a light ‚Äúruntime‚Äù image. That keeps the final image small, quick to start, and easier to secure. # Build stage FROM python:3.11 AS builder WORKDIR /build COPY requirements.txt . # Install build dependencies RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends gcc libpq-dev \u0026\u0026 \\ pip wheel --no-cache-dir --wheel-dir /wheels -r requirements.txt # Final stage FROM python:3.11-slim WORKDIR /app # Copy only wheels from builder COPY --from=builder /wheels /wheels RUN pip install --no-cache-dir --no-index --find-links=/wheels /wheels/* COPY . . CMD [\"python3\", \"app.py\"] ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:4","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"Use .dockerignore file Before building, Docker sends your project folder to the engine (i.e., build context, the set of files/folders Docker sends to the Docker engine at the start of docker build.). Use .dockerignore to exclude junk (git files, venvs, caches) so builds are faster and caching works. # Version control .git/ .gitignore # Python artifacts __pycache__/ *.py[cod] *$py.class .pytest_cache/ .coverage # Environments \u0026 secrets .env .venv # Build outputs build/ dist/ *.egg-info/ # Optional: tests and local tooling not needed in the image tests/ .idea/ .vscode/ ","date":"2025-09-13","objectID":"/docker-dockerfile/:2:5","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"References The Linux DevOps Handbook, Damian Wojs≈Çaw and Grzegorz Adamowicz How to Write Efficient Dockerfiles for Your Python Applications ","date":"2025-09-13","objectID":"/docker-dockerfile/:3:0","tags":["Docker","Dockerfile","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 3: Dockerfile","uri":"/docker-dockerfile/"},{"categories":["Docker","DevOps","Programming"],"content":"A Docker tutorial.","date":"2025-09-13","objectID":"/docker-networks/","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"This is part of my Docker Basics series ‚Äî introductory guides to help you get started with Docker, learn key concepts, and build your skills step by step. Part 1: Understanding Container Part 2: Basic Commands Part 3: Dockerfile Part 4: Networks Docker Networking Docker offers four built-in network drivers: none, bridge, host, and overlay. Bridge (default): Creates an isolated, software-defined network. Containers on the same bridge get private IPs and can communicate with each other, while anything outside can‚Äôt reach them unless you explicitly publish ports. Host: Removes the isolation layer and uses the host‚Äôs network stack directly. The container shares the host‚Äôs IP address and network interfaces. Overlay: Builds a virtual network that spans multiple Docker hosts, so containers on different machines can talk as if they‚Äôre on the same one‚Äîhandy for Docker Swarm. None: Disables networking (other than loopback) for the container. You can create and manage custom networks of any of these types with the Docker CLI. ","date":"2025-09-13","objectID":"/docker-networks/:0:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"None Network A none network in Docker is a special type of network mode that disables all networking for a container. When a container is run in none network mode, it does not have access to any network resources and cannot communicate with other containers or the host machine. In short, the container is completely isolated from external networks. The following command will fail because the container has no access to external networks. docker run --network none alpine ping google.com The none network is useful for running workloads that aren‚Äôt supposed to use any network connections, for example, for offline data processing on mounted volumes. ","date":"2025-09-13","objectID":"/docker-networks/:1:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"Bridge mode In bridge mode (Docker‚Äôs default), Docker creates a virtual Ethernet interface for each container and plugs it into a Linux bridge on the host (the default device is docker0). Each container receives a private IP on that subnet (commonly 172.17.0.0/16). The host machine acts as a gateway for the containers, routing traffic between the containers and the outside network. When a container wants to communicate with another container or the host machine, it sends the packet to the virtual network interface. The virtual network interface then routes the packet to the correct destination. By default, it‚Äôs a 172.17.0.0/16 network and it‚Äôs connected to a virtual bridge network, docker0, in your machine. Within this network, all traffic between containers and the host machine is allowed. By default, external systems cannot directly access containers unless you publish ports. It means containers in a given bridge network can only talk to other containers on the same bridge network by default. When Docker creates a bridge (docker0 or a user-defined one), it sets up a virtual subnet with its own IP range (e.g., 172.18.0.0/16). Containers attached to that bridge get IPs in that subnet. Docker also adds iptables rules to restrict traffic so only containers on the same bridge can reach each other. All containers are attached to the default bridge network if no network was selected. You can create use a user-defined bridge by using the --network option when executing the docker run command. docker network create mybridge docker run -d --network mybridge --name c1 alpine sleep 3600 docker run -it --network mybridge alpine ping c1 You can list all available networks using the following command: docker network ls ","date":"2025-09-13","objectID":"/docker-networks/:2:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"Host mode In host networking mode, the container exploits the host‚Äôs network stack and network interfaces directly. This means that the container shares your machine‚Äôs IP address and network settings, and can directly access the same network resources. As a result, it can bind straight to host ports (no -p needed), but also competes with other processes for the same ports. One of the main advantages of host networking mode is that it provides slightly better latency/throughput as the container doesn‚Äôt have to go through an additional network stack. Also, it simplifies routing and debugging as everything appears on the host network. However, this mode is less secure than the other networking mode as the container has direct access to the host‚Äôs network resources and can listen to connections on the host‚Äôs interface. Example: docker run --network host nginx The Nginx server will be directly available on the host‚Äôs IP without -p port mapping. ","date":"2025-09-13","objectID":"/docker-networks/:3:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"Overlay An overlay network is created by a manager node, which is responsible for maintaining the network configuration and managing the membership of worker nodes. The manager node creates a virtual network switch and assigns IP addresses to each container on the network. It spans multiple Docker hosts, creating a distributed virtual network so containers on different machines can communicate as if they are on the same host. In Docker, this is typically used with Swarm: managers maintain network state; workers join and connect containers to the overlay. Under the hood, Docker uses VXLAN encapsulation to carry container traffic between nodes. Example, # Initialize Swarm on a manager node docker swarm init # Create an attachable overlay so both services and 'docker run' can join docker network create -d overlay --attachable myoverlay # Deploy a service onto the overlay docker service create --name web --network myoverlay -p 80:80 nginx # (Optional) Attach a regular container to the same overlay docker run -d --name util --network myoverlay busybox sleep 1d docker network command The docker network command is used to manage container networking. With it, you can create, inspect, list, remove, and connect/disconnect containers to networks. By default, Docker creates a few networks automatically, but you can also define your own for better control and isolation. It provides several benefits: Containers are isolated, but they often need to communicate with each other. Docker networks provide controlled connectivity: you can group containers, control who can talk to whom, and even connect containers across multiple hosts (with Swarm or Kubernetes). Creating a new network: docker network create mynetwork Inspecting a network: docker network inspect mynetwork Removing a network: docker network rm mynetwork Listing networks: docker network ls Connecting a container to a network: docker network connect mynetwork CONTAINER_NAME_OR_ID Disconnecting a container from a network: docker network disconnect mynetwork CONTAINER_NAME_OR_ID ","date":"2025-09-13","objectID":"/docker-networks/:4:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["Docker","DevOps","Programming"],"content":"References The Linux DevOps Handbook, Damian Wojs≈Çaw and Grzegorz Adamowicz ","date":"2025-09-13","objectID":"/docker-networks/:5:0","tags":["Docker","network","Programming","DevOps","Tutorial"],"title":"Docker Tutorial Part 4: Networks","uri":"/docker-networks/"},{"categories":["study notes"],"content":"My ML/DL study note","date":"2025-09-07","objectID":"/studynotes/dsl-note/","tags":["machine learning","deep learning","study notes"],"title":"Deep Statistical Learning","uri":"/studynotes/dsl-note/"},{"categories":["study notes"],"content":"üìñ Deep Statistical Learning üëâ Full repository: deep_statistical_learning I started writing these notes during my master‚Äôs degree as a way to organize and clarify my understanding of machine learning and deep learning. Over time, the notes have grown into a comprehensive study resource that combines theoretical foundations with practical insights. I enjoy organizing knowledge and writing things down, and these notes reflect that process ‚Äî carefully breaking down concepts, connecting ideas, and recording them in a way that can be revisited and built upon. The goal of this collection is to: ‚ú® Build strong intuition ‚Äî explain mathematical ideas in clear, accessible language. üß© Connect theory and practice ‚Äî link statistical concepts to modern deep learning methods. üìö Serve as a long-term reference ‚Äî a resource I can return to as my research and projects evolve. The notes cover a range of topics including probability and statistics, optimization methods, neural networks, and modern deep learning architectures, along with worked examples and derivations. I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. ","date":"2025-09-07","objectID":"/studynotes/dsl-note/:0:0","tags":["machine learning","deep learning","study notes"],"title":"Deep Statistical Learning","uri":"/studynotes/dsl-note/"},{"categories":["study notes"],"content":"My dl system design note","date":"2025-09-07","objectID":"/studynotes/dl-system-design/","tags":["system design","deep learning","LLM"],"title":"DL System Design","uri":"/studynotes/dl-system-design/"},{"categories":["study notes"],"content":"üèóÔ∏è Deep Learning System Design üëâ Repository: DL System Design This note is about the practical design of deep learning and LLM-based systems. It focuses on bridging research with production: covering concepts such as scalable architectures, model deployment, monitoring, and system reliability. The aim is to capture design patterns and lessons that make ML/DL services both effective and maintainable. I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. ","date":"2025-09-07","objectID":"/studynotes/dl-system-design/:0:0","tags":["system design","deep learning","LLM"],"title":"DL System Design","uri":"/studynotes/dl-system-design/"},{"categories":["study notes"],"content":"My linear algebra note","date":"2025-09-07","objectID":"/studynotes/matrix-methods/","tags":["machine learning","linear algebra","study notes"],"title":"Matrix Methods","uri":"/studynotes/matrix-methods/"},{"categories":["study notes"],"content":"üìê Matrix Methods üëâ Repository: Matrix Methods This note collects my study materials on linear algebra and matrix methods, focusing on the concepts most relevant to machine learning and deep learning. It includes explanations, worked examples, and connections between theory and practical applications. I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. ","date":"2025-09-07","objectID":"/studynotes/matrix-methods/:0:0","tags":["machine learning","linear algebra","study notes"],"title":"Matrix Methods","uri":"/studynotes/matrix-methods/"},{"categories":["study notes"],"content":"My RL study note","date":"2025-09-07","objectID":"/studynotes/rl-note/","tags":["reinforcement learning","study notes"],"title":"Reinforcement Learning","uri":"/studynotes/rl-note/"},{"categories":["study notes"],"content":" I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. üìò Reinforcement Learning Notes üëâ You can check out the full notes here: reinforcement_learning_note There are already a number of excellent tutorials and lectures on Reinforcement Learning (RL), but I often find that many of them do not provide enough detail or explanation of the formulas behind the concepts. In many cases, key ideas are assumed to be obvious or straightforward‚Äîwhich can be a challenge for someone like me, who has a weaker background in mathematics but still wants a comprehensive understanding of RL. To address this, I am writing these notes with the following goals: ‚ú® Clarity over assumptions I attempt to explain the theory as clearly as possible, breaking down formulas with step-by-step reasoning and intuitive examples. üß© Bridging theory and implementation Many Python RL implementations rely heavily on external libraries or are overly simplistic. My goal is to connect the mathematical foundation to practical code‚Äîwithout skipping over the details. üôè Acknowledging the giants This work builds on numerous outstanding sources, and I am forever indebted to them for their contributions. ","date":"2025-09-07","objectID":"/studynotes/rl-note/:0:0","tags":["reinforcement learning","study notes"],"title":"Reinforcement Learning","uri":"/studynotes/rl-note/"},{"categories":["study notes"],"content":"üìö References Reinforcement Learning ‚Äî Richard S. Sutton and Andrew G. Barto Mastering Reinforcement Learning with Python Grokking Deep Reinforcement Learning ‚Äî Miguel Morales Introduction to Reinforcement Learning ‚Äî David Silver (lecture series) PyTorch 1.x Reinforcement Learning Cookbook Deep Learning from Scratch 4 ","date":"2025-09-07","objectID":"/studynotes/rl-note/:1:0","tags":["reinforcement learning","study notes"],"title":"Reinforcement Learning","uri":"/studynotes/rl-note/"},{"categories":["study notes"],"content":"My statistics note","date":"2025-09-07","objectID":"/studynotes/statistics/","tags":["Statistics"],"title":"Statistics","uri":"/studynotes/statistics/"},{"categories":["study notes"],"content":"üìä Statistics üëâ Repository: Statistics This note collects my study materials on probability and statistics, with a focus on the foundations needed for data science, machine learning, and deep learning. It combines key definitions, derivations, and examples, aiming to make abstract ideas easier to understand and apply. I welcome all comments and suggestions‚Äîand I‚Äôd be happy to improve and grow this note together with you. ","date":"2025-09-07","objectID":"/studynotes/statistics/:0:0","tags":["Statistics"],"title":"Statistics","uri":"/studynotes/statistics/"},{"categories":["NL2SQL","AI","LLM","SQL"],"content":"A tutorial for Pydantic-AI","date":"2025-09-06","objectID":"/nl2sql/","tags":["NL2SQL","Agent","AI","LLM","SQL"],"title":"NL2SQL Part 1.","uri":"/nl2sql/"},{"categories":["NL2SQL","AI","LLM","SQL"],"content":"üí°Natural Language to SQL (NL2SQL) in the LLM Era Data has become one of the most valuable resources of our time. Companies in finance, healthcare, logistics, retail, and many other fields collect enormous amounts of information every day. Much of this information is stored in relational databases, which are typically accessed using SQL. While SQL provides the raw outputs of a query, the critical step lies in interpreting these results. Developing intuition from retrieved data is essential for identifying meaningful patterns, uncovering relationships, and supporting evidence-based decision-making. This is where Natural Language to SQL (NL2SQL) becomes valuable. Instead of writing SQL code, users can simply ask a question in plain language, such as: ‚ÄúWhat are the top five products sold in Asia this year?‚Äù and automatically receive the result‚Äîwithout having to construct a query like: SELECT product, SUM(sales) FROM transactions WHERE region = 'Asia' AND date \u003e= '2024-01-01' GROUP BY product ORDER BY SUM(sales) DESC LIMIT 5; The motivation is clear: make structured data accessible to everyone. In practice, this means: Reducing dependency on technical expertise. Enabling deeper understanding of data. Supporting more efficient research and analysis. However, achieving this vision introduces significant technical challenges. ","date":"2025-09-06","objectID":"/nl2sql/:1:0","tags":["NL2SQL","Agent","AI","LLM","SQL"],"title":"NL2SQL Part 1.","uri":"/nl2sql/"},{"categories":["NL2SQL","AI","LLM","SQL"],"content":"The Challenges of NL2SQL Developing systems that can reliably translate natural language into SQL queries is a long-standing research challenge that sits at the intersection of natural language processing, information retrieval, and database management. The difficulty lies not only in the inherent ambiguity and variability of human language, but also in the structural complexity and heterogeneity of real-world databases. While the vision of democratizing access to data through natural language interfaces is compelling, realizing it in practice requires addressing several deep challenges‚Äîranging from robust language understanding and schema alignment, to handling noisy data, adapting to diverse SQL dialects, and ensuring secure and efficient deployment. Beyond these technical hurdles, organizational culture plays a decisive role. In many enterprises, data access is still mediated by technical gatekeepers, and decision-making cultures are shaped by long-standing workflows and hierarchies. For NL2SQL systems to be truly effective, companies must embrace a culture of data accessibility‚Äîempowering non-technical users, fostering trust in automated systems, and promoting responsible use of sensitive information. Without this cultural shift, even the most advanced systems risk being underutilized or confined to experimental settings. The next sections will explore the key challenges that stand in the way of realizing NL2SQL at scale. ","date":"2025-09-06","objectID":"/nl2sql/:2:0","tags":["NL2SQL","Agent","AI","LLM","SQL"],"title":"NL2SQL Part 1.","uri":"/nl2sql/"},{"categories":["study notes"],"content":"My coding study note","date":"2025-09-06","objectID":"/studynotes/coding-note/","tags":["programming","study notes"],"title":"Programming / Coding Note","uri":"/studynotes/coding-note/"},{"categories":["study notes"],"content":"üíª Coding Note ‚ö†Ô∏è This section is temporarily closed while I reorganize and refine the notes. It will be updated and re-opened later. This is a collection of tutorial-style study notes on programming. The aim is to make concepts clear, practical, and applicable, with examples that can be adapted to real coding tasks. ","date":"2025-09-06","objectID":"/studynotes/coding-note/:0:0","tags":["programming","study notes"],"title":"Programming / Coding Note","uri":"/studynotes/coding-note/"},{"categories":["study notes"],"content":"üìö Topics Agile \u0026 Software Development ‚Äî workflows, methodologies, and best practices Algorithms \u0026 Computer Science ‚Äî problem-solving patterns, data structures, and theory Programming Languages ‚Äî C, Go, Python, Rust, SQL Machine Learning \u0026 Deep Learning ‚Äî theory notes, implementations, and experiments Natural Language Processing (NLP) ‚Äî applied tutorials and coding exercises DevOps \u0026 Linux ‚Äî shell scripts, automation, system tools, and environment setup Git \u0026 Vim ‚Äî version control workflows and editor productivity Regular Expressions (RegEx) ‚Äî pattern matching and cheat sheets Web Scraping ‚Äî techniques and scripts for data collection Reading Notes (ToReads, KS-Study) ‚Äî study references and knowledge summaries ","date":"2025-09-06","objectID":"/studynotes/coding-note/:0:1","tags":["programming","study notes"],"title":"Programming / Coding Note","uri":"/studynotes/coding-note/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"A tutorial for Pydantic-AI","date":"2025-08-31","objectID":"/pydantic-ai/","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Introduction AI has already changed how we interact with technology. The real shift is happening now with agents: AI systems that can reason, make decisions, and take action. Unlike a chatbot that passively replies, an agent can break down complex tasks, call APIs or databases, use tools, and deliver structured results. This is what makes the idea of Agentic AI so powerful ‚Äî it‚Äôs not just about conversation, it‚Äôs about problem-solving with initiative. So far, LangGraph has emerged as the de-facto approach for building complex, stateful agents. But there‚Äôs also a growing need for something simpler, more ergonomic ‚Äî and that‚Äôs where Pydantic AI comes in. Pydantic AI is a Python agent framework designed to make it less painful to build production-grade applications with Generative AI. Just as FastAPI revolutionized web development with an ergonomic design built on Pydantic validation, Pydantic AI brings that same ‚ÄúFastAPI feeling‚Äù to GenAI app development. Developers can apply the same best type-safe and Python-centric practices they have used in any other project. Structured responses are validated through Pydantic, dependencies can be injected directly into prompts and tools and for complex scenarios, Pydantic Graph provides a clean way to define control flow without descending into spaghetti code. In this post ‚Äî the first in a short series ‚Äî we‚Äôll explore the basics of Agentic AI and Pydantic AI, why they matter, and how they‚Äôre shaping the way we build intelligent systems. ","date":"2025-08-31","objectID":"/pydantic-ai/:1:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"A Minimal Example Set up with uv: uv init pydantic_ai_tutorial cd pydantic_ai_tutorial uv add pydantic pydantic-ai python-dotenv from pydantic_ai import Agent agent = Agent( 'google-gla:gemini-1.5-flash', system_prompt='Be concise, reply with one sentence.', ) result = agent.run_sync('Where does \"hello world\" come from?') print(result.output) # The first known use of \"hello, world\" was in a 1974 textbook about the C programming language. Here, Pydantic AI sends both the system prompt and the user query to the LLM. The model then returns a plain text response. That‚Äôs a good start ‚Äî but real agents often need more than text. Let‚Äôs build something more powerful. ","date":"2025-08-31","objectID":"/pydantic-ai/:2:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Building a Structured Agent We start by importing the essentials: from pydantic import BaseModel from pydantic_ai import Agent from pydantic_ai.models.openai import OpenAIChatModel BaseModel: the foundation for defining typed, validated data models. Agent: the main abstraction for running tasks with an LLM. OpenAIChatModel: a wrapper for OpenAI-compatible chat models. ","date":"2025-08-31","objectID":"/pydantic-ai/:3:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Step 1: Define the Schema Pydantic AI uses a dependency injection system to provide data and services to your agent‚Äôs system prompts, tools, and output validators. Dependencies can be any Python type, and dataclasses are often a convenient container when you need to include multiple objects. At the same time, by using a schema with Pydantic, we can enforce a variety of validators. This is important because we want the agent‚Äôs output to always be a structured object. For example, here‚Äôs a simple schema with two fields: from pydantic import BaseModel class CityLocation(BaseModel): city: str country: str # Alternatively, using a dataclass: # from dataclasses import dataclass # @dataclass # class CityLocation: # city: str # country: str By defining this schema, we force the model to always return a structured object with two fields: city and country. Without this, the agent might return a sentence like: ‚ÄúThe 2012 Summer Olympics were held in London, United Kingdom.‚Äù With this schema, the output is always: CityLocation(city=\"London\", country=\"United Kingdom\") This is critical for agents ‚Äî structured outputs can be passed directly to tools, APIs, or databases without string parsing. ","date":"2025-08-31","objectID":"/pydantic-ai/:3:1","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Step 2: Configure the Model from pydantic_ai.providers.ollama import OllamaProvider model = OpenAIChatModel( model_name='llama3.1:8b', provider=OllamaProvider(base_url='http://localhost:11434/v1') ) OpenAIChatModel: wraps the model in an OpenAI-like interface. OllamaProvider: lets us run a local Llama 3.1 model. Swap this with \"openai:gpt-4o\" and you can run against OpenAI‚Äôs hosted models with no other changes. This flexibility is one of Pydantic AI‚Äôs strengths. ","date":"2025-08-31","objectID":"/pydantic-ai/:3:2","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Step 3: Create the Agent agent = Agent( model, # or \"openai:gpt-4o\" output_type=CityLocation ) This is the heart of Pydantic AI: model ‚Üí the LLM backend. output_type=CityLocation ‚Üí ensures the output always validates against our schema. ","date":"2025-08-31","objectID":"/pydantic-ai/:3:3","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Step 4: Run the Agent result = agent.run_sync('Where were the olympics held in 2012?') print(result.output) #\u003e city='London' country='United Kingdom' print(result.usage()) #\u003e RunUsage(input_tokens=57, output_tokens=8, requests=1) When we run the agent: The LLM is called. The output is validated against CityLocation. If the model produces something invalid, Pydantic AI automatically retries until it fits the schema. We also get detailed usage stats (tokens, requests) for cost tracking and monitoring. ","date":"2025-08-31","objectID":"/pydantic-ai/:3:4","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Introducing Tools So far, our agent has taken in text and returned structured data. That‚Äôs powerful, but real-world agents usually need to do things: query a database, call an API, or run some custom Python code. In Pydantic AI, this is where tools come in. Tools are simply Python functions you expose to the agent. The agent decides when and how to call them. Let‚Äôs look at a simple example: import random from pydantic_ai import Agent, RunContext from dotenv import load_dotenv # Load environment variables from .env load_dotenv() # Load your API Key agent = Agent( 'google-gla:gemini-1.5-flash', deps_type=str, system_prompt=( \"You're a dice game, you should roll the die and see if the number \" \"you get back matches the user's guess. If so, tell them they're a winner. \" \"Use the player's name in the response.\" ), ) @agent.tool_plain def roll_dice() -\u003e str: \"\"\"Roll a six-sided die and return the result.\"\"\" return str(random.randint(1, 6)) @agent.tool def get_player_name(ctx: RunContext[str]) -\u003e str: \"\"\"Get the player's name.\"\"\" return ctx.deps dice_result = agent.run_sync('My guess is 4', deps='Anne') print(dice_result.output) #\u003e Congratulations Anne, you guessed correctly! You're a winner! ","date":"2025-08-31","objectID":"/pydantic-ai/:4:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Breaking It Down System Prompt We set the rules of the game in the system_prompt: the agent must roll a die and check if the result matches the player‚Äôs guess. roll_dice Tool (@agent.tool_plain) This is a simple tool which requires no extra context from the agent. It rolls a random number from 1 to 6 and returns it. The agent can call this function whenever it needs a die result. get_player_name Tool (@agent.tool) This tool shows how dependencies come into play. We pass the player‚Äôs name as a dependency (deps='Anne'). The tool accesses it through the RunContext. This is powerful: you can inject things like database connections, API keys, or user profiles in exactly the same way. Running the Agent The user says: ‚ÄúMy guess is 4.‚Äù The agent calls roll_dice to simulate the game. It calls get_player_name to personalize the response. Finally, it returns a natural sentence, ‚ÄúCongratulations Anne‚Ä¶‚Äù ","date":"2025-08-31","objectID":"/pydantic-ai/:4:1","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"Why Tools Matter Tools are what make an agent actionable: They let your LLM interact with the real world. They allow personalization through dependency injection. They keep logic in Python, not just in prompts. You can test your tools by from pydantic_ai import Agent, RunContext agent = Agent('test', deps_type=int) @agent.tool def foobar(ctx: RunContext[int], x: int) -\u003e int: return ctx.deps + x @agent.tool(retries=2) async def spam(ctx: RunContext[str], y: float) -\u003e float: return ctx.deps + y result = agent.run_sync('foobar', deps=1) print(result.output) #\u003e {\"foobar\":1,\"spam\":1.0} The number of retries to allow for this tool, defaults to the agent‚Äôs default retries, which defaults to 1. With tools, your agent is no longer just a Q\u0026A bot ‚Äî it becomes an active participant that can call functions, fetch data, and take action. ","date":"2025-08-31","objectID":"/pydantic-ai/:4:2","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"SRE Example So far we‚Äôve built simple agents that return structured outputs and call a few tools. Let‚Äôs put everything together in a more realistic scenario: an on-call SRE (Site Reliability Engineering) assistant that analyzes service health and suggests a remediation plan. Instead of ‚Äúsystem administrators‚Äù doing manual firefighting, Google created a team of engineers who used automation, monitoring, and code to manage reliability at scale. That practice evolved into what we now call SRE. Imagine you‚Äôre on call. Users report slow responses and 5xx errors. Instead of digging manually through dashboards and logs, you can ask an agent to do the initial triage for you. Below is a runnable example: import os import asyncio from dataclasses import dataclass from typing import Any, Literal from pydantic import BaseModel, Field from pydantic_ai import Agent, RunContext from pydantic_ai.models.openai import OpenAIChatModel from pydantic_ai.providers.openai import OpenAIProvider from pydantic_ai.providers.ollama import OllamaProvider from dotenv import load_dotenv load_dotenv() api_key = os.getenv(\"OPENAI_API_KEY\") # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # Mock infra backend # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ class MetricsClient: async def cpu_usage(self, service: str) -\u003e float: return {\"api\": 82.4, \"worker\": 64.1, \"db\": 29.8}.get(service, 0.0) async def error_rate(self, service: str) -\u003e float: # errors per minute return {\"api\": 14.0, \"worker\": 1.2, \"db\": 0.4}.get(service, 0.0) async def recent_logs(self, service: str, level: Literal[\"ERROR\", \"WARN\"], limit: int = 20) -\u003e list[str]: logs = { \"api\": [ \"[ERROR] upstream timeout /v1/translate (llm gateway)\", \"[ERROR] redis timeout queue=llm_jobs\", \"[WARN] slow request /v1/health 1200ms\", ], \"worker\": [ \"[WARN] retry publish job_id=abc\", \"[WARN] prefetch backlog=40\", ], \"db\": [ \"[WARN] connection spike from api\", ], } out = [l for l in logs.get(service, []) if l.startswith(f\"[{level}]\")] return out[:limit] # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # Dependencies \u0026 output schema # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ @dataclass class OnCallDeps: service: str metrics: MetricsClient class RemediationPlan(BaseModel): message: str = Field(description=\"Human-readable summary for the on-call runbook.\") severity: Literal[\"SEV1\", \"SEV2\", \"SEV3\", \"INFO\"] = Field(description=\"Incident severity.\") notify_oncall: bool = Field(description=\"Page the primary on-call?\") actions: list[str] = Field(description=\"Ordered steps to mitigate the issue.\") # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # Model \u0026 Agent # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # model = OpenAIChatModel(\"gpt-4o-mini\") model = OpenAIChatModel('gpt-5-nano', provider=OpenAIProvider(api_key=api_key)) # If using Ollama, pick a model that supports tools/function calling, e.g. # model = OpenAIChatModel( # model_name=\"qwen3:8b\", # provider=OllamaProvider(base_url=\"http://localhost:11434/v1\"), # ) oncall_agent = Agent( model, deps_type=OnCallDeps, output_type=RemediationPlan, system_prompt=( \"You are an SRE assistant. Diagnose service incidents using tools before concluding. \" \"Prefer concrete evidence (metrics/logs). Be concise and practical.\" ), ) # Dynamic system prompt: injects service name at runtime # The !r in an f-string is a conversion flag that tells Python to use the repr() of the value instead of the default str() when formatting. @oncall_agent.system_prompt async def service_context(ctx: RunContext[OnCallDeps]) -\u003e str: return f\"Target service: {ctx.deps.service!r}. Environment: production.\" # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # Tools the agent can call # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ @oncall_agent.tool async def get_cpu(ctx: RunContext[OnCallDeps]) -\u003e float: \"\"\"Return current CPU utilization percentage for the target service.\"\"\" return await ctx.deps.metrics.cpu_usage(ctx.deps.service) @oncall_agent.tool async def get_error_rate(ctx: RunContext[OnCallDeps]) -\u003e float: \"\"\"Return errors-per-minute for the target service.\"\"\" return await ctx.deps.metrics.error_rate(ctx.deps","date":"2025-08-31","objectID":"/pydantic-ai/:5:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic AI","AI","Pydantic"],"content":"References Pydantic AI ","date":"2025-08-31","objectID":"/pydantic-ai/:6:0","tags":["Python","Pydantic","Pydantic-AI","Agent","AI"],"title":"Agentic AI with Pydantic-AI Part 1.","uri":"/pydantic-ai/"},{"categories":["Python","Pydantic"],"content":"A tutorial for Pydantic","date":"2025-08-30","objectID":"/pydantic/","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":" üìù Update (2025-08): This post was originally published in April 2024 and has been updated to reflect changes in Pydantic v2, including the new field validator, model validator, and Annotated-based validation patterns. Also, this post now includes a new section on using Pydantic with MongoDB. Python‚Äôs dynamic typing system is indeed convenient, allowing you to create variables without explicitly declaring their types. While this flexibility can streamline development, it can also introduce unexpected behavior, particularly when handling data from external sources like APIs or user input. Python‚Äôs dynamic typing system is indeed convenient, allowing you to create variables without explicitly declaring their types. While this flexibility can streamline development, it can also introduce unexpected behavior, particularly when handling data from external sources like APIs or user input. Consider the following scenario: employee = Employee(\"Han\", 30) # Correct employee = Employee(\"Moon\", \"30\") # Correct Here, the second argument is intended to represent an age, typically an integer. However, in the second example, it‚Äôs a string, potentially leading to errors or unexpected behavior down the line. To address such issues, Pydantic offers a solution through data validation. Pydantic is a library specifically designed for this purpose, ensuring that the data conforms to pre-defined schemas. The primary method of defining schemas in Pydantic is through models. Models are essentially classes that inherit from pydantic.BaseModel and define fields with annotated attributes. You can think of models as similar to structs in languages like C. While Pydantic models share similarities with Python‚Äôs dataclasses, they are preferred when data validation is essential. Pydantic models guarantee that the fields of an instance will adhere to specified types, providing both runtime validation and serving as type hints during development. Let‚Äôs illustrate this with a simple example: from pydantic import BaseModel class User(BaseModel): id: int name: str = \"John Doe\" User model has two fields: id integer and name string, which has a default value. You can create an instance, user = User(id=\"123\") You can also define models that include other models, allowing for complex data structures: from pydantic import BaseModel from typing import List class Item(BaseModel): name: str price: float class Order(BaseModel): items: List[Item] total_price: float order = Order( items=[{\"name\": \"Burger\", \"price\": 5.99}, {\"name\": \"Fries\", \"price\": 2.99}], total_price=8.98 ) print(order) ","date":"2025-08-30","objectID":"/pydantic/:0:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Field In Pydantic, the Field() function is used to provide extra metadata and control over how a field behaves ‚Äî beyond just its type. It is especially useful for: Setting default values Adding aliases (e.g., mapping from external keys like _id) Adding validation constraints (e.g., min/max length, regex) Documenting fields (for use in OpenAPI docs, etc.) For example, from pydantic import BaseModel, Field class User(BaseModel): name: str = Field(..., description=\"The full name of the user\") age: int = Field(default=0, ge=0, lt=150) # Set a default value You can test it class Product(BaseModel): name: str = Field(..., min_length=3, max_length=50) price: float = Field(..., gt=0) prod = Product(name=\"test\", price=-10) # price # Input should be greater than 0 [type=greater_than, input_value=-10, input_type=int] Field with an Alias (e.g., for MongoDB _id): class User(BaseModel): id: str = Field(..., alias=\"_id\") Now Pydantic will accept both: User(id=\"abc\") # native style User(**{\"_id\": \"abc\"}) # MongoDB style ‚úÖ When you dump it back, you can choose which name to use: user = User(id=\"abc\") print(user.model_dump()) # {'id': 'abc'} print(user.model_dump(by_alias=True)) # {'_id': 'abc'} Sometimes a field needs a dynamic value at runtime like: UUID Timestamp You can‚Äôt use default=... because it would be evaluated once at class definition time, not per instance. So, we use default_factory. Now every time you create an instance: from datetime import datetime class Event(BaseModel): id: str = Field(default_factory=lambda: \"evt_\" + datetime.utcnow().isoformat()) event1 = Event() event2 = Event() You‚Äôll get unique ids like: print(event1.id) # evt-2025-08-30T11:28:01.123456 print(event2.id) # evt-2025-08-30T11:28:03.987654 ","date":"2025-08-30","objectID":"/pydantic/:1:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Validators Pydantic provides validators, which enables you to impose custom validation rules on model fields. These validators extend beyond simple type validation and allow you to enforce additional checks. ","date":"2025-08-30","objectID":"/pydantic/:2:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Field validators a field validator is a callable taking the value to be validated as an argument and returning the validated value. Here‚Äôs a simple example: from typing import Annotated from pydantic import BaseModel, AfterValidator, BaseModel, ValidationError def check_age(value): if value \u003c 18: raise ValueError('Age must be at least 18') return value class Person(BaseModel): name: str age: Annotated[int, AfterValidator(check_age)] # This will raise an error because the age is below 18 try: Person(name=\"Charlie\", age=17) except ValidationError as e: print(e) This uses Python‚Äôs typing.Annotated type to attach validation logic to a field in a declarative way. int: The field type (age is an integer). AfterValidator(check_age): Runs check_age(value) after Pydantic has validated and parsed the raw value (e.g., converting string to int if needed). AfterValidator ensures your custom validator runs after type coercion and default validation. You can use a single validator function to apply the same logic (e.g., capitalization, stripping, type conversion, etc.) to multiple fields by using the decorator pattern. from pydantic import BaseModel, field_validator class User(BaseModel): first_name: str last_name: str @field_validator('first_name', 'last_name', mode='before') @classmethod def capitalize_names(cls, value: str) -\u003e str: return value.capitalize() user = User(first_name=\"alice\", last_name=\"cooper\") print(user.first_name) # Alice print(user.last_name) # Cooper ","date":"2025-08-30","objectID":"/pydantic/:2:1","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Model Validators The @model_validator is a new feature in Pydantic v2 that replaces the older @root_validator from v1. It lets you validate the entire model at once ‚Äî useful when: Fields depend on each other (e.g., confirm passwords match) You want to enforce cross-field consistency You want to do post-processing after all fields are parsed from typing_extensions import Self from pydantic import BaseModel, model_validator class UserModel(BaseModel): username: str password: str password_repeat: str @model_validator(mode='after') def check_passwords_match(self) -\u003e Self: if self.password != self.password_repeat: raise ValueError('Passwords do not match') return self try: user = UserModel(username=\"alice\", password=\"secret\", password_repeat=\"notsecret\") except ValueError as e: print(f\"Validation failed: {e}\") # Validation failed: 1 validation error for UserModel # Passwords do not match (type=value_error) @model_validator(mode='after') runs after all field-level validation is complete Runs on the model instance (self, UserModel in this case) instead of just individual fields You can access any field via self.fieldname You must return self, or raise ValueError if validation fails ","date":"2025-08-30","objectID":"/pydantic/:2:2","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"When to Use Each Type of Validator Validator Purpose Scope Return Value @field_validator Validate one or more individual fields Field-level Transformed value @model_validator (after) Validate the entire model Model-level Return self @model_validator (before) Preprocess the input dict before any field validation Dict-level Return a dict ","date":"2025-08-30","objectID":"/pydantic/:3:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Pydantic for Configuration Management Pydantic isn‚Äôt just for validating user input ‚Äî it‚Äôs also an excellent tool for managing application settings through environment variables or .env files. This is especially useful for 12-factor apps that rely on external configuration across environments. To use this feature in Pydantic v2, install the standalone pydantic-settings package: pip install pydantic-settings Then, for example from pydantic_settings import BaseSettings, SettingsConfigDict class DatabaseSettings(BaseSettings): api_key: str database_password: str model_config = SettingsConfigDict(env_file=\".env\") # loads from .env by default settings = DatabaseSettings() print(settings.api_key) print(settings.database_password) This automatically reads values from environment variables or a .env file (if present), making it ideal for managing sensitive or environment-specific values. ","date":"2025-08-30","objectID":"/pydantic/:4:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"Pydantic SecretStr Pydantic provides special types like SecretStr to handle sensitive information, such as passwords or API keys. These ensure that secrets are not accidentally printed or logged: from pydantic import BaseModel, SecretStr class User(BaseModel): username: str password: SecretStr user = User(username=\"john_doe\", password=\"supersecret\") print(user) # Output: User username='john_doe' password=SecretStr('********') # Access the raw secret value when needed print(user.password.get_secret_value()) You can safely store secrets in environment variables and load them with SecretStr: from pydantic_settings import BaseSettings, SettingsConfigDict from pydantic import SecretStr class SecureSettings(BaseSettings): api_key: SecretStr database_password: SecretStr mongo_uri: str = \"mongodb://localhost:27017\" model_config = SettingsConfigDict(env_file=\".env\", env_prefix=\"APP_\") settings = SecureSettings() print(settings.api_key) # Output: SecretStr('********') ","date":"2025-08-30","objectID":"/pydantic/:4:1","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Python","Pydantic"],"content":"MongoDB example Here‚Äôs a simple example of using Pydantic v2 models with MongoDB: from typing import Annotated from bson import ObjectId from datetime import datetime from pydantic import BaseModel, Field, EmailStr, AfterValidator # Validator for ObjectId def validate_object_id(value: str | ObjectId) -\u003e ObjectId: if not ObjectId.is_valid(value): raise ValueError(\"Invalid ObjectId\") return ObjectId(value) # Annotated alias for validated ObjectId PyObjectId = Annotated[ObjectId, AfterValidator(validate_object_id)] # MongoDB document model class UserDocument(BaseModel): id: PyObjectId = Field(default_factory=ObjectId, alias=\"_id\") name: Annotated[str, Field(min_length=1, max_length=50)] email: EmailStr created_at: datetime = Field(default_factory=datetime.utcnow) class Config: populate_by_name = True # Allow using \"id\" as input even though it's \"_id\" in Mongo arbitrary_types_allowed = True # Allow ObjectId type, since it is not a built-in Pydantic type user = UserDocument(name=\"Alice\", email=\"alice@example.com\") print(user.id) # \u003cObjectId\u003e like 68b2625b99495155b9498fe7 print(user.created_at) # UTC timestamp like 2025-08-30 02:30:51.347367 # This ensures \"_id\" key is present (MongoDB-friendly) print(user.model_dump(by_alias=True)) # Output: # { # \"_id\": ObjectId(\"...\"), # \"name\": \"Alice\", # \"email\": \"alice@example.com\", # \"created_at\": datetime(...) # } You can load a document like from bson import ObjectId mongo_data = { \"_id\": ObjectId(), \"name\": \"Bob\", \"email\": \"bob@example.com\", \"created_at\": datetime.utcnow() } user = UserDocument(**mongo_data) print(user.name) # Bob print(user.id) # Valid ObjectId ","date":"2025-08-30","objectID":"/pydantic/:5:0","tags":["Python","Pydantic","config","environment variables","mongodb"],"title":"Clean Validation with Pydantic v2","uri":"/pydantic/"},{"categories":["Programming","Python","Algorithm"],"content":"Binary search","date":"2025-06-30","objectID":"/binary-search/","tags":["algorithm","programming"],"title":"A Lesson from a Naive Binary Search","uri":"/binary-search/"},{"categories":["Programming","Python","Algorithm"],"content":"A Lesson from a Naive Binary Search I‚Äôve been grinding hard every day, solving coding problems to get better at algorithms. Recently, I came across something interesting‚Äîa naive implementation of binary search can actually cause a bug. It‚Äôs a small detail, but it matters. def binary_search(nums, target): left, right = 0, len(nums) - 1 while left \u003c= right: mid = (left + right) // 2 if nums[mid] == target: return mid elif nums[mid] \u003c target: left = mid + 1 else: right = mid - 1 return -1 It works fine in Python‚Äîbut I recently learned that the way I calculate mid can cause problems in some cases. The issue is with this line: mid = (left + right) // 2 If left and right are very large, their sum might be too big in some languages, causing an overflow. A safer way to write it is: mid = left + (right - left) // 2 This version avoids adding two potentially large numbers. Even something as simple as binary search deserves careful thought. It reminded me that writing correct code isn‚Äôt just about making it work‚Äîit‚Äôs about making it right. ","date":"2025-06-30","objectID":"/binary-search/:1:0","tags":["algorithm","programming"],"title":"A Lesson from a Naive Binary Search","uri":"/binary-search/"},{"categories":["linux","pass"],"content":"Password management","date":"2025-05-24","objectID":"/pass/","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"A Minimalist‚Äôs Guide to pass‚Äî the Unix Password Manager Safely wrangle your secrets from the command-line using GPG encryption and a few intuitive commands. ","date":"2025-05-24","objectID":"/pass/:0:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"1. Why pass? Single-purpose \u0026 transparent ‚Äì every secret is just a GPG-encrypted file in ~/.password-store/. Leverages tools you already trust ‚Äì GnuPG for encryption and standard Unix commands for everything else (grep, git, find, etc.). Portable \u0026 scriptable ‚Äì works the same on any POSIX shell and is easy to automate. Prerequisites GnuPG ‚â• 2.2 pass package (available in most distro repos: pacman -S pass, apt install pass, etc.) A clipboard utility (xclip, xsel, wl-clipboard, or pbcopy on macOS) if you want the copy-to-clipboard feature. ","date":"2025-05-24","objectID":"/pass/:1:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"2. Generate your first GPG key gpg --full-generate-key Key Type ‚Äì pick the default RSA \u0026 RSA (or ECC if you prefer). Key Size ‚Äì 3072 or 4096 bits (stronger ‚áí larger). Expiration ‚Äì choose a sensible period (e.g., 2 years) so compromised keys self-retire. Identity ‚Äì enter the name + e-mail that will label this key. Passphrase ‚Äì a strong one! You‚Äôll type this each time GPG needs your key (or unlock once per session via a GPG agent). ","date":"2025-05-24","objectID":"/pass/:2:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"3. Find your key ID gpg --list-secret-key --keyid-format LONG Look for the line that starts with sec: sec rsa3072/AB12CD34EF56GH78 2025-05-17 [SC] AB12CD34EF56GH78 (16 hexadecimal characters after the slash) is your key ID ‚Äì copy it; we‚Äôll use it to initialise pass. ","date":"2025-05-24","objectID":"/pass/:3:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"4. Initialise pass pass init AB12CD34EF56GH78 What happens? pass creates ~/.password-store/ Every file placed there will be encrypted for the listed key(s). A .gpg-id file records which keys to use so you can share the store with additional people later. (If you ever rotate keys, run pass init --path . newKEYID to re-encrypt subsets of the store.) ","date":"2025-05-24","objectID":"/pass/:4:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"5. Add your first secret pass insert twitter.com pass opens your $EDITOR (= vi, nano, etc.). Type your password on the first line; anything after that is free-form notes (e.g., username, 2FA scratch codes). Save \u0026 quit ‚Äì you‚Äôll be prompted for your GPG passphrase and the file twitter.com.gpg is created inside the store. Directory layout after one entry: ~/.password-store/ ‚îú‚îÄ‚îÄ .gpg-id ‚îî‚îÄ‚îÄ twitter.com.gpg (Feel free to nest categories like Business/github.com, they become sub-directories.) ","date":"2025-05-24","objectID":"/pass/:5:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"6. Display or decrypt secrets Plain display (prints to STDOUT): pass twitter.com # same as `pass twitter` One-off manual decryption (rarely needed, but shows nothing up pass‚Äôs sleeve): gpg -d ~/.password-store/facebook.com.gpg ","date":"2025-05-24","objectID":"/pass/:6:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"7. Copy to clipboard (auto-clear!) pass -c git_token The password is pushed to your clipboard. After 45 seconds (configurable via PASSWORD_STORE_CLIP_TIME), pass industriously scrubs it. ","date":"2025-05-24","objectID":"/pass/:7:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"8. Remove an entry pass rm Business/cheese-whiz-factory Flags worth knowing: -r ‚Üí recursive (delete directories). -f ‚Üí force (skip confirmation). Deleted files go to your desktop trash only if your shell supports it; otherwise they‚Äôre gone forever (but still recoverable via git, see below). ","date":"2025-05-24","objectID":"/pass/:8:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"9. Pro tips \u0026 hygiene Task Command / Tip Version control your store cd ~/.password-store \u0026\u0026 git init \u0026\u0026 git add . \u0026\u0026 git commit -m \"First secret\" With Git you get effortless history and the ability to sync between machines over SSH. Use multiple recipients (e.g., team store) pass init KEYID1 KEYID2 ... ‚Äì future inserts are encrypted for all recipients. Rename or move a secret pass mv oldname newname ‚Äì keeps history intact. Bulk import existing passwords pass import pass-dump.txt or script with pass insert -m \u003cname\u003e. Search pass grep \u003cpattern\u003e ‚Äì greps filenames and decrypted contents. Shell tab-completion Enable the bundled pass.bash-completion or pass.fish-completion for lightning-fast navigation. GUI helpers qtpass, browserpass, passff let your browser/mobile talk to the same store. ","date":"2025-05-24","objectID":"/pass/:9:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"10. Backing up \u0026 restoring Because the store is plain GPG files: tar czf pass-backup-$(date +%F).tar.gz ~/.password-store To restore: tar xzf pass-backup-2025-05-17.tar.gz -C ~/ pass git checkout . (If you kept the Git repo you can just git pull from your remote.) ","date":"2025-05-24","objectID":"/pass/:10:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["linux","pass"],"content":"11. Revoking / rotating your key Generate \u0026 publish a revocation certificate right after key creation: gpg --output ~/revocation.asc --gen-revoke AB12CD34EF56GH78 If the key is ever compromised, import that file (gpg --import revocation.asc) and re-encrypt the store with a new key: gpg --full-generate-key # new key pass init NEWKEYID # re-encrypt everything ","date":"2025-05-24","objectID":"/pass/:11:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/pass/"},{"categories":["python","programming"],"content":"Logging tutorial","date":"2025-05-17","objectID":"/python-logging/","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"A gentle, practical introduction to logging in Python ","date":"2025-05-17","objectID":"/python-logging/:1:0","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Why bother with a dedicated logging library? Prints don‚Äôt scale. print() is fine during quick experiments, but real programs need a record that can be filtered, rotated, or shipped elsewhere. Separation of concerns. You decide what to log in your code; logging decides where and how to write it (console, file, etc.). Built-in, no extra dependency. The standard library‚Äôs logging module is powerful enough for most applications. ","date":"2025-05-17","objectID":"/python-logging/:1:1","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Core concepts Concept Role in the ecosystem Typical examples Logger The entry point your code calls (logger.info(...)). You can have many, one per module. \"__main__\", \"my_package.worker\" Handler Decides where the record goes. StreamHandler (stdout), FileHandler, TimedRotatingFileHandler, SMTPHandler Formatter Decides how the record looks. '%(asctime)s - %(levelname)s - %(name)s - %(message)s' ","date":"2025-05-17","objectID":"/python-logging/:1:2","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"A minimal logger import logging logging.basicConfig( level=logging.INFO, format=\"%(levelname)s | %(message)s\" ) logging.info(\"Hello, world!\") basicConfig is a one-liner good for small scripts. In bigger projects, mixing multiple modules / log files, you‚Äôll want finer control. ","date":"2025-05-17","objectID":"/python-logging/:1:3","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Rotating files at midnight Rotating a log file means creating a new log file after a certain time or size limit is reached. In this case, a new file is created every night at midnight. Only the most recent two log files are kept‚Äîyesterday‚Äôs and today‚Äôs‚Äîwhile older ones are deleted automatically. Keeps log files from growing forever. Eases log shipping/archiving. A single command wipes logs older than n days. The TimedRotatingFileHandler in the helper below: Parameter Value when=\"midnight\" Rotate every day at 00:00 server local time. interval=1 Every 1 unit of when (here: days). backupCount=1 Keep one old file (log_file.log.2025-05-17). Older ones are deleted. encoding=\"utf-8\" Avoids surprises with non-ASCII characters. If you set backupCount=30, it will keep: Today‚Äôs log file (log_file.log), and The 30 most recent rotated log files (log_file.log.2025-05-17, log_file.log.2025-05-16, ‚Ä¶) ","date":"2025-05-17","objectID":"/python-logging/:1:4","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Drop-in helper: get_logger # logger.py import logging from pathlib import Path from logging.handlers import TimedRotatingFileHandler LOG_FILE = Path(\"./logs/log_file.log\") LOG_FILE.parent.mkdir(parents=True, exist_ok=True) # ensure ./logs/ def get_logger(name: str) -\u003e logging.Logger: \"\"\"Return a module-specific logger configured for daily rotation.\"\"\" logger = logging.getLogger(name) logger.setLevel(logging.INFO) # Prevent adding duplicate handlers if the module is imported repeatedly if not logger.handlers: handler = TimedRotatingFileHandler( filename=LOG_FILE, when=\"midnight\", interval=1, backupCount=1, encoding=\"utf-8\", ) formatter = logging.Formatter( \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", ) handler.setFormatter(formatter) logger.addHandler(handler) # Block propagation so the same record is not also printed logger.propagate = False return logger if not logger.handlers:: Guarantees that multiple imports don‚Äôt attach multiple handlers, which would duplicate every line in the output. logger.propagate = False: Stops messages from bubbling up to the root logger, so you don‚Äôt accidentally get console spam when your app also configures a root handler. In other words, ‚ÄúDon‚Äôt pass my log message to parent loggers. I‚Äôll handle it here.‚Äù ","date":"2025-05-17","objectID":"/python-logging/:1:5","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Using the helper in your scripts # worker.py from logger import get_logger logger = get_logger(__name__) def create_job(job_id: str): logger.info(f\"Create: JobID: {job_id}\") Handling exceptions: try: result = do_something() except ProcessException as e: # Log message plus full traceback logger.error(\"%s: %s\", e.code, e.message, exc_info=True) When you‚Äôre inside an except block and you want to record not just the error message, but also where exactly the error happened. This is a case where exc_info comes in: try: 1 / 0 except ZeroDivisionError as e: logger.error(\"An error occurred!\", exc_info=True) This will produce a log like: 2025-05-17 10:23:00 - ERROR - __main__ - An error occurred! Traceback (most recent call last): File \"main.py\", line 2, in \u003cmodule\u003e 1 / 0 ZeroDivisionError: division by zero Without it, you would only see 2025-05-17 10:23:00 - ERROR - __main__ - An error occurred! ","date":"2025-05-17","objectID":"/python-logging/:1:6","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/python-logging/"},{"categories":["python","programming"],"content":"Pathlib tutorial","date":"2025-05-17","objectID":"/pathlib/","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["python","programming"],"content":"From Type Hint to Power Tool: Python‚Äôs Pathlib For a long time, I used Path from Python‚Äôs pathlib module purely as a type hint - a way to make function signatures look more modern and semantically clear. Like this: from pathlib import Path def process_file(file_path: Path): ... It changed when I started building an application that handled user-uploaded documents. I had to create temporary folders, write intermediate files, manage output paths, and ensure directories existed before saving results. That‚Äôs when Path went from just a type hint to a core part of my file management logic. ","date":"2025-05-17","objectID":"/pathlib/:1:0","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["python","programming"],"content":"Why pathlib is Worth More than a Hint Here are the use cases where it transformed my workflow: ","date":"2025-05-17","objectID":"/pathlib/:2:0","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["python","programming"],"content":"Ensuring Directories Exist Before: import os if not os.path.exists(\"output\"): os.makedirs(\"output\") Now: from pathlib import Path Path(\"output\").mkdir(parents=True, exist_ok=True) parents=True: Creates any missing parent directories. exist_ok=True: Prevents an error if the directory already exists. This is equivalent to os.makedirs(), but more cleaner and readable! ","date":"2025-05-17","objectID":"/pathlib/:2:1","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["python","programming"],"content":"Managing Directories While handling temporary files for a file-processing API, I needed to: Create a temp folder Ensure it exists Write intermediate files Clean up afterward With Path, this became natural and structured: from pathlib import Path temp_dir = Path(\"/tmp/myapp\") / \"job123\" temp_dir.mkdir(parents=True, exist_ok=True) output_file = temp_dir / \"translated.txt\" output_file.write_text(\"Translated content\") content = output_file.read_text() write_text(data: str): Writes a string to a text file (overwrites if it exists). read_text(): Reads file content and returns a string. To cleanup the (temporary) directories: # Cleanup later if needed output_file.unlink() temp_dir.rmdir() unlink: Deletes a file rmdir: Removes an empty directory You can use unlink with a parameter missing_ok: Without missing_ok=True: raises FileNotFoundError if the file doesn‚Äôt exist. With missing_ok=True: silently does nothing if the file is already gone. You can also check path status to avoid runtime errors: p = Path(\"result.txt\") p.exists() # True if file or directory exists p.is_file() # True if it's a regular file p.is_dir() # True if it's a directory To list directory contents: p = Path(\"my_folder\") for item in p.iterdir(): print(item) ","date":"2025-05-17","objectID":"/pathlib/:2:2","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["python","programming"],"content":"Path Arithmetic Joining paths becomes expressive with /: base = Path(\"/data\") file = base / \"user\" / \"output.txt\" Compared to: file = os.path.join(\"/data\", \"user\", \"output.txt\") You can also extract name, stem, suffix (file extension) and so on: p = Path(\"/home/user/project/file.txt\") p.name # 'file.txt' p.stem # 'file' p.suffix # '.txt' p.parent # PosixPath('/home/user/project') p.parts # ('/', 'home', 'user', 'project', 'file.txt') ","date":"2025-05-17","objectID":"/pathlib/:2:3","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/pathlib/"},{"categories":["git","programming","vim"],"content":"Git with Vim Fugitive","date":"2025-04-13","objectID":"/vim-fugitive/","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"If you‚Äôre working with Git and Vim, vim-fugitive is an essential plugin that transforms your editor into a full-fledged Git interface. Here‚Äôs how I use Fugitive to review, stage, and commit changes‚Äîwithout ever leaving Vim. ","date":"2025-04-13","objectID":"/vim-fugitive/:0:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Browsing Git History and Logs First Before jumping into edits, it‚Äôs often useful to understand the file‚Äôs history or recent project changes. :Git log ‚Äî shows the project‚Äôs commit history in reverse chronological order :0Gllog ‚Äî shows the history of the current file To explore who changed what in a file: :Git blame Press \u003cEnter\u003e on a line to inspect its commit, or press g? to see other commands. ","date":"2025-04-13","objectID":"/vim-fugitive/:1:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Viewing and Comparing Versions of a File You might want to compare your current changes with previous versions: :Gedit HEAD~2:% ‚Äî opens the file as it was 2 commits ago :Gdiffsplit ‚Äî shows the current file against the index (staged version) Use :Gvdiffsplit for vertical splits or :Ghdiffsplit for horizontal In diff mode, use: do ‚Äî to obtain changes from the other pane dp ‚Äî to put your changes into the other pane ","date":"2025-04-13","objectID":"/vim-fugitive/:2:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Making and Reviewing Changes After edits, you may want to review your local changes: :Gdiffsplit This shows differences between the working copy and the staged version. Use this view to double-check before staging. If you decide to undo your changes: :Gread This reverts the buffer to the version from the index or last commit. ","date":"2025-04-13","objectID":"/vim-fugitive/:3:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Staging and Preparing Commits Now you‚Äôre ready to prepare your changes for commit: :Git This opens a status window with: Untracked files Modified (unstaged) files Staged files You can: Press - to toggle staged/unstaged Press X to discard changes Press dv, dh, or dd to open diff views for review When you‚Äôre satisfied, press: cc to commit staged changes ca to amend the previous commit You‚Äôll get a buffer to write your commit message. Save and close it to finish. ","date":"2025-04-13","objectID":"/vim-fugitive/:4:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Managing Files and Advanced Commands Here are more powerful tools from Fugitive: :Gwrite ‚Äî stage the file (like git add) :GMove / :GRename ‚Äî move or rename a file with Git :GDelete / :GRemove ‚Äî remove a file and buffer :Ggrep, :Glgrep ‚Äî grep across the repository :GBrowse ‚Äî open the current file on GitHub or other providers You can extend :GBrowse with plugins for: GitHub: vim-rhubarb GitLab: fugitive-gitlab.vim Others like Bitbucket, Azure DevOps, and Sourcehut ","date":"2025-04-13","objectID":"/vim-fugitive/:5:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Fugitive Cheat Sheet Here are some quick facts and commands that make vim-fugitive incredibly powerful: :Git with no arguments opens a summary/status window for the current repo :Gdiffsplit or :Gvdiffsplit opens staged vs working tree versions for side-by-side diff :Gread reverts local changes (undo-able with u) :Gwrite stages the file (or updates it from history, depending on context) :Git blame opens interactive blame mode ‚Äî press \u003cEnter\u003e on a line to view its commit, or g? to see available options :Gedit HEAD~3:% opens the current file as it existed 3 commits ago :Ggrep and :Glgrep perform Git-powered searches within the repository ","date":"2025-04-13","objectID":"/vim-fugitive/:6:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"File Management :GMove ‚Äî Perform a git mv on the file and rename the buffer :GRename ‚Äî Like :GMove, but destination is relative to current file :GDelete ‚Äî git rm + close buffer :GRemove ‚Äî git rm + keep buffer open ","date":"2025-04-13","objectID":"/vim-fugitive/:6:1","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"Web Integration Use :GBrowse to open the current file on your Git hosting provider. It even supports line ranges and works well in visual mode. Plugins exist for: GitHub: vim-rhubarb GitLab: fugitive-gitlab.vim Bitbucket, Gitee, Azure DevOps, and others also supported. ","date":"2025-04-13","objectID":"/vim-fugitive/:6:2","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["git","programming","vim"],"content":"üèÅ Final Thoughts Vim Fugitive brings Git right into your fingertips, allowing you to manage version control without ever leaving your editor. Whether you‚Äôre staging, reviewing diffs, or digging into commit history, Fugitive streamlines your workflow and keeps your focus in code. ","date":"2025-04-13","objectID":"/vim-fugitive/:7:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/vim-fugitive/"},{"categories":["python","programming"],"content":"Tutorial for UV python package manager","date":"2025-04-13","objectID":"/uv-package-manager/","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":" üìùUpdate (2025-09-06): I‚Äôve added a new section on using --native-tls with corporate proxies. It covers why uv may fail with SSL errors at work and how to fix it by making uv trust your system certificates. ","date":"2025-04-13","objectID":"/uv-package-manager/:0:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Meet uv¬†‚Äì¬†A Blazingly¬†Fast, All‚Äëin‚ÄëOne Python Package¬†Manager In my last post, I covered Poetry. It‚Äôs a solid dependency manager‚Äîbut it still leaves you juggling other tools: pip for installs, virtualenv for isolation, pyenv for Python versions, and pip-tools or Pipenv for locks. That bounce between tools adds friction. uv removes it. This single, Rust-built project manager‚Äînow one of the most popular tools in the Python ecosystem‚Äîinstalls Python, creates virtual environments, resolves and locks dependencies, and even publishes to PyPI, all from one blazing-fast CLI (often 10‚Äì1000√ó faster). ","date":"2025-04-13","objectID":"/uv-package-manager/:1:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Why Python Packaging Needed a Fresh Start Pain Point Traditional Landscape How uv Fixes It Tool sprawl pip / virtualenv / pyenv / Poetry / pipx Covers the full workflow Performance Pure‚ÄëPython dependency resolution and single‚Äëthreaded downloads Rust core, SAT‚Äëbased solver (PubGrub) Reproducibility requirements.txt is order‚Äësensitive \u0026 lacks metadata Deterministic uv.lock capturing hashes \u0026 markers Since its public launch, uv already powers \u003e10‚ÄØ% of all downloads on PyPI‚Äîevidence that developers crave a faster, simpler workflow. ","date":"2025-04-13","objectID":"/uv-package-manager/:2:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"The Lock‚ÄëFile Landscape at a Glance Before we dive into uv, it helps to remember what ‚Äúlocking‚Äù looks like in the existing ecosystem: Pipenv ‚Äì¬†Pipfile + Pipfile.lock Poetry ‚Äì¬†pyproject.toml + poetry.lock pip ‚Äì¬†requirements.txt created via pip freeze Each of these pins versions, but they store different metadata and often disagree on the resolver algorithm. uv adopts pyproject.toml syntax and adds its own uv.lock that captures exact versions and SHA‚Äë256 hashes for deterministic installs. ","date":"2025-04-13","objectID":"/uv-package-manager/:3:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"From init to publish ","date":"2025-04-13","objectID":"/uv-package-manager/:4:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"1. Spin‚Äëup uv init myapp # ‚ûä scaffold project \u0026 git repo uv chooses a Python interpreter (downloading if necessary), creates .venv, writes .python-version, and commits a minimal pyproject.toml. ","date":"2025-04-13","objectID":"/uv-package-manager/:4:1","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"2. Add Dependencies uv add ruff pytest # ‚ûã add linting \u0026 tests Environment bootstrap ‚Äì if .venv doesn‚Äôt exist, it‚Äôs created. Dependency resolution ‚Äì the PubGrub SAT solver computes a compatible graph (an NP‚Äëhard problem) in milliseconds. Lockfile update ‚Äì uv.lock is regenerated atomically so that users can reproduce the exact set. Installation If you Need to remove something, uv remove pytest cleans both the environment and the lockfile. ","date":"2025-04-13","objectID":"/uv-package-manager/:4:2","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"3. Iterate Quickly uv run hello.py # ‚ûå execute code inside .venv uv run uvicorn main:app --reload # run uvicorn uvx ruff check # ‚ûç run a CLI tool in a throw‚Äëaway env uv includes a dedicated interface for interacting with tools. Tools can be invoked without installation using uv tool run, in which case their dependencies are installed in a temporary virtual environment isolated from the current project. Because it is very common to run tools without installing them, a uvx alias is provided for uv tool run ‚Äî the two commands are exactly equivalent. For brevity, the documentation will mostly refer to uvx instead of uv tool run. ","date":"2025-04-13","objectID":"/uv-package-manager/:4:3","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"4. Manage Python Versions Mid‚ÄëStream uv python install 3.12.0 # ‚ûé fetch new interpreter # switch by editing .python-version, then: uv sync # rebuild .venv deterministically No sudo, no global state‚Äîinterpreters live under ~/.local/share/uv/python. ","date":"2025-04-13","objectID":"/uv-package-manager/:4:4","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"5. Publish uv publish # ‚ûè upload to PyPI Zero extra config; metadata comes from pyproject.toml. ","date":"2025-04-13","objectID":"/uv-package-manager/:4:5","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Workspaces: Multiple Apps, One Cohesive Repo uv can treat a directory tree as a workspace. Sub‚Äëprojects share the same interpreter and .venv, which is perfect for monorepos housing a library, CLI, and docs site side‚Äëby‚Äëside. uv init api # creates api/ inside current repo uv init web --no-workspace # standalone env if you prefer isolation Switching between shared and isolated workflows is a flag away. ","date":"2025-04-13","objectID":"/uv-package-manager/:5:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Dependency Groups for Dev \u0026 Prod Sometimes you want linting and testing tools only in development. uv follows Poetry‚Äôs group syntax: uv add --dev pytest ruff # added under [tool.poetry.group.dev] uv sync --no-group dev # skip dev deps ","date":"2025-04-13","objectID":"/uv-package-manager/:6:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Testing with Pytest (and a VS¬†Code Tip) When you start a project, it‚Äôs worth deciding up front where your test files will live. I like to keep them in a dedicated tests/ directory that sits alongside the src/ directory rather than inside it. That keeps production code and test code clearly separated and makes the project tree easy to scan: project_root/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ main.py ‚îÇ ‚îî‚îÄ‚îÄ utils.py ‚îî‚îÄ‚îÄ tests/ ‚îú‚îÄ‚îÄ test_main.py ‚îî‚îÄ‚îÄ test_utils.py With this layout, tools such as pytest find your tests automatically, and IDEs can apply different run configurations or linters to src/ and tests/ without extra setup. Enable pytest discovery in VS¬†Code by adding: // .vscode/settings.json { \"python.testing.pytestEnabled\": true, \"python.testing.cwd\": \"${workspaceFolder}/tests\" } Install and run tests: uv add --dev pytest uv run pytest -q ","date":"2025-04-13","objectID":"/uv-package-manager/:7:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Migrating an Existing venv + pip Project Freeze current deps: pip freeze \u003e requirements.txt Initialize uv in place and create a virtual environment: uv init . uv venv Import: uv pip install -r requirements.txt uv lock Delete the old .venv and enjoy the speed‚Äëup. ","date":"2025-04-13","objectID":"/uv-package-manager/:8:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Managing Private PyPI Repositories with uv If you‚Äôre using uv as your Python package manager, you don‚Äôt always need to pass --index-url when installing packages from a private or internal PyPI mirror (e.g, at your work). You can configure it globally with # ~/.config/uv/uv.toml [[index]] url = \"http://----\" default = true Now, you can just run uv add \u003cpackage-name\u003e ","date":"2025-04-13","objectID":"/uv-package-manager/:9:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Using uv with Private Indexes and Corporate Proxies If you work in a corporate environment, chances are you‚Äôre installing packages through an internal PyPI mirror and behind a company proxy. Here‚Äôs how to make uv work smoothly in that setup. ","date":"2025-04-13","objectID":"/uv-package-manager/:10:0","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Configuring a Private PyPI Index Instead of typing --index-url every time you add a package, you can configure your internal index globally: # ~/.config/uv/uv.toml [[index]] url = \"http://repo.mycompany.net/simple\" default = true Now you can simply run: uv add numpy and uv will fetch packages from your internal repository automatically. ","date":"2025-04-13","objectID":"/uv-package-manager/:10:1","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Setting Proxy Variables If your network requires a proxy, set these environment variables: export HTTP_PROXY=\"http://proxy.mycompany.net:3128\" export HTTPS_PROXY=\"http://proxy.mycompany.net:3128\" If authentication is required: export HTTPS_PROXY=\"http://username:password@proxy.mycompany.net:3128\" And to bypass the proxy for local services: export NO_PROXY=\"localhost,127.0.0.1,.mycompany.net\" ","date":"2025-04-13","objectID":"/uv-package-manager/:10:2","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Why --native-tls Matters By default, uv uses Rustls for TLS/SSL. That‚Äôs fine at home, but at work you‚Äôll often hit errors like: certificate verify failed: unable to get local issuer certificate This happens because Rustls doesn‚Äôt automatically trust your company‚Äôs custom root certificates. The fix: tell uv to use your operating system‚Äôs certificate store: uv add --native-tls requests or make it permanent in your config: # ~/.config/uv/uv.toml native-tls = true Now uv respects the certificates your IT team has already installed (OpenSSL on Linux, Schannel on Windows, SecureTransport on macOS). ","date":"2025-04-13","objectID":"/uv-package-manager/:10:3","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["python","programming"],"content":"Cheat Sheet pip / virtualenv uv python -m venv .venv uv venv pip install pkg uv add pkg pip install -r requirements.txt uv pip install -r requirements.txt pip uninstall pkg uv remove pkg pip freeze uv pip freeze pip list uv pip list I hope you enjoyed my post! ","date":"2025-04-13","objectID":"/uv-package-manager/:10:4","tags":["python","uv","package manager","Virtual Environment"],"title":"UV Tutorial: All‚Äëin‚ÄëOne Python Package¬†Manager!","uri":"/uv-package-manager/"},{"categories":["NLP","LLM","Deep Learning"],"content":"A Gentle Guide to DeepSeek","date":"2025-02-14","objectID":"/deepseek-inside/","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"DeepSeek‚Äôs latest moves have sent ripples through the AI community. Not only has it marked the beginning of a new era in artificial intelligence, but it has also made significant contributions to the open-source AI landscape. Their engineering techniques behind DeepSeek are truly impressive, and their reports are quite enjoyable. However, understanding their core ideas can be challenging and demands a substantial amount of effort. At the forefront of this innovation is DeepSeek-R1, a model that built upon the foundation established by preceding projects such as DeepSeek Coder, Math, MoE, and notably, the DeepSeek-V3 model. While DeepSeek-R1 is the center of the DeepSeek‚Äôs frenzy, its success is rooted on these past works. To help general readers navigate DeepSeek‚Äôs innovations more easily, I decided to write this post as a gentle introduction to their key components. I will begin by exploring the key ideas of V3 model, which serves as a cornerstone for DeepSeek-R1. I hope that this post will provide a clear and accessible explanation of their major contributions. Also, I strongly encourage you to read their reports :) ","date":"2025-02-14","objectID":"/deepseek-inside/:0:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Contents Multi-Head Latent Attention Quick Review of Multi-Head Attention Low-Rank Joint Compression Efficient Computation Without Explicit Key \u0026 Value Computation Decoupled RoPE Mixture-of-Experts in DeepSeek The Role of Shared Experts Group Relative Policy Optimization Proximal Policy Optimization GRPO: PPO for DeepSeek DeepSeek R1-Zero Conclusion Multi-Head Latent Attention ","date":"2025-02-14","objectID":"/deepseek-inside/:1:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Quick Review of Multi-Head Attention The query, key, and value in a standard multi-head attention (MHA) mechanism can be expressed as follows: \\begin{align*} \\mathbf{q}_t \u0026= W^{Q}\\mathbf{h}_t\\\\ \\mathbf{k}_t \u0026= W^{K}\\mathbf{h}_t\\\\ \\mathbf{v}_t \u0026= W^{V}\\mathbf{h}_t \\end{align*} $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in \\mathbb{R}^{d_hn_h}$ $\\mathbf{h}_t\\in \\mathbb{R}^{d}$: Attention input of the $t$-th token at an layer. $d_h$: the attention head‚Äôs dimension $n_h$: the number of attention heads During inference, all keys and values need to be cached to speed up computation. A cache requirement of MHA is roughly $2n_hd_hl$ elements per token (i.e., key, value for across all layers and heads). This heavy KV cache creates a major bottleneck, limiting the maximum batch size and sequence length during deployment. ","date":"2025-02-14","objectID":"/deepseek-inside/:1:1","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Low-Rank Joint Compression DeepSeek addresses this memory-intensive KV caching problem by introducing an alternative attention mechanism called Multi-Head Latent Attention (MLA). The core idea behind MLA is to compress keys and values into a low-dimensional latent space. Let‚Äôs break it down step by step: \\begin{align*} \\mathbf{c}_t^{KV} \u0026= W^{DKV}\\mathbf{h}_t\\\\ [\\mathbf{k}_{t,1}^C; \\mathbf{k}_{t,2}^C; \\dots ;\\mathbf{k}_{t,n_h}^C] = \\mathbf{k}_t^{C} \u0026= W^{UK}\\mathbf{c}_t^{KV}\\\\ \\mathbf{k}_t^{R} \u0026= \\text{RoPE}(W^{KR}\\mathbf{h}_t)\\\\ \\mathbf{k}_{t,i} \u0026= [\\mathbf{k}_{t,i}^C;\\mathbf{k}_t^{R}]\\\\ [\\mathbf{v}_{t,1}^C; \\mathbf{v}_{t,2}^C; \\dots ;\\mathbf{v}_{t,n_h}^C] = \\mathbf{v}_t^{C} \u0026= W^{UV}\\mathbf{c}_t^{KV} \\end{align*} The superscripts $D$ and $U$ denote the up- and down- projection, respectively. $\\mathbf{c}_t^{KV}\\in \\mathbb{R}^{d_c}$ is the compressed latent vector for keys and values, where $d_c\\ll d_hn_h$. Note that this is not a query vector. $W^{DKV}\\in \\mathbb{R}^{d_c\\times d}$ is the down-projection matrix that generates the latent vector $\\mathbf{c}_t^{KV}$. $W^{UK},W^{UV}\\in \\mathbb{R}^{d_hn_h\\times d_c}$ are the up-projection matrices for keys and values, respectively. These operations help reconstruct the compressed information of $\\mathbf{h}_t$. $W^{KR}\\in \\mathbb{R}^{d_h^R\\times d}$ is the matrix responsible for generating the positional embedding vector. I will explain it soon. Unlike standard attention mechanisms like Grouped-Query Attention (GQA) or Multi-Query Attention (MQA), MLA only needs to cache the compressed vector $\\mathbf{c}_t^{KV}$ during inference. MLA does not reduce the number of keys and values, allowing it to maintain the full representational power of self-attention while alleviating memory bottlenecks. The following figure provides an overview of KV-caching approaches in various attention mechanisms. Efficient Computation Without Explicit Key and Value Computation A key advantage of MLA is that it avoids explicit computation of the full-sized key and value matrices. Instead, attention scores can be computed as follows: \\begin{align*} q_t^Tk_t \u0026= (W^{UQ}c_t^Q)^T(W^{UK}c_t^{KV})\\\\ \u0026= (c_t^Q)^T(W^{UQT}W^{UK})c_t^{KV}, \\end{align*} where $W^{UQT}W^{UK}$ is a pre-computed matrix product of the two projection matrices. Similarly, for values: \\begin{align*} o_{t,i} = \\text{AttnScore}\\cdot v_t^C. \\end{align*} The final output is given by \\begin{align*} u_t \u0026= W^O[o_{t,1}, \\dots, o_{t,n_h}]\\\\ \u0026= W^O[\\text{AttnScore}\\cdot (W^{UV}c_t^{KV})]\\\\ \u0026= W^OW^{UV}[\\text{AttnScore}\\cdot (c_t^{KV})], \\end{align*} where $W^O\\in \\mathbb{R}^{d\\times d_hn_h}$ is the output projection matrix. Decoupled RoPE A potential issue with MLA is how to incorporate positional embeddings (PE). While sinusoidal positional embeddings are a straightforward option, research has shown that Rotary Positional Embedding (RoPE) tends to provide better performance. However, applying RoPE in MLA poses a challenge: normally, RoPE modifies keys and values directly, which would require explicit computation of keys (i.e, $\\mathbf{k}_t^{C}=W^{UK} c_t^{KV}$)‚Äîdefeating the MLA‚Äôs efficiency. DeepSeek circumvents this issue by introducing an explicit positional embedding vector $\\mathbf{k}_t^{R}$, called decoupled RoPE. The PE vector is separately broadcasted across the keys. This allows MLA to benefit from RoPE without losing the efficiency gains of its compression scheme. To further reduce activation memory during training, DeepSeek also compresses queries: \\begin{align*} \\mathbf{c}_t^{Q} \u0026= W^{DQ}\\mathbf{h}_t\\\\ [\\mathbf{q}_{t,1}^C; \\mathbf{q}_{t,2}^C; \\dots ;\\mathbf{q}_{t,n_h}^C] = \\mathbf{q}_t^{C} \u0026= W^{UQ}\\mathbf{c}_t^{Q}\\\\ [\\mathbf{q}_{t,1}^R; \\mathbf{q}_{t,2}^R; \\dots ;\\mathbf{q}_{t,n_h}^R] = \\mathbf{q}_t^{R} \u0026= \\text{RoPE}(W^{QR}\\mathbf{c}_t^Q)\\\\ \\mathbf{q}_{t,i} \u0026= [\\mathbf{q}_{t,i}^C;\\mathbf{q}_{t,i}^{R}] \\end{align*} $\\mathbf{c}_t^{Q}\\in \\mathbb{R}^{d_c‚Äô}$ is the compressed latent vector for queries, where $d_c‚Äô\\ll d_hn_h$ $W^{DQ}\\in \\mathbb","date":"2025-02-14","objectID":"/deepseek-inside/:1:2","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"The Role of Shared Experts Unlike traditional MoE models, where all experts compete to process tokens, DeepSeek‚Äôs shared experts are always active. These experts are responsible for capturing and consolidating common knowledge across different input contexts. By concentrating general knowledge within shared experts, DeepSeek reduces redundancy among routed experts. This separation of responsibilities allows routed experts to focus more effectively on specialized tasks, leading to better model efficiency and performance. Group Relative Policy Optimization DeepSeek introduces a reinforcement learning (RL) algorithm called Group Relative Policy Optimization (GRPO), a simple variant of Proximal Policy Optimization (PPO). If you have a basic understanding of RL and PPO, you‚Äôll find GRPO quite straightforward. Let‚Äôs first review PPO. ","date":"2025-02-14","objectID":"/deepseek-inside/:2:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Proximal Policy Optimization PPO was proposed to address the instability of the vanilla policy gradient algorithm (i.e., REINFORCE algorithm). The core idea of PPO is to stabilize the policy update process by restricting the amount of update. The objective function of PPO is given by \\begin{align*} \\theta_{k+1} = \\operatorname{argmax}_{\\theta} \\mathbb{E}_{s,a\\sim \\pi_{\\theta_k}} [L(s, a, \\theta_k, \\theta)], \\end{align*} where $\\theta_{k}$ is a parameter of a policy network at $k$-th step, $\\theta$ is the current policy we want to update, and the $A$ is the advantage (\\i.e., reward). Finally, the loss function $L$ is given by \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text {k}}}\\left(a | s\\right)} A^{\\pi_{\\theta_k}}(s,a), \\text{ Clip}\\Bigg(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1-\\varepsilon, 1+\\varepsilon\\Bigg) A^{\\pi_{\\theta_k}}(s,a)\\right). \\end{align*} Roughly, $\\varepsilon$ is a hyperparameter which says how far away the new policy is allowed to go from the old one. A simpler expression of the above expression is \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text {k}}}\\left(a | s\\right)} A^{\\pi_{\\theta_k}}(s,a), g(\\varepsilon, A^{\\pi_{\\theta_k}}(s,a)) \\right), \\end{align*} where \\begin{align*} g(\\varepsilon,A) = \\begin{cases} (1+\\varepsilon)A \u0026 A\\geq 0\\\\ (1-\\varepsilon)A \u0026 A\u003c 0. \\end{cases} \\end{align*} There are two cases: $A\\geq 0$: The advantage for that state-action pair is positive, in which case its contribution to the objective reduces to \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1+\\varepsilon \\right) A^{\\pi_{\\theta_k}}(s,a). \\end{align*} Since the advantage is positive, the objective function increases if the action $a$ becomes likely under the policy (i.e., $\\pi_{\\theta}(a|s)$ increases). However, the $\\min$ operator limits how much the objective can increase. Once \\begin{align*} \\pi_{\\theta}(a|s) \u003e (1+\\epsilon) \\pi_{\\theta_k}(a|s), \\end{align*} the $\\min$ ensures that the term is capped at $(1+\\epsilon) A^{\\pi_{\\theta_k}}(s,a)$. This prevents the new policy from straying too far from the old policy. $A\u003c0$: The advantage for that state-action pair is negative, its contribution is given by \\begin{align*} L(s, a, \\theta_k, \\theta) = \\max \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1-\\varepsilon \\right) A^{\\pi_{\\theta_k}}(s,a). \\end{align*} Since the advantage is negative, the objective function increases if the action $a$ becomes less likely (i.e., $\\pi_{\\theta}(a|s)$ decreases). The $\\max$ operator limits this increase. Once \\begin{align*} \\pi_{\\theta}(a|s) \u003c (1-\\varepsilon) \\pi_{\\theta_k}(a|s), \\end{align*} the $\\max$ caps the term at $(1-\\varepsilon) A^{\\pi_{\\theta_k}}(s,a)$, again, ensuring that the new policy does not deviate too far from the old policy. In sum, clipping serves as a regularizer by restricting the rewards to the policy, which change it dramatically with the hyperparameter $\\varepsilon$ corresponds to how far away the new policy can go from the old while still profiting the objective. ","date":"2025-02-14","objectID":"/deepseek-inside/:3:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"GRPO: PPO for DeepSeek GRPO can be expressed as follows: \\begin{align*} \\mathcal{J} = \\frac{1}{G}\\sum_{i=1}^{G} \\min \\left(\\frac{\\pi_{\\theta}\\left(o_i | q\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(o_i | q\\right)} A_i, \\text{ Clip}\\left(\\frac{\\pi_{\\theta}\\left(o_i | q\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(o_i | q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right) -\\beta D_{KL}(\\pi_{\\theta}| \\pi_{\\text{ref}}). \\end{align*} GRPO first samples a group of outputs ${o_1, o_2,\\dots, o_G }$ from the old policy $\\pi_{\\theta_k}$ Then it optimizes the policy model by maximizing the objective The KL-divergence is used to restrict the sudden change of policy. The advantage can be calculated by averaging and normalizing the rewards. Instead of using a value model explicitly, GRPO computes a value of the states (i.e., $o_i$) by averaging them. Note that $A(s,a) = Q(s,a)-V(s)$ $\\pi_{\\text{ref}}$ is the reference model, which is an initial SFT model. DeepSeek-R1 DeepSeek-R1 is essentially a large language model fine-tuned using reinforcement learning. The process begins with training the DeepSeek-V3 model using the GRPO technique described earlier. Before applying RL, the model is pre-tuned with a small, carefully curated set of warm-up data designed to encourage logical outputs in a Chain-of-Thought (CoT) format. This preliminary step significantly improves training stability. Interestingly, DeepSeek‚Äôs researchers first experimented with pure RL training‚Äîwithout any supervised signals‚Äîwhich led to the creation of DeepSeek-R1-Zero. Here are some observations from that experiment and my opinions. ","date":"2025-02-14","objectID":"/deepseek-inside/:4:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"DeepSeek R1-Zero To train DeepSeek-R1-Zero, a rule-based reward signal was adopted. Two types of rewards are used: Accuracy rewards: The reward model evaluates whether the response is correct. For example, in math problems with deterministic results, the model is required to provide the final answer in a specified format, enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. Format rewards: In addition to the accuracy reward model, they employed a format reward model that enforces the model to put its thinking process between \u003cthink\u003e and \u003c/think\u003e tags. Notably, DeepSeek-R1 does not rely on a neural reward model‚Äîlikely because neural models may not consistently provide reliable rewards for training. The team also reported an intriguing ‚Äúaha moment‚Äù with DeepSeek-R1-Zero: ‚ÄúDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.‚Äù However, such behavior appears infrequently. More often, DeepSeek-R1-Zero tends to generate gibberish outputs, which may be attributed to the inherently unstable nature of RL training. Conclusion In this post, I‚Äôve introduced some of the core ideas behind the DeepSeek-v3 and R1 models. While their deep learning techniques are undoubtedly interesting, I find their hardware-level engineering particularly more impressive. In my opinion, their success lies in these engineering achievements, and I look forward to exploring this aspect in a forthcoming post. ","date":"2025-02-14","objectID":"/deepseek-inside/:5:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/deepseek-inside/"},{"categories":["python","programming"],"content":"Tutorial for Python's abstract classes and protocols","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"Introduction When it comes to writing clean, maintainable, and scalable Python code, design matters. As your projects grow, you‚Äôll often find yourself needing to enforce structure, ensure consistency, and promote reusability. This is where Python‚Äôs Abstract Base Classes (ABCs) and Protocols come into play‚Äîtwo powerful features that help you design better software. Abstract classes act as blueprints for other classes, allowing you to define methods that must be implemented by any subclass. They‚Äôre typically used for creating a shared foundation while enforcing a specific structure. Protocols, on the other hand, take a more flexible approach. Instead of relying on inheritance, they let you define interfaces based on behavior, making them ideal for duck typing (or structural subtyping) and runtime flexibility. However, when should you use abstract classes, and when are protocols the better choice? How do these concepts differ, and what problems do they solve? In this blog post, we‚Äôll explore these questions in detail. Through clear explanations and practical examples, you‚Äôll learn how to leverage abstract classes and protocols to write more elegant and maintainable Python code. Whether you‚Äôre designing a small script or a large-scale application, these tools will help you take your Python skills to the next level. Let‚Äôs dive in! What are Abstract Base Classes An abstract class in Python is a class that cannot be instantiated on its own and is designed to be a blueprint for other classes. It allows you to define methods that must be created within any child classes built from the abstract class. Abstract classes are used primarily in situations where a base class is required to define a common interface for a set of derived classes. Consistency: Ensures that all subclasses implement certain methods, providing a consistent interface. Documentation: Serves as a form of documentation by clearly specifying which methods need to be implemented. Design: Helps in designing a robust architecture by defining a template for subclasses. ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:0:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"Pure ABCs (ABC as Interface) The simplest way to use an ABC is as a pure ABC, for example: from abc import ABC, abstractmethod class Animal(ABC): @abstractmethod def walk(self) -\u003e None: pass @abstractmethod def speak(self) -\u003e None: pass Here we have defined an ABC Animal with two methods: walk and speak. Note that the way to do this is to subclass ABC and to decorate the methods that must be implemented (i.e. part of the ‚Äúinterface‚Äù) with the @abstractmethod decorator. Now we can implement this ‚Äúinterface‚Äù to create a Dog class Dog(Animal): def walk(self) -\u003e None: print(\"This is a dog walking\") def speak(self) -\u003e None: print(\"Woof!\") This works as expected. However, if we forget to implement the speak method, Python will raise an error when we try to instantiate the class: \u003e\u003e\u003e dog = Dog() TypeError: Can't instantiate abstract class Dog with abstract method speak We can see that we get an error because we haven‚Äôt implemented the abstract method speak. This ensures that all subclasses implement the correct ‚Äúinterface‚Äù. ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:1:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"ABCs as a tool for code reuse Another, and probably more common, use case for ABCs is for code reuse. Below is a slightly more realistic example of a base class for a statistical or Machine Learning regression model from abc import ABC, abstractmethod from typing import List, TypeVar import numpy as np T = TypeVar(\"T\", bound=\"Model\") class Model(ABC): def __init__(self): self._is_fitted = False def fit(self: T, data: np.ndarray, target: np.ndarray) -\u003e T: fitted_model = self._fit(data, target) self._is_fitted = True return fitted_model def predict(self, data: np.ndarray) -\u003e List[float]: if not self._is_fitted: raise ValueError(f\"{self.__class__.__name__} must be fit before calling predict\") return self._predict(data) @property def is_fitted(self) -\u003e bool: return self._is_fitted @abstractmethod def _fit(self: T, data: np.ndarray, target: np.ndarray) -\u003e T: pass @abstractmethod def _predict(self, data: np.ndarray) -\u003e List[float]: pass In this example, the Model class provides a reusable structure for fitting and predicting data, while leaving the implementation of _fit and _predict to subclasses. Protocols ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:2:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"Dynamic v.s. Static Typing To better understand protocols, it‚Äôs important to first grasp the concept of typing in Python. Python is a dynamically typed language. What does that mean? In Python, type declarations are not required. For example, you can define a function without specifying the types of its arguments or its return type: def simple_function(a, b): return a + b Types are handled and checked at runtime. You can call simple_function with integers, floats, or a mix of both, and the return type will depend on the input: result = simple_function(2, 8) # type(result) -\u003e int result = simple_function(1.4, 9) # type(result) -\u003e float Compare this to a statically typed language like C, where type declarations are mandatory: int simple_function(int a, int b) { return a + b; } In C, providing any other type would result in a compilation error. For example, the following code would not compile: int result = simple_function(2.2, 9); This highlights a key benefit of statically typed languages: types are checked at compile time, so you‚Äôre less likely to encounter type-related issues at runtime. In Python, however, you might run into type-related errors at runtime, which could have been caught earlier in a statically typed language. On the flip side, dynamically typed languages like Python offer greater flexibility when it comes to the types they accept. They also eliminate the need for explicit type declarations, which can be a boon for productivity. ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:3:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"Duck Typing Dynamic typing is often referred to as duck typing, a concept captured by the saying: If it walks like a duck and it quacks like a duck, then it must be a duck. In programming terms, this means that if an object behaves like a certain type (i.e., it has the required methods and attributes), it can be treated as that type. Protocols embrace this concept, allowing you to define interfaces based on behavior rather than explicit inheritance. from typing import Protocol class Flyer(Protocol): def fly(self) -\u003e None: ... def let_it_fly(entity: Flyer) -\u003e None: entity.fly() class Bird: def fly(self) -\u003e None: print(\"Bird is flying\") class Airplane: def fly(self) -\u003e None: print(\"Airplane is flying\") bird = Bird() airplane = Airplane() # Both Bird and Airplane instances can be passed to let_it_fly, thanks to Duck Typing let_it_fly(bird) # Output: Bird is flying let_it_fly(airplane) # Output: Airplane is flying In this example, both Bird and Airplane implement the flay method, so they can be treated as instances of the Flyer protocol. This flexibility is one of the key strengths of duck typing and protocols in Python. ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:4:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["python","programming"],"content":"So ABCs or Protocols? In summary, here are the best practices for choosing between ABCs and Protocols: When to use abstract classes: When you need to share common implementation code. When you want to enforce a strict class hierarchy. When to use protocols: When you care about behavior, not implementation. When you want to avoid tight coupling between classes. Avoid overusing inheritance; prefer composition where possible. ","date":"2025-01-28","objectID":"/python-protocol-abstract-classes/:5:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/python-protocol-abstract-classes/"},{"categories":["programming","linux"],"content":"Setting Up DL/ML Environments","date":"2025-01-11","objectID":"/pytorch-container/","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","linux"],"content":"Setting Up DL Experiment Environments ","date":"2025-01-11","objectID":"/pytorch-container/:0:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","linux"],"content":"A Challenge for Arch Linux Users If you‚Äôve ever tried to set up a new experiment environment for deep learning on Arch Linux, you‚Äôre probably familiar with the challenges involved. Arch Linux, renowned for its rolling-release model and cutting-edge updates, provides unparalleled flexibility and control over your system. However, this same flexibility can often lead to headaches when setting up complex environments for machine learning or deep learning experiments. Dependency conflicts, missing libraries, and version mismatches are all too common. One particular pain point is the setup of Python environments using tools like Conda or virtual environments. While these tools work seamlessly on many systems, Arch Linux users often encounter version conflicts. Installing Conda itself can be tricky and painful. For researchers and developers, this process can feel like a significant barrier to productivity. Instead of focusing on model development or data analysis, hours are spent troubleshooting environment issues. On Arch Linux, where package versions are always at the bleeding edge, finding compatibility between your system and the tools required by frameworks like PyTorch can be very challenging. This is where Docker steps in to save the day. By using Docker containers, you can create isolated, portable environments that encapsulate all the dependencies you need, regardless of the host operating system. For PyTorch users who rely on a CPU-only setup for studying DL/ML and testing PyTorch code, Docker offers a streamlined solution to avoid the usual hassle of configuring local environments on Arch Linux. In this blog post, I will go over the process of setting up a PyTorch container using Docker, exploring how it simplifies the creation of a reproducible environment for your deep learning experiments. ","date":"2025-01-11","objectID":"/pytorch-container/:1:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","linux"],"content":"Install Docker on Arch sudo pacman -S docker docker-compose docker-buildx Then, sudo systemctl enable --now docker.service ","date":"2025-01-11","objectID":"/pytorch-container/:2:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","linux"],"content":"PyTorch Container ","date":"2025-01-11","objectID":"/pytorch-container/:3:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","linux"],"content":"Steps for Using PyTorch with CPU-Only Docker Image Pull the CPU-Only PyTorch Docker Image The latest PyTorch images without CUDA can be pulled using the following command: docker pull pytorch/pytorch:latest If you prefer a specific version, for example, PyTorch 1.13.1, use: docker pull pytorch/pytorch:1.13.1-cpu Run the PyTorch Container Start the container interactively: docker run -it --rm pytorch/pytorch:latest This will launch a Python environment with PyTorch installed. Mount Your Project Files (Optional) If you want to access your local project files inside the container, mount a directory: docker run -it --rm -v $(pwd):/workspace pytorch/pytorch:latest -it -i: Keeps STDIN open, allowing you to interact with the container (important for running interactive shells or REPLs). -t: Allocates a pseudo-TTY, which is useful for interactive sessions. --rm: This flag automatically removes the container when it stops. It‚Äôs useful to avoid cluttering your system with stopped containers. -v $(pwd):/workspace: This is the volume flag for mounting a directory. Here‚Äôs how it works: $(pwd) refers to the current working directory on your host machine (outside the container). /workspace is the directory inside the container where the mounted files will be accessible. The files inside /workspace in the container are directly linked to the files in your host machine‚Äôs current directory. If you modify a script in your host machine, the changes will be visible inside the container immediately. Similarly, if you create or edit a file inside the /workspace directory in the container, the changes will reflect on your host machine. Install Additional Python Libraries (If Needed) Install any extra libraries required for your project: pip install \u003cpackage-name\u003e To save this setup for future use, create a custom Docker image with these dependencies pre-installed. Write and Test PyTorch Code Create a simple PyTorch script (e.g., test.py): import torch # Check if CUDA is available print(\"CUDA Available:\", torch.cuda.is_available()) # Perform a simple tensor operation x = torch.tensor([5.0, 10.0, 15.0]) print(\"Tensor:\", x) Run it inside the container: python test.py The output should confirm that CUDA is not available and display the tensor. Save and Exit To persist changes, save your code in the mounted directory (e.g., /workspace). You can also commit the container if you‚Äôve made extensive modifications: exit docker ps -a # Find the container ID docker commit \u003ccontainer_id\u003e my_pytorch_cpu_image Note that the container would not appear if you run the container with --rm. ","date":"2025-01-11","objectID":"/pytorch-container/:3:1","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/pytorch-container/"},{"categories":["programming","python"],"content":"Introduction to asyncio library","date":"2025-01-05","objectID":"/asyncio/","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"For the past few months, I‚Äôve been working on an exciting internal project at my company: taking users‚Äô documents and running them through LLM APIs to translate and summarize their content, somewhat similar to DeepL. The output is a collection of translated documents, each overlaid with the newly translated text. Our goal is to provide a stable service that can handle large files efficiently for thousands of employees at Samsung‚Äîno small task! To achieve this, we needed a concurrency strategy that supports high throughput while remaining responsive. That‚Äôs where Asyncio comes in. In this post, we‚Äôll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We‚Äôll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle large-scale I/O more effectively. Whether you‚Äôre building an internal service for employees or creating a high-performance web server, Asyncio‚Äôs approach to concurrency might just be the key to unlocking the scalability you need. A Deep Dive into Asynchronous I/O Modern software frequently needs to handle large volumes of input/output (I/O) operations. For instance, you might be retrieving data from web services, communicating with microservices over a network, or running multiple database queries simultaneously. These tasks can often take hundreds of milliseconds‚Äîor even seconds‚Äîif the network is under heavy load or the database is busy. If you approach these operations in a strictly synchronous manner‚Äîdoing one after another‚Äîeach I/O request can block the execution of your entire application. When you have many such requests, total execution time can balloon significantly. Picture having to process 100 requests, each taking 1 second. Doing that sequentially results in a 100-second runtime. However, if you can handle them concurrently, you might complete all in roughly the same amount of time as a single request. In this post, we‚Äôll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We‚Äôll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle I/O more efficiently. ","date":"2025-01-05","objectID":"/asyncio/:0:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Why Concurrency Matters ","date":"2025-01-05","objectID":"/asyncio/:1:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"The Synchronous Bottleneck In a synchronous application, each line of code must complete before moving on to the next. This is usually acceptable for simple tasks but becomes problematic if a single operation is slow or unresponsive. While any operation can block an application, many applications will be stuck waiting for I/O. I/O refers to a computer‚Äôs input and output devices such as a keyboard, hard drive, and, most commonly, a network card. A classic example is a web server that processes each request in series; if one request takes longer than expected, all subsequent requests are delayed. Users of a slow website or client application may experience hang-ups, timeouts, or sluggish responsiveness due to this ‚Äúqueue‚Äù of operations. ","date":"2025-01-05","objectID":"/asyncio/:1:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Concurrency as a Solution Concurrency allows multiple tasks to be in progress simultaneously. In code terms, this often means starting multiple operations and then efficiently switching between them, so the application doesn‚Äôt grind to a halt waiting on just one task. For I/O-bound tasks, concurrency can provide remarkable speedups because while one operation is waiting on a response, your program can continue working on other tasks. ","date":"2025-01-05","objectID":"/asyncio/:1:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Concurrency vs. Parallelism It‚Äôs important to distinguish concurrency from parallelism: Concurrency means you can have multiple tasks in progress at once, but they are not necessarily all running at the exact same moment. Parallelism means two or more tasks truly run at the same time, which requires at least as many CPU cores as the number of tasks you want to run in parallel. Even on a single-core machine, you can achieve concurrency by rapidly switching (or time slicing) between tasks. However, true parallelism requires multiple CPU cores, letting tasks run simultaneously without interrupting each other. ","date":"2025-01-05","objectID":"/asyncio/:1:3","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"The I/O-Bound vs. CPU-Bound Distinction When we label a particular operation as I/O-bound or CPU-bound, we‚Äôre describing what fundamentally limits its performance. I/O-Bound: The operation spends most of its time waiting for I/O devices such as hard drives or network interfaces. Examples include fetching a remote web page, reading from a file, or waiting on a database query. These tasks can benefit significantly from concurrency, because while one operation waits, the program can do other work. CPU-Bound: The task is primarily gated by processor speed. Examples include computing the nth Fibonacci number using a recursive function, performing complex data analysis, or running CPU-intensive algorithms. Concurrency alone may not help here, especially in Python, because of the Global Interpreter Lock (GIL). ","date":"2025-01-05","objectID":"/asyncio/:2:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"A Primer on Processes, Threads, and the GIL ","date":"2025-01-05","objectID":"/asyncio/:3:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Processes A process is a running instance of an application with its own memory space. An example of creating a Python process would be running a simple ‚Äúhello world‚Äù application or typing python at the command line to start up the REPL (read eval print loop). Modern operating systems allow multiple processes to run at once. If your CPU has multiple cores, it can execute processes truly in parallel. Otherwise, the OS uses time slicing to rapidly switch among processes. ","date":"2025-01-05","objectID":"/asyncio/:3:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Threads A thread is a more lightweight form of concurrency that runs within a single process, sharing the parent process‚Äôs memory space. Threads have no their own memory. A process will always have at least one thread associated with it, usually known as the main thread. A process can also create other threads, which are more commonly known as worker or background threads. These threads can perform other work concurrently alongside the main thread. Threads, much like processes, can run alongside one another on a multi-core CPU, and the operating system can also switch between them via time slicing. When we run a normal Python application, we create a process as well as a main thread that will be responsible for running our Python application. import os import threading print(f'Python process with process id: {os.getpid()}') num_threads = threading.active_count() thread_name = threading.current_thread().name print(f'{num_threads} thread(s) are running') print(f'The current thread is {thread_name}') Multithreading in Python You might assume that starting multiple threads automatically takes advantage of multi-core systems. However, Python has a key constraint called the Global Interpreter Lock (GIL). The GIL ensures that only one thread can run one Python instruction at a time. This means that even on a multi-core machine, your Python code cannot run more than one CPU-bound thread simultaneously within the same process. So, are threads useless in Python? Far from it. Threads do provide genuine concurrency for I/O-bound tasks because Python releases the GIL during I/O operations. This allows you to overlap network calls, file reads, etc., effectively improving throughput. Yet for CPU-bound tasks, you won‚Äôt get true parallelism using just threads. ","date":"2025-01-05","objectID":"/asyncio/:3:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"The Global Interpreter Lock (GIL) in More Detail The Global Interpreter Lock is often regarded as a tricky limitation in Python. At a high level, the GIL: Prevents multiple native threads from executing Python bytecode simultaneously. Releases the lock when code interacts with the operating system for I/O (e.g., network or disk). Reacquires the lock once I/O completes and Python bytecode needs to be executed again. Why does it exist? The main reason is memory safety in the CPython implementation, which relies heavily on reference counting to manage objects. While convenient, reference counting can become unsafe when multiple threads mutate the same objects without careful synchronization. For I/O-bound code‚Äîlike sending concurrent HTTP requests‚Äîthis arrangement works well. You start multiple threads, each waiting on different I/O operations, and the GIL is periodically released while those operations happen, giving an overall speedup. For CPU-bound tasks‚Äîlike computing Fibonacci numbers with a naive recursion‚Äîthreads won‚Äôt help much because the lock is rarely released. Instead, you might use multiprocessing or specialized libraries that bypass the GIL for compute-intensive work. ","date":"2025-01-05","objectID":"/asyncio/:4:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Enter Asyncio: Asynchronous I/O in a Single Thread Asyncio is Python‚Äôs built-in library (introduced in Python 3.4 and improved in Python 3.5 with the async and await keywords) that focuses on concurrent I/O without the overhead of managing threads or processes. ","date":"2025-01-05","objectID":"/asyncio/:5:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Coroutines The foundation of Asyncio is the concept of a coroutine‚Äîa special function that can pause itself (await) while waiting for an I/O operation, and then resume right where it left off once the operation completes. While one coroutine is waiting, other coroutines can continue running, effectively achieving concurrency within a single thread. ","date":"2025-01-05","objectID":"/asyncio/:5:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Event Loop At the core of every Asyncio program is the event loop. Think of it as a manager that schedules coroutines. The event loop steps through coroutines one by one: A coroutine starts running until it hits an await for an I/O operation. The coroutine ‚Äúpauses,‚Äù returning control to the event loop. The event loop checks if there‚Äôs another coroutine ready to run. If so, it switches to that coroutine immediately. Meanwhile, the operating system handles the actual I/O. Once the I/O is ready (e.g., the network has responded), the event loop ‚Äúwakes up‚Äù the paused coroutine and resumes its execution. Because only one thread is responsible for executing Python code, the GIL is never contended between multiple threads. ","date":"2025-01-05","objectID":"/asyncio/:5:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Where Asyncio Shines‚Äîand Where It Doesn‚Äôt ","date":"2025-01-05","objectID":"/asyncio/:6:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"The Sweet Spot: I/O-Bound Work Asyncio is incredibly useful when you‚Äôre dealing with a large number of concurrent I/O operations. Common examples include: Building high-performance web servers that handle thousands of simultaneous connections. Writing web scrapers that fetch and parse dozens or hundreds of pages concurrently. Coordinating multiple microservice requests in a single workflow without blocking. In these scenarios, Asyncio‚Äôs single-threaded event loop can handle many I/O-bound coroutines, each pausing when it must wait for data. This often results in a dramatic improvement in throughput compared to a purely synchronous approach. ","date":"2025-01-05","objectID":"/asyncio/:6:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Handling CPU-Bound Work What if your task is mainly compute-heavy? Asyncio won‚Äôt magically run CPU-bound code in parallel because the GIL still applies to Python bytecode, and you‚Äôre still on a single thread. For CPU-bound tasks‚Äîlike image processing, machine learning, or large-scale data transformations‚Äîyou‚Äôd likely want to offload work to another process or leverage special libraries that release the GIL. That said, Asyncio does provide interoperability with threading and multiprocessing; you can combine CPU-intensive tasks with your I/O-bound coroutines. For instance, you can use asyncio.to_thread (in Python 3.9+) to run a CPU-bound function in a separate thread or harness a process pool executor for true parallelism at the CPU level. ","date":"2025-01-05","objectID":"/asyncio/:6:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Putting It All Together: An Example Below is a simplified comparison of synchronous, multithreaded, and Asyncio-based approaches to fetching two web pages: ","date":"2025-01-05","objectID":"/asyncio/:7:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Synchronous Approach import time import requests def fetch_example(): response = requests.get('https://www.example.com') return response.status_code def sync_fetch(): start = time.time() status_one = fetch_example() status_two = fetch_example() end = time.time() print(f\"Synchronous: {status_one}, {status_two}, time={end - start:.4f}s\") if __name__ == \"__main__\": sync_fetch() Synchronous mode fetches one URL at a time. If each request blocks for one second, the total time is roughly two seconds. Here, Python will release the GIL while waiting for the network, letting both threads run concurrently. The total time is potentially cut almost in half, assuming the responses come back quickly. ","date":"2025-01-05","objectID":"/asyncio/:7:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["programming","python"],"content":"Asyncio Approach import time import asyncio import aiohttp async def async_fetch_example(): async with aiohttp.ClientSession() as session: async with session.get('https://www.example.com') as response: status = response.status print(status) async def main(): start = time.time() await asyncio.gather(async_fetch_example(), async_fetch_example()) end = time.time() print(f\"Asyncio: time={end - start:.4f}s\") if __name__ == \"__main__\": asyncio.run(main()) With Asyncio, both fetch operations are initiated concurrently in the same thread, with the event loop switching between them whenever one is waiting for I/O. Like multithreading, you should see a meaningful speedup compared to the synchronous approach‚Äîbut without the complexities of shared data across threads. ","date":"2025-01-05","objectID":"/asyncio/:7:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/asyncio/"},{"categories":["linux"],"content":"How to install Arch Linux","date":"2025-01-05","objectID":"/arch/","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"I recently bought a mini PC because I wanted a lightweight machine that I can easily carry anywhere. Arch Linux‚Äôs minimalistic, rolling-release approach aligns perfectly with my love for a Vim-based workflow and a highly customizable setup. While the process can seem intimidating at first, it‚Äôs an incredibly rewarding experience that offers complete control over your system. Installing Arch Linux (UEFI or BIOS) Arch Linux is well-known for giving users full control over their system. This guide walks you through a fresh Arch Linux installation. While it is detailed, always refer to the official Arch Wiki for up-to-date information. ","date":"2025-01-05","objectID":"/arch/:0:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Contents Prerequisites Creating a Bootable USB Initial Setup Check UEFI or BIOS Wi-Fi Connection Check Internet Time \u0026 NTP Partitioning Using fdisk or cfdisk BIOS Partition Scheme UEFI Partition Scheme Formatting and Mounting Creating File Systems Mounting Partitions Installing the Base System Fast Mirror Selection pacstrap / basestrap Generating fstab Chroot Configuration Setting up Network Manager Installing and Configuring GRUB Root Password Locale and Timezone Hostname Final Steps Post-Installation Creating a New User Sudoers Configuration Installing X.org and a Window Manager Fonts Enabling a Display Manager (Optional) Sound Setup Installing Yay (AUR Helper) ","date":"2025-01-05","objectID":"/arch/:1:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Prerequisites A working internet connection on another device (in case you need help or to follow the Arch Wiki). A USB drive of at least 2‚ÄØGB capacity. Familiarity with the command line. Important: Installing Arch Linux involves formatting drives, which is destructive. Back up all important data before proceeding. ","date":"2025-01-05","objectID":"/arch/:2:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Creating a Bootable USB ","date":"2025-01-05","objectID":"/arch/:3:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"On Linux sudo dd if=\u003cpath-to-arch-iso\u003e of=\u003cpath-to-usb\u003e status=progress if = input file (the ISO file). of = output file (usually something like /dev/sdb). Be sure to confirm the correct USB path using lsblk or fdisk -l before running the command. ","date":"2025-01-05","objectID":"/arch/:3:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"On Windows Use Rufus. It‚Äôs a straightforward tool that avoids many potential pitfalls. ","date":"2025-01-05","objectID":"/arch/:3:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Initial Setup Boot from your USB and select the Arch Linux USB in your system‚Äôs boot menu. You should see a command-line shell once Arch boots. ","date":"2025-01-05","objectID":"/arch/:4:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Check UEFI or BIOS ls /sys/firmware/efi/efivars If you get an error, you‚Äôre in BIOS (Legacy) mode. If you see contents, you‚Äôre in UEFI mode. ","date":"2025-01-05","objectID":"/arch/:4:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Wi-Fi Connection If you‚Äôre on Wi-Fi, use iwctl: iwctl device list station \u003cwlan\u003e scan station \u003cwlan\u003e get-networks station \u003cwlan\u003e connect \u003cwifi-name\u003e station \u003cwlan\u003e show exit ","date":"2025-01-05","objectID":"/arch/:4:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Check Internet ping google.com If you have no connection, re-check your Wi-Fi settings or use a wired connection. ","date":"2025-01-05","objectID":"/arch/:4:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Time \u0026 NTP timedatectl set-ntp true This ensures your system clock stays synchronized. ","date":"2025-01-05","objectID":"/arch/:4:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Partitioning ","date":"2025-01-05","objectID":"/arch/:5:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Using fdisk or cfdisk Identify target disk: lsblk Open the disk utility: fdisk /dev/sda or cfdisk /dev/sda ","date":"2025-01-05","objectID":"/arch/:5:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"BIOS Partition Scheme A common layout might be: Boot: +200M Swap: typically 50% of your RAM size (+8G for 16‚ÄØGB RAM) Root: at least +25G Home: rest of the disk space Press w to write changes and exit. ","date":"2025-01-05","objectID":"/arch/:5:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"UEFI Partition Scheme EFI: around +550M and formatted as FAT32. Swap: 50% of RAM or as needed. Root: Minimum +25G or more. Home: Rest of the disk (if desired on a separate partition). ","date":"2025-01-05","objectID":"/arch/:5:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Formatting and Mounting ","date":"2025-01-05","objectID":"/arch/:6:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Creating File Systems Example commands (adjust partitions to suit your layout): mkfs.ext4 /dev/sda1 # For /boot or /root or /home mkfs.fat -F32 /dev/sda1 # For UEFI partition if using UEFI mkswap /dev/sda2 # Swap partition ","date":"2025-01-05","objectID":"/arch/:6:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Mounting Partitions Swap: swapon /dev/sda2 Root: mount /dev/sda3 /mnt Boot (UEFI): mkdir /mnt/boot mount /dev/sda1 /mnt/boot Home (if separate): mkdir /mnt/home mount /dev/sda4 /mnt/home ","date":"2025-01-05","objectID":"/arch/:6:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Installing the Base System ","date":"2025-01-05","objectID":"/arch/:7:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Fast Mirror Selection Choose the fastest mirrors by editing /etc/pacman.d/mirrorlist. Move the closest/fastest mirrors to the top. This significantly speeds up package downloads. ","date":"2025-01-05","objectID":"/arch/:7:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"pacstrap / basestrap For Arch Linux: pacstrap /mnt base base-devel linux linux-firmware vim git For Artix Linux (example): basestrap -i /mnt base base-devel runit elogind-runit linux linux-firmware \\ grub networkmanager networkmanager-runit cryptsetup lvm2 lvm2-runit neovim vim ","date":"2025-01-05","objectID":"/arch/:7:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Generating fstab genfstab -U /mnt \u003e\u003e /mnt/etc/fstab Check the file to ensure correct entries: vim /mnt/etc/fstab ","date":"2025-01-05","objectID":"/arch/:7:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Chroot Now ‚Äúenter‚Äù the new system: Arch: arch-chroot /mnt Artix: artix-chroot /mnt bash ","date":"2025-01-05","objectID":"/arch/:7:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Configuration ","date":"2025-01-05","objectID":"/arch/:8:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Setting up Network Manager pacman -S networkmanager systemctl enable NetworkManager (In Artix, you would enable the corresponding runit service instead.) ","date":"2025-01-05","objectID":"/arch/:8:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Installing and Configuring GRUB For BIOS pacman -S grub grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg For UEFI pacman -S grub efibootmgr grub-install --target=x86_64-efi --efi-directory=/boot grub-mkconfig -o /boot/grub/grub.cfg ","date":"2025-01-05","objectID":"/arch/:8:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Root Password passwd Set a strong password for the root user. ","date":"2025-01-05","objectID":"/arch/:8:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Locale and Timezone Edit /etc/locale.gen and uncomment your locale lines (e.g., en_US.UTF-8). Generate them: locale-gen Create /etc/locale.conf: echo \"LANG=en_US.UTF-8\" \u003e /etc/locale.conf Set your timezone: ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # or use tzselect ","date":"2025-01-05","objectID":"/arch/:8:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Hostname echo \"myhostname\" \u003e /etc/hostname ","date":"2025-01-05","objectID":"/arch/:8:5","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Final Steps Exit the chroot environment: exit umount -R /mnt reboot Remove your USB before booting, and the system should start from the newly installed Arch Linux. ","date":"2025-01-05","objectID":"/arch/:8:6","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Post-Installation ","date":"2025-01-05","objectID":"/arch/:9:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Creating a New User Log in as root. Create a user: useradd -m -g wheel \u003cusername\u003e passwd han Add additional groups if needed: usermod -aG audio,video,storage han ","date":"2025-01-05","objectID":"/arch/:9:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Sudoers Configuration Edit /etc/sudoers: visudo Add or uncomment a line to allow members of the wheel group to use sudo: %wheel ALL=(ALL:ALL) ALL ","date":"2025-01-05","objectID":"/arch/:9:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Installing X.org and a Window Manager Install Xorg: pacman -S xorg-server xorg-xinit Minimal Window Manager: pacman -S i3 dmenu rxvt-unicode Start X (for testing): startx For an automated start, add exec i3 in your ~/.xinitrc. ","date":"2025-01-05","objectID":"/arch/:10:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Fonts sudo pacman -S noto-fonts noto-fonts-cjk noto-fonts-emoji noto-fonts-extra Or any other font packages that suit your language preferences. For powerline or devicons, install Nerd Fonts: yay -S nerd-fonts-hack ","date":"2025-01-05","objectID":"/arch/:11:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Enabling a Display Manager (Optional) If you prefer a graphical login screen: sudo pacman -S lightdm lightdm-gtk-greeter sudo systemctl enable lightdm.service (Again, adapt for runit or other init systems.) ","date":"2025-01-05","objectID":"/arch/:12:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Sound Setup ","date":"2025-01-05","objectID":"/arch/:13:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Alsa sudo pacman -S alsa-utils alsa-plugins amixer Use M to unmute any channels. ","date":"2025-01-05","objectID":"/arch/:13:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"PulseAudio sudo pacman -S pulseaudio pulsemixer pulseaudio --start ","date":"2025-01-05","objectID":"/arch/:13:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Installing Yay (AUR Helper) Clone the Yay repository: git clone https://aur.archlinux.org/yay.git Build and install: cd yay makepkg -si Yay lets you install packages from both the official repositories and the AUR. ","date":"2025-01-05","objectID":"/arch/:14:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["linux"],"content":"Conclusion Congratulations! Your Arch Linux system is now up and running. From here, you can customize it with whatever software and configurations you like. Remember, the Arch Wiki is your best friend for finding detailed guides and troubleshooting tips. ","date":"2025-01-05","objectID":"/arch/:15:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/arch/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels","date":"2024-09-01","objectID":"/svm3/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/svm3/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels Recall that the dual form of LS-SVM is given by \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way. Asymmetric kernels are particularly useful in capturing directional relationships in data that symmetric kernels cannot. For instance, in scenarios involving directed graphs or conditional probabilities, the relationship from $x$ to $y$ is inherently different from the relationship from $y$ to $x$. ","date":"2024-09-01","objectID":"/svm3/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/svm3/"},{"categories":["machine learning"],"content":"AsK-LS Primal Problem Formulation We first define a generalized kernel trick for an inner product of two mappings $\\phi_s$ and $\\phi_t$. \\begin{align*} K(\\mathbf{u}, \\mathbf{v}) = \\langle \\phi_s(\\mathbf{u}), \\phi_t(\\mathbf{v})\\rangle, \\forall \\mathbf{u} \\in \\mathbb{R}^{d_s}, \\mathbf{v} \\in \\mathbb{R}^{d_t}, \\end{align*} where $\\phi_s: \\mathbb{R}^{d_s}\\to \\mathbb{R}^{p}$, $\\phi_t: \\mathbb{R}^{d_t}\\to \\mathbb{R}^{p}$, and $\\mathbb{R}^p$ is a high-dimensional or even an infinite-dimensional space. Note that $d_s$ and $d_t$ can be different. This formulation is closely related to the traditional LS-SVM but extends it by simultaneously considering both source and target feature spaces. The optimization goal is to find the weight vectors $ \\omega $ and $ \\nu $, and bias terms $ b_1 $ and $ b_2 $, that minimize the following objective function: \\begin{align*} \\min_{\\omega, \\nu, b_1, b_2, e, h} \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2, \\end{align*} subject to the constraints: \\begin{align*} \u0026 y_i (\\omega^T \\phi_s(x_i) + b_1) = 1 - e_i\\\\ \u0026 y_i (\\nu^T \\phi_t(x_i) + b_2) = 1 - h_i \\end{align*} Here: $ \\omega $ and $ \\nu $ are weight vectors for the source and target features. $ \\phi_s(x) $ and $ \\phi_t(x) $ are the source and target feature mappings. $ e_i $ and $ h_i $ are error terms for the source and target constraints. $ \\gamma $ is a regularization parameter. Note that this formulation is almost the same as the LS-SVM except that this considers both the source and target feature spaces simultaneously. ","date":"2024-09-01","objectID":"/svm3/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/svm3/"},{"categories":["machine learning"],"content":"Dual Form Let‚Äôs transform it into a dual form. The dual problem involves solving a system of linear equations derived from the primal problem‚Äôs Lagrangian. The Lagrangian function for the primal problem is: \\begin{align*} \\mathcal{L}( \\omega, \\nu, b_1, b_2, e, h, \\alpha, \\beta) \u0026= \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2\\\\ + \\sum_{i=1}^m \\alpha_i (1 - e_i \u0026- y_i (\\omega^T \\phi_s(x_i) + b_1)) + \\sum_{i=1}^m \\beta_i (1 - h_i - y_i (\\nu^T \\phi_t(x_i) + b_2)) \\end{align*} The KKT conditions are derived by setting the partial derivatives of the Lagrangian with respect to $ \\omega, \\nu, b_1, b_2, e, $ and $ h $ to zero. The dual problem leads to the following linear system: \\begin{align*} \\begin{bmatrix} 0 \u0026 0 \u0026 Y^T \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 Y^T \\\\ Y \u0026 0 \u0026 \\frac{I}{\\gamma} \u0026 H \\\\ 0 \u0026 Y \u0026 H^T \u0026 \\frac{I}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\alpha \\\\ \\beta \\end{bmatrix} =\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} where: $ Y $ is a vector of class labels. $ H $ is the kernel matrix with elements $ H_{ij} = y_i K(x_i, x_j) y_j $, where $ K(x_i, x_j) = \\langle \\phi_s(x_i), \\phi_t(x_j) \\rangle $ is the asymmetric kernel function. For an asymmetric kernel $ K $, the kernel function $ K(x_i, x_j) \\neq K(x_j, x_i) $. This asymmetry is directly incorporated into the matrix $ H $, where: \\begin{align*} H_{ij} \u0026= y_i K(x_i, x_j) y_j \\\\ H_{ji} \u0026= y_j K(x_j, x_i) y_i \\end{align*} AsK-LS uses two different feature mappings $ \\phi_s $ and $ \\phi_t $ for the source and target features. This approach allows capturing more information compared to symmetric kernels. The dual solution provides weight vectors $ \\omega $ and $ \\nu $, which span the target and source feature spaces, respectively. The decision functions for classification from the source and target perspectives are given by \\begin{align*} f_t(x) \u0026= \\sum_{i=1}^m \\alpha_i y_i K(x_i, x) + b_1\\\\ f_s(x) \u0026= \\sum_{i=1}^m \\beta_i y_i K(x, x_i) + b_2 \\end{align*} These decision functions leverage the learned asymmetric relationships in the data, providing a more nuanced classification model. ","date":"2024-09-01","objectID":"/svm3/:2:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/svm3/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 2.","date":"2024-08-31","objectID":"/svm2/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/svm2/"},{"categories":["machine learning"],"content":"Introduction to Least-Square SVM ","date":"2024-08-31","objectID":"/svm2/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/svm2/"},{"categories":["machine learning"],"content":"Introduction Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a least squares cost function. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve. ","date":"2024-08-31","objectID":"/svm2/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/svm2/"},{"categories":["machine learning"],"content":"Optimization Problem (Primal Problem) \\begin{align*} \u0026\\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2,\\\\ \u0026\\text{subject to } y_i (w^T \\phi(x_i) + b) = 1 - e_i, \\ \\forall i \\end{align*} where: $w$ is the weight vector. $b$ is the bias term. $e_i$ are the error variables. $\\gamma$ is a regularization parameter. $\\phi(x_i)$ is the feature mapping function. Note that $y_i^{-1} = y_i$, since $y_i = \\pm 1$. ","date":"2024-08-31","objectID":"/svm2/:1:1","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/svm2/"},{"categories":["machine learning"],"content":"Lagrangian Function To solve the constraint optimization problem, we define the Lagrangian function: \\begin{align*} L(w, b, e, \\alpha) = \\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2 - \\sum_{i=1}^n \\alpha_i \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right], \\end{align*} where $\\alpha_i$ are Lagrange multipliers. Then, by setting the partial derivatives of the Lagrangian with respect to $w$, $b$, $e$, and $\\alpha$ to zero, we get the KKT conditions. $w$: \\begin{align*} \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) = 0 \\implies w = \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\end{align*} $b$: \\begin{align*} \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^n \\alpha_i y_i = 0 \\end{align*} $e_i$: \\begin{align*} \\frac{\\partial L}{\\partial e_i} = \\gamma e_i - \\alpha_i = 0 \\implies \\alpha_i = \\gamma e_i \\end{align*} Thus, $e_i = \\frac{\\alpha_i}{\\gamma}$ $\\alpha_i$: \\begin{align*} \\frac{\\partial L}{\\partial \\alpha_i} = - \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right] = 0 \\implies y_i (w^T \\phi(x_i) + b) = 1 - e_i, i=1,\\dots, N. \\end{align*} Let‚Äôs substitute $w$ and $e$: $K$: kernel matrix $\\alpha = [\\alpha_1, \\alpha_2, \\ldots, \\alpha_n]^T$: $N\\times 1$ $y = [y_1, y_2, \\ldots, y_n]^T$. $\\Omega = YKY^T$, where $\\Omega_{kl}= y_ky_l\\phi(x_k)^T\\phi(x_l)$ $Y = \\text{diag}(y)$. $b$: $1\\times 1$ Then, we can express it compactly \\begin{align*} \u0026 Y(KY^T\\alpha+b\\mathbf{1})-\\mathbf{1}+\\frac{\\alpha}{2\\gamma} = 0\\\\ \u0026 \\mathbf{1}^TY\\alpha = 0. \\end{align*} \\begin{align*} \\end{align*} By using the expression of $\\alpha$ and $b$, we get \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1_n \\end{bmatrix} \\end{align*} Note that the dimension of the matrix on the left-hand side is $(N+1)\\times (N+1)$. Once we have $b$ and $\\alpha$ by solving the linear system, the decision function for a new input $x$ can be obtained by: \\begin{align*} f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b. \\end{align*} Example Suppose we have three training examples with feature vectors $x_1, x_2$, and $x_3$, and corresponding labels $y_1, y_2$, and $y_3$. The kernel matrix $\\Omega$ is defined as: \\begin{align*} \\Omega_{ij} = y_i y_j K(x_i, x_j) \\end{align*} The dual form is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} $y = \\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\end{bmatrix}$ $\\alpha = \\begin{bmatrix} \\alpha_1\\ \\alpha_2 \\ \\alpha_3 \\end{bmatrix} $ $e = \\begin{bmatrix} 1 \\ 1 \\ 1 \\end{bmatrix}$ $I$ is a $3 \\times 3$ identity matrix Then, the $\\Omega$ is given by \\begin{align*} \\Omega = \\begin{bmatrix} y_1 y_1 K(x_1, x_1) \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 y_1 K(x_2, x_1) \u0026 y_2 y_2 K(x_2, x_2) \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3 y_3 K(x_3, x_3) \\end{bmatrix} \\end{align*} Now, the complete matrix equation is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 \\Omega_{11} + \\frac{1}{\\gamma} \u0026 \\Omega_{12} \u0026 \\Omega_{13} \\\\ y_2 \u0026 \\Omega_{21} \u0026 \\Omega_{22} + \\frac{1}{\\gamma} \u0026 \\Omega_{23} \\\\ y_3 \u0026 \\Omega_{31} \u0026 \\Omega_{32} \u0026 \\Omega_{33} + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} This can be written explicitly as: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 y_1^2 K(x_1, x_1) + \\frac{1}{\\gamma} \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 \u0026 y_2 y_1 K(x_2, x_1) \u0026 y_2^2 K(x_2, x_2) + \\frac{1}{\\gamma} \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 \u0026 y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3^2 K(x_3, x_3) + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} The solution to this matrix equation provides the values of $b","date":"2024-08-31","objectID":"/svm2/:1:2","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/svm2/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 1.","date":"2024-08-25","objectID":"/svm1/","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Support Vector Machine ","date":"2024-08-25","objectID":"/svm1/:0:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Introduction Support Vector Machines (SVMs) are among the most effective and versatile tools in machine learning, widely used for various tasks. SVMs work by finding the optimal boundary, or hyperplane, that separates different classes of data with the maximum margin, making them highly reliable for classification, especially with complex datasets. What truly sets SVMs apart is their ability to handle both linear and non-linear data through the kernel trick, allowing them to adapt to a wide range of problems with impressive accuracy. In this blog post, we‚Äôll delve into how SVMs work and gently explore the mathematical foundations behind their powerful performance. ","date":"2024-08-25","objectID":"/svm1/:1:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Decision Boundary with Margin A hyperplane(or decision surface) is used to separate data points belonging to different classes. The goal of SVM is to find the optimal separating hyperplane. However, what is the optimal separating hyperplanes? The optimal hyperplane is the one which maximizes the distance from the hyperplane to the nearest data point of any class. Support vectors are the data points that lie closest to the hyperplane. The distance is referred to as the margin. SVMs maximize the margin around the separating hyperplane. The equation of a hyperplane in $\\mathbb{R}^p$ can be expressed as: $$\\mathbf{w}\\cdot \\mathbf{x}+b=0.$$ Here, $\\mathbf{w}$ is the normal vector to the hyperplane. It is clear if we set it $b = \\mathbf{w}\\cdot\\mathbf{x}_0$ $$\\mathbf{w}(\\mathbf{x}-\\mathbf{x}_0)=0.$$ Let‚Äôs consider a simple scenario, where training data is linearly separable: $$\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) | \\mathbf{x}_i \\in \\mathbb{R}^p,\\ y_i \\in {-1,1}\\}_{i=1}^N.$$ Then, we can build two supporting hyperplanes separating the data with no points between them: $H_1:\\mathbf{w}\\cdot \\mathbf{x}+b=1$ $H_2:\\mathbf{w}\\cdot \\mathbf{x}+b=-1$ All samples have to satisfy one of two constraints: $\\mathbf{w}\\cdot \\mathbf{x}+b\\geq1$ $\\mathbf{w}\\cdot \\mathbf{x}+b\\leq-1$ These constraints can be combined into a single expression: $$y(\\mathbf{w}\\cdot \\mathbf{x}+b)\\geq 1.$$ To maximize the margin, we can consider a unit vector $\\mathbf{u} = \\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}$, which is perpendicular to the hyperplanes and a point $x_0$ on the hyperplane $H_2$. If we scale $\\mathbf{u}$ from $\\mathbf{x}_0$, we get $z = \\mathbf{x}_0+\\gamma\\mathbf{u}$. If we assume $z$ is on $H_1$, then $\\mathbf{w}\\cdot z +b=1$. This is equivalent to \\begin{align*} \\mathbf{w}\\cdot (\\mathbf{x}_0+\\gamma \\mathbf{u})+b=1\\\\ \\mathbf{w}x_0+\\mathbf{w}\\gamma\\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}+b=1\\\\ \\mathbf{w}x_0+\\gamma\\lVert \\mathbf{w}\\rVert +b=1\\\\ \\mathbf{w}x_0+b=1-\\gamma\\lVert \\mathbf{w}\\rVert \\end{align*} As $x_0$ is on $H_2$, we get $\\mathbf{w}x_0+b=-1$. Finally, we obtain \\begin{align*} -1=1-\\gamma\\lVert \\mathbf{w}\\rVert \\\\ \\gamma=\\frac{2}{\\lVert \\mathbf{w}\\rVert }. \\end{align*} Note that the scaled unit vector $\\gamma \\mathbf{u}$‚Äôs magnitude is $\\gamma$. Thus, the maximization of margin is equivalent to maximize $r$. To maximize $r$, we have to minimize $\\lVert \\mathbf{w} \\rVert$. Thus, finding the optimal hyperplane reduces to solving the following optimization problem: \\begin{align*} \u0026\\min \\lVert \\mathbf{w}\\rVert ,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Equivalently, \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert w\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Now, we have convex quadratic optimization problem. The solution of this problem gives us the optimal hyperplane that maximizes the margin (Details are in the following section). However, in practice, the data may not be perfectly separable. To account for this, we introduce a soft margin that allows for some misclassification. This is done by admitting small errors in classification and potentially using a more complex, nonlinear decision boundary, improving the generalization of the model. ","date":"2024-08-25","objectID":"/svm1/:1:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Alternative Derivation Consider a point $\\mathbf{x}$. Let $\\mathbf{d}$ be the vector from a hyperplane (i.e., $\\mathbf{w}\\mathbf{x}+b=0$) to $\\mathbf{x}$ of minimum length. Let $\\mathbf{x}_0$ be the projection of $\\mathbf{x}$ onto the hyperplane. Then, \\begin{align*} \\mathbf{x}_0 = \\mathbf{x} - \\mathbf{d}. \\end{align*} As $\\mathbf{d}$ is parallel to $\\mathbf{w}$, so $\\mathbf{d} = \\alpha \\mathbf{w}$ for some $\\alpha\\in \\mathbb{R}$. Since $\\mathbb{x}_0$ is on the hyperplane, $\\mathbf{w}\\mathbf{x}_0 + b = 0$. Thus, \\begin{align*} \\mathbf{w}\\mathbf{x}_0 + b = \\mathbf{w}(\\mathbf{x}-\\mathbf{d}) + b = \\mathbf{w}(\\mathbf{x}-\\alpha\\mathbf{w}) + b = 0. \\end{align*} Then, we get \\begin{align*} \\alpha = \\frac{\\mathbf{w}\\mathbf{x}+b}{\\mathbf{w}^T\\mathbf{w}}. \\end{align*} The length of $\\mathbf{d}$ is given by \\begin{align*} \\lVert \\mathbf{d} |_2 = \\sqrt{\\alpha^2\\mathbf{w}^T\\mathbf{w}} = \\frac{|\\mathbf{w}\\mathbf{x}+b|}{\\sqrt{\\mathbf{w}^T\\mathbf{w}}} = \\frac{|\\mathbf{w}\\mathbf{x}+b|}{\\lVert\\mathbf{w}\\rVert_2}. \\end{align*} We can obtain the margin by choosing the support vector, which is the closest point to the hyperplane by \\begin{align*} \\gamma(\\mathbf{w}, b) = \\min_{\\mathbf{x}\\in \\mathcal{D}}\\frac{|\\mathbf{w}\\mathbf{x}+b|}{\\lVert\\mathbf{w}\\rVert_2}. \\end{align*} Note that the margin and hyperplane are scale invarianct: \\begin{align*} \\gamma(\\beta\\mathbf{w}, \\beta b) = \\gamma(\\mathbf{w}, b). \\end{align*} ","date":"2024-08-25","objectID":"/svm1/:1:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Error Handling in SVM In practice, it‚Äôs unrealistic to expect a perfect separation of data, especially when the data is noisy or not linearly separable. To address this, we can allow for some prediction errors while still striving to find an optimal decision boundary. One approach is to minimize the norm of the weight vector, while penalizing the number of errors $N_e$. The optimization problem can be formulated as follows: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 +C\\cdot N_{e},\\quad \\text{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad \\forall i. \\end{align*} Here, $C$ is a regularization parameter that controls the trade-off between minimizing the weight vector and the number of errors. The penalty approach described here is known as 0-1 loss, where all errors are treated equally. However, this approach is not commonly used. Instead, a more practical approach introduces a slack variable with hinge loss. The slack variable ($\\xi_j$) measures the degree of misclassification or how much a point is violating the margin. This leads to the following problem: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 +C\\sum_j\\xi_j ,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1-\\xi_j \\quad \\forall i,\\ \\xi_j\\geq 0,\\ \\forall j. \\end{align*} Note that $\\xi_j\u003e1$, when SVMs make errors: \\begin{align*} \\xi_j = (1-(\\mathbf{w}\\mathbf{x}_j+b)y_j)_+ \\end{align*} Let‚Äôs look at the new constraint. If some data points are misclassified, then $\\xi_j\u003e1$ and $y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\leq 0$. This approach is called soft-margin SVM. Lastly, how do we set $C$? A large value of $C$ places a higher penalty on errors, leading to a narrower margin but fewer misclassifications (i.e., the SVM will try to classify all data points correctly), whereas a smaller value of $C$ allows for a wider margin but potentially more misclassifications. The optimal value of $C$ is typically chosen through cross-validation. ","date":"2024-08-25","objectID":"/svm1/:2:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"SVM Optimization: Lagrange Multipliers ","date":"2024-08-25","objectID":"/svm1/:3:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Lagrange Multipliers Consider the optimization problem: \\begin{align*} \u0026\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\ \u0026\\text{subject to}\\quad g(\\mathbf{x})=0. \\end{align*} To find the minimum of $f$ under the constraint $g(\\mathbf{x})$, we use the method of Lagrange multipliers. The key idea is that at the optimal point, the gradient of $f(\\mathbf{x})$ must be parallel to the gradient of $g(\\mathbf{x})$. Mathematically, this condition is expressed as: $$\\nabla f(\\mathbf{x}) = \\lambda\\nabla g(\\mathbf{x}).$$ Example: Consider a simple 2D example where you want to minimize the function $f(x,y)=x^2+y^2$, which represents a circle centered at the origin. This function increases as you move away from the origin, so the minimum is at the origin. Now, consider the constraint: $g(x,y)=x+y-1=0$. This constraint is a line that runs through the $xy$-plane. Our goal is to find the point on this line that minimizes $f(x,y)$. A Lagrange multiplier is like a balancing factor that adjusts the direction and magnitude of your search along the constraint. As you move along the constraint line $g(x,y)$, $\\lambda$ ensures that the solution also respects the shape of the function $f(x,y)$ that you are trying to minimize. To solve the constraint optimization problem, we define the Lagrangian function: $$\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x}).$$ To find the minimum, we take the partial derivatives of $\\mathcal{L}(\\mathbf{x}, \\lambda)$ with respect to both $\\mathbf{x}$ and $\\lambda$, and set them equal to zero. ","date":"2024-08-25","objectID":"/svm1/:3:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"SVM Optimization Recall that we want to solve the following convex quadratic optimization problem: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} The objective is to find the optimal hyperplane that maximizes the margin between two classes of data points. We can reformulate this optimization problem using the method of Lagrange multipliers, which introduces a set of multipliers $\\alpha_i$ (one for each constraint). The Lagrangian function for this problem is given by: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} ","date":"2024-08-25","objectID":"/svm1/:3:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Duality and the Lagrangian Problem While we could attempt to solve the primal optimization problem directly, it is often more practical, especially for large datasets, to reformulate the problem using the duality principle. The dual form is advantageous because it depends only on the inner products of the data points, which allows the use of kernel methods for non-linear classification. To find the solution to the primal problem, we solve the following problem: \\begin{align*} \u0026\\max_{\\mathbf{w}, b}\\min_\\alpha \\mathcal{L}(\\mathbf{w}, b, \\alpha)\\\\ \u0026\\textrm{subject to}\\quad \\alpha_i\\geq 0, \\forall i. \\end{align*} Here, we maximize the Lagrangian with respect to the multipliers $\\alpha_i$, while minimizing with respect to the primal variables $\\mathbf{w}$ and $b$. ","date":"2024-08-25","objectID":"/svm1/:3:3","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Handling Inequality Constraints with KKT Conditions You may observe that the method of Lagrange multipliers is used for equality constraints. However, it can be extended to handle inequality constraints through the use of additional conditions known as the Karush-Kuhn-Tucker (KKT) conditions. These conditions ensure that the solution satisfies the necessary optimality criteria for problems with inequality constraints. ","date":"2024-08-25","objectID":"/svm1/:3:4","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"The Wolfe Dual Problem The Lagrangian problem for SVM optimization involves $N$ inequality constraints, where $N$ is the number of training examples. This problem is typically tacked using its dual form. The duality principle provides a powerful framework, stating that an optimization problem can be approached from two perspectives: The primal problem, which in our context is a minimization problem. The dual problem, which is a maximization problem. An important aspect of duality is that the maximum value of the dual problem is always less than or equal to the minimum value of the primal problem. This relationship means that the dual problem provides a lower bound to the solution of the primal problem. In the context of SVM optimization, we are dealing with a convex optimization problem. According to Slater‚Äôs condition, which applies to problems with affine constraints, strong duality holds. Strong duality implies that the optimal values of the primal and dual problems are equal, meaning the maximum value of the dual problem equals the minimum value of the primal problem. This equality allows us to solve the dual problem instead of the primal problem, often leading to computational advantages. Recall that we aim to solve the following optimization problem: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} The minimization problem involves solving the partial derivatives of $\\mathcal{L}$ with respect to $\\mathbf{w}$ and $b$ and set them equal to zero: \\begin{align*} \u0026\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\mathbf{w} - \\sum_i \\alpha_i y_i \\mathbf{x}_i\\\\ \u0026 \\nabla_b\\mathcal{L}(\\mathbf{w}, b, \\alpha) = -\\sum_i \\alpha_i y_i \\end{align*} Form the first equation, we obtain: \\begin{align*} \u0026\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i\\ \\end{align*} Next, we substitute the objective function with $\\mathbf{w}$: \\begin{align*} \\mathbf{W}(\\alpha, b) \u0026= \\frac{1}{2}\\left(\\sum_i \\alpha_i y_i \\mathbf{x}_i\\right)\\cdot \\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\left[y_i\\left(\\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\cdot \\mathbf{x}_i+b\\right)-1\\right]\\\\ \u0026= \\frac{1}{2}\\Big(\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j\\Big)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\Bigg[y_i\\Bigg(\\Big(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\Big)\\cdot \\mathbf{x}_i+b\\Bigg)\\Bigg]+\\sum_i \\alpha_i \\\\ \u0026= \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j - \\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i \\cdot \\mathbf{x}_j\\\\ \u0026\\quad -\\sum_i \\alpha_i y_i b+\\sum_i \\alpha_i \\\\ \u0026= \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j-\\sum_i \\alpha_i y_i b \\end{align*} Note that we use two indices, $i$ and $j$ when substituting $\\mathbf{W}$. This is obvious if we consider a simple example. Imagine you have two data points: \\begin{align*} \\mathbf{x}_1,y_1\u0026={(1,2),1}\\\\ \\mathbf{x}_2,y_2\u0026={(2,1),‚àí1} \\end{align*} Then, \\begin{align*} \\lVert \\mathbf{w}\\rVert^2=\\mathbf{w}\\cdot \\mathbf{w}=\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_i}\\cdot\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_j}. \\end{align*} This simplification shows that the optimization problem can be reformulated purely in terms of the Lagrange multipliers $\\alpha_i$. Note that the term involving $b$ can be removed by setting $b=0$, simplifying our equation further: \\begin{align*} \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j (\\mathbf{x}_i\\cdot \\mathbf{x}_j) \\end{align*} This expression is known as the Wolfe dual Lagrangian function. We have transformed the problem into one involving only the multipliers $\\alpha_i$, resulting in a quadratic programming problem, commonly referred to as the Wolfe dual problem: \\begin{align*} \u0026\\max_\\alpha \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{","date":"2024-08-25","objectID":"/svm1/:4:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Karush-Kuhn-Tucker conditions When dealing with optimization problems that involve inequality constraints, such as those encountered in Support Vector Machines (SVMs), an additional requirement must be met: the solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The KKT conditions are a set of first-order necessary conditions that must be satisfied for a solution to be optimal. These conditions extend the method of Lagrange multipliers to handle inequality constraints and are particularly useful in non-linear programming. For the KKT conditions to apply, the problem must also satisfy certain regularity conditions. Fortunately, one of these regularity conditions is Slater‚Äôs condition, which we have already established holds true for SVMs. ","date":"2024-08-25","objectID":"/svm1/:5:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"KKT Conditions and SVM Optimization In the context of SVMs, the optimization problem is convex, meaning that the KKT conditions are not only necessary but also sufficient for optimality. This implies that if a solution satisfies the KKT conditions, it is guaranteed to be the optimal solution for both the primal and dual problems. Moreover, in this case, there is no duality gap, meaning the optimal values of the primal and dual problems are equal. ","date":"2024-08-25","objectID":"/svm1/:5:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"Prediction Firstly, for a Linear SVM with no kernel, the primal weight vector is given by $$ w=\\sum_{i\\in \\mathcal S}\\alpha_i,y_i,x_i $$ Then, the decision function is $$ f(x)=w^{!\\top}x + b $$ Since the feature map $\\phi(,\\cdot,)$ may live in a huge or even infinite-dimensional space, we never form $w$ explicitly. Instead we keep the kernel trick inside the decision function: $$ f(x)=\\sum_{i\\in\\mathcal S}\\alpha_i,y_i,K(x_i,,x)+b $$ where $K(x_i,x)=\\langle\\phi(x_i),\\phi(x)\\rangle$. Typical kernels are the RBF $K(u,v)=\\exp\\!\\bigl(-\\frac{|u-v|^2}{2\\sigma^2}\\bigr)$ or the polynomial $K(u,v)=(u^{\\!\\top}v+c)^p$. Finally, we need to turn the decision value into a class label. For binary classification the prediction is simply the sign of the decision function: \\begin{align*} \\hat y = \\operatorname{sign}\\bigl(f(x)\\bigr) = \\begin{cases} +1 \u0026\\text{if }f(x)\\gt 0,\\\\ -1 \u0026\\text{if }f(x)\\lt 0. \\end{cases} \\end{align*} In sum, \\begin{align*} \\textbf{Predict}(x): \\quad f(x)=\\sum_{i\\in\\mathcal S}\\alpha_i,y_i,K(x_i,x)+b,; \\hat y=\\text{sign}\\bigl(f(x)\\bigr) \\end{align*} Reference Alexandre Kowalczyk, Support Vector Machines Succinctly, 2017 ","date":"2024-08-25","objectID":"/svm1/:6:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/svm1/"},{"categories":["machine learning"],"content":"On the direction of gradient descent update","date":"2024-08-19","objectID":"/gradient-descent/","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/gradient-descent/"},{"categories":["machine learning"],"content":"On Gradient Descent Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the function‚Äôs minimum value. It is a fundamental concept in machine learning, particularly in training models such as neural networks. The gradient is a vector that represents the direction of the steepest increase of the function at a given point. For example, for a convex function $z = ax^2 + by^2$, the gradient is $[2ax, 2by]$, which points in the direction of the steepest ascent. In gradient descent, the goal is to minimize the function, so the algorithm moves in the opposite direction of the gradient, which is $[-2ax, -2by]$. This opposite direction is chosen because it is the direction of the steepest decrease in the function value. But how do we know that moving in this direction will strictly decrease the function value? ","date":"2024-08-19","objectID":"/gradient-descent/:0:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/gradient-descent/"},{"categories":["machine learning"],"content":"Direction of Gradient Descent Let‚Äôs investigate the direction of gradient descent. The derivative of the objective function $f(\\mathbf{x})$ provides the slope of $f(\\mathbf{x})$ at the point $f(\\mathbf{x})$. It tells us how to change $\\mathbf{x}$ in order to make a small improvement in our goal. A function $f(\\mathbf{x})$ can be approximated by its first-order Taylor expansion at $\\bar{\\mathbf{x}}$: $$f(\\mathbf{x})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T(x-\\bar{\\mathbf{x}})$$ Now let $\\mathbf{d}\\neq0, |\\mathbf{d}|=1$ be a direction, and in consideration of a new point $\\mathbf{x}:=\\bar{\\mathbf{x}}+\\mathbf{d}$, we define: $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}$$ We would like to choose $\\mathbf{d}$ that minimizes the function $f$. From the Cauchy-Schwarz inequality1 , we know that $$|\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}|\\leq \\lVert\\nabla f(\\bar{\\mathbf{x}})\\rVert \\ \\lVert\\mathbf{d}\\rVert.$$ The equality holds if and only if $\\mathbf{d}=\\lambda \\nabla f(\\bar{\\mathbf{x}})$, where $\\lambda\\in \\mathbb{R}$. Since we want to minimize the function $f$, we negate the steepest direction $\\mathbf{d}^{*}$, then $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}}).$$ Since $\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always positive, the term $-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always negative. Therefore, by updating $\\mathbf{x}$ $$\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\lambda \\nabla f(\\mathbf{x}^{(k)}),$$ we get $$f(\\mathbf{x}^{(k+1)}) \u003c f(\\mathbf{x}^{(k)}).$$ Cauchy-Schwarz Inequaility: $|\\mathbf{a}\\cdot \\mathbf{b}|\\leq \\lVert\\mathbf{a}\\rVert \\ \\lVert\\mathbf{b}\\rVert$. Equality holds if and only if either $\\mathbf{a}$ or $\\mathbf{b}$ is a multiple of the other.¬†‚Ü©Ô∏é ","date":"2024-08-19","objectID":"/gradient-descent/:1:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/gradient-descent/"},{"categories":["machine learning"],"content":"Latent variable tutorial","date":"2024-08-18","objectID":"/latent-variable-part1/","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"Latent Variable Modeling ","date":"2024-08-18","objectID":"/latent-variable-part1/:0:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"Motivation of Latent Variable Modeling Let‚Äôs say we want to classify some data. If we had access to a corresponding latent variable for each observation $ \\mathbf{x}_i $, modeling would be more straightforward. To illustrate this, consider the challenge of finding the latent variable (i.e., the true class of $ \\mathbf{x} $). It can be expressed like $ z^* = \\argmax_{z} p(\\mathbf{x} | z) $. It is hard to identify the true clusters without prior knowledge about them. For example, we can cluster like Fig. (b) or (c). Consider modeling the complete data set $ p(\\mathbf{x} | z) $ under the assumption that the observations are independent and identically distributed (i.i.d.). Based on the above Fig. (c), the joint distribution for a single observation $ (\\mathbf{x}_i, \\mathbf{z}_i) $ given the model parameters $ \\boldsymbol{\\theta} $ can be expressed: \\begin{align*} p(\\mathbf{x}_i, \\mathbf{z}_i | \\boldsymbol{\\theta}) = \\begin{cases} p(\\mathcal{C}_1) p(\\mathbf{x}_i | \\mathcal{C}_1) \u0026 \\text{if } z_i = 0 \\\\ p(\\mathcal{C}_2) p(\\mathbf{x}_i | \\mathcal{C}_2) \u0026 \\text{if } z_i = 1 \\\\ p(\\mathcal{C}_3) p(\\mathbf{x}_i | \\mathcal{C}_3) \u0026 \\text{if } z_i = 2 \\\\ \\end{cases} \\end{align*} Given $ N $ observations, the joint distribution for the entire dataset $ { \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N } $ along with their corresponding latent variables $ { \\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_N } $ is: \\begin{align*} p(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N, \\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_N | \\boldsymbol{\\theta}) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\pi_k^{z_{nk}} \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_{nk}} \\end{align*} Here, $ \\pi_k = p(\\mathcal{C}_k) $ represents the prior probability of the $ k $-th component, and $ p(\\mathbf{x}_n | \\mathcal{C}_k) = \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ denotes the Gaussian distribution associated with component $ \\mathcal{C}_k $. Also, $z_{nk}\\in{0,1}$ and $\\sum_k z_{nk}=1$. However, in practice, the latent variables $ \\mathbf{z}_k $ are often not directly observable, which complicates the modeling process. In the following sections, we present various methods for identifying and handling these latent variables to improve the classification and modeling of data. ","date":"2024-08-18","objectID":"/latent-variable-part1/:1:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"K-Means Clustering Suppose that we have a data set $\\mathbf{X} = {\\mathbf{x}_1,\\dots, \\mathbf{x}_n}$ consisting of $N$ observations of a random $D$-dimensional variable $\\mathbf{x}\\in \\mathbb{R}^{D}$. Our goal is to partition the data into $K$ of clusters. Intuitively, a cluster can be thought as a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This notion can be formalized by introducing a set of $D$-dimensional vectors $\\boldsymbol{\\mu}_k$, which represents the centers of the clusters. Our goal is to find an assignment of data points to clusters, as well as a set of vectors ${\\boldsymbol{\\mu}_k}$. Objective function of $K$-means clustering (distortion measure) can be defined as follows: $$J = \\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}\\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k\\rVert^2$$ , where $r_{nk}\\in{0,1}$ is a binary indicator variable which represents the membership of data $\\mathbf{x}_n$. It can be expressed as follows: % The $r_{nk}$ can be optimized in a closed-form solution as follows: \\begin{align*} r_{nk}=\\begin{cases} 1 \u0026 \\text{if } k=\\argmin_{j} \\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_j\\rVert^2\\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\end{align*} Our goal is to find values for the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$ that minimize $J$. We can minimize $J$ through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$. First we choose some initial values for the $\\boldsymbol{\\mu}_k$. Then, in the first phase, we minimize $J$ with respect to the $r_{nk}$, keeping the $\\boldsymbol{\\mu}_k$ fixed. In the second phase we minimize $J$ with respect to the $\\boldsymbol{\\mu}_k$, keeping $r_{nk}$ fixed. This two-stage optimization is then repeated until convergence. Now consider the optimization of the $\\boldsymbol{\\mu}_k$ with the $r_{nk}$ held fixed. The objective function $J$ is a quadratic function of $\\boldsymbol{\\mu}_k$, and it can be minimized by setting its derivative with respect to $\\boldsymbol{\\mu}_k$ to zero giving \\begin{align*} 2\\sum_{n=1}^{N}r_{nk}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k) = 0. \\end{align*} We can arrange as \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{\\sum_n r_{nk}\\boldsymbol{x}_n}{\\sum_n r_{nk}}. \\end{align*} The denominator of $\\boldsymbol{\\mu}_k$ is equal to the number of points assigned to cluster $k$. The mean of cluster $k$ is essentially the same as the mean of data points $\\mathbf{x}_n$ assigned to cluster $k$. For this reason, the procedure is known as the $K$-means clustering algorithm. The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments. These two phases reduce the value of the objective function $J$, so the convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of $J$. We can also sequentially update the $\\mu_k$ as follows: \\begin{align*} \\mu_{k+1} = \\mu_{k} + \\eta(\\mathbf{x}_k-\\mu_{k}) \\end{align*} There are some properties to note: It is a hard clustering algorithm ($\\leftrightarrow$ soft clustering) It is sensitive to the initialization of centroid. The number of clusters is uncertain. Sensitive to distance metrics (e.g., Euclidean?) ","date":"2024-08-18","objectID":"/latent-variable-part1/:2:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"Gaussian Mixture Models $K$-means clustering is a form of hard clustering, where each data point is assigned to exactly one cluster. However, in some cases, soft clustering‚Äîwhere data points can belong to multiple clusters with varying degrees of membership‚Äîprovides a better model in practice. A Gaussian Mixture Model (GMM) assumes a linear superposition of Gaussian components, offering a richer class of density models than a single Gaussian distribution. In essence, rather than assuming that all data points are generated by a single Gaussian distribution, we assume that the data is generated by a mixture of $ K $ different Gaussian distributions, where each Gaussian represents a different component in the mixture. For a single sample, the Gaussian Mixture Model can be expressed as a weighted sum of these individual Gaussian distributions: \\begin{align*} p(\\mathbf{x}) \u0026= \\sum_\\mathbf{z} p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) \\\\ \u0026= \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\end{align*} Here, $ \\mathbf{x} $ is a data point, $ \\pi_k $ represents the mixing coefficients, $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is a Gaussian distribution with mean $ \\boldsymbol{\\mu}_k $ and covariance $ \\boldsymbol{\\Sigma}_k $, and $ K $ is the number of Gaussian components. A key quantity in GMMs is the conditional probability of $ \\mathbf{z} $ given $ \\mathbf{x} $, denoted as $ p(z_k = 1 | \\mathbf{x}) $ or $ \\gamma(z_k) $. This is also known as the responsibility or assignment probability, which represents the probability that a given data point $ \\mathbf{x} $ belongs to component $ k $ of the mixture. Essentially, this can be thought of as the classification result for $ \\mathbf{x} $. This responsibility is updated using Bayes‚Äô Theorem, and can be expressed as: \\begin{align*} \\gamma(z_k) \\equiv p(z_k=1|\\mathbf{x}) \u0026 \\equiv \\frac{p(z_k=1)p(\\mathbf{x}|z_k=1)}{\\sum_{j=1}^{K}p(z_j=1)p(\\mathbf{x}|z_j=1)} \\\\ \u0026 = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\end{align*} In this expression, $ \\pi_k $ is the prior probability (or mixing coefficient) for component $ k $, and $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is the likelihood of the data point $ \\mathbf{x} $ under the Gaussian distribution corresponding to component $ k $. The denominator is a normalization factor that ensures the responsibilities sum to 1 across all components for a given data point. This framework allows for a soft classification of data points, where each point is associated with a probability of belonging to each cluster, rather than being strictly assigned to a single cluster as in $K$-means. ","date":"2024-08-18","objectID":"/latent-variable-part1/:3:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"Maximum Likelihood Suppose we have a data set of observations $\\mathbf{X}=\\{ \\mathbf{x}_1,\\dots,\\mathbf{x}_n \\}^{T}\\in\\mathbb{R}^{N\\times D}$ and we want to model the data distribution $p(\\mathbf{X})$ using GMM. Assuming the data is independent and identically distributed (i.i.d.), the likelihood of the entire dataset can be expressed as: \\begin{align*} p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\prod_{n=1}^{N}\\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg). \\end{align*} To simplify the optimization process, we consider the log-likelihood function, which is given by: \\begin{align*} \\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \u0026= \\sum_{n=1}^{N}\\ln \\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg) \\end{align*} To solve the Maximum Likelihood Estimation (MLE) for Gaussian Mixture Models (GMMs), we typically consider the iterative Expectation-Maximization (EM) algorithm due to the non-convex nature of the problem. Before discussing how to maximize the likelihood, it is important to emphasize two significant issues that arise in GMMs: singularities and identifiability. Singularity A major challenge in applying the maximum likelihood framework to Gaussian Mixture Models is the presence of singularities. This problem arises because the likelihood function can become unbounded under certain conditions, leading to an ill-posed optimization problem. For simplicity, consider a Gaussian mixture model where each component has a covariance matrix of the form $ \\Sigma_k = \\sigma^2_k I $, where $ I $ is the identity matrix. Suppose one of the mixture components, say the $ j $-th component, has its mean $ \\boldsymbol{\\mu}_j $ exactly equal to one of the data points $ \\mathbf{x}_n $, so that $ \\boldsymbol{\\mu}_j = \\mathbf{x}_n $ for some value of $ n $. The contribution of this data point to the likelihood function would then be: \\begin{align*} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{x}_n, \\sigma^2_j I) = \\frac{1}{\\sqrt{2\\pi} \\sigma_j} \\cdot \\exp^0 \\end{align*} As $ \\sigma_j $ approaches 0, this term goes to infinity, causing the log-likelihood function to also diverge to infinity. Therefore, maximizing the log-likelihood function becomes an ill-posed problem because such singularities can always be present. These singularities occur whenever one of the Gaussian components collapses onto a specific data point, leading to a covariance matrix with a determinant approaching zero. This issue did not arise with a single Gaussian distribution because the variance cannot be zero by definition. Identifiability Another issue in finding MLE solutions for GMMs is related to identifiability. For any given maximum likelihood solution, a GMM with $ K $ components has a total of $ K! $ equivalent solutions. This arises from the fact that the $ K! $ different ways of permuting the $ K $ sets of parameters (means, covariances, and mixing coefficients) yield the same likelihood. In other words, for any point in the parameter space that represents a maximum likelihood solution, there are $ K! - 1 $ additional points that produce exactly the same probability distribution. This lack of identifiability means that the solution is not unique, complicating both the interpretation of the model and the optimization process. ","date":"2024-08-18","objectID":"/latent-variable-part1/:3:1","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning"],"content":"Expectation Maximization for GMM The goal of the Expectation-Maximization (EM) algorithm is to find maximum likelihood solutions for models that involve latent variables. Suppose that directly optimizing the likelihood $p(\\mathbf{X}|\\boldsymbol{\\theta})$ is difficult. However, it is easier to optimize the complete-data likelihood function $p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})$ as as discussed in the previous sections. In such cases, we can use the EM algorithm. EM algorithm is a general technique for finding maximum likelihood solutions in latent variable models. Let‚Äôs begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. By setting the derivatives of $\\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with respect to the means $\\boldsymbol{\\mu}_k$ of the Gaussian components to zero, we obtain \\begin{align*} 0 = -\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}\\boldsymbol{\\Sigma}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k) \\end{align*} Multiplying by $\\boldsymbol{\\Sigma}_k^{-1}$ (which we assume to be non-singular) and rearranging we obtain \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\gamma(z_{nk})\\mathbf{x}_n, \\end{align*} where we have defined \\begin{align*} N_k = \\sum_{n=1}^{N}\\gamma(z_{nk}). \\end{align*} We can interpret $N_k$ as the effective number of points assigned to cluster $k$. We can obtain the MLE solutions for other variables similarly. References: Christoper M. Bishop, Pattern Recognition and Machine Learning, 2006 ","date":"2024-08-18","objectID":"/latent-variable-part1/:3:2","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/latent-variable-part1/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular value decomposition tutorial","date":"2024-08-15","objectID":"/svd/","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/svd/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular Value Decomposition In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square matrix by extending the concept to asymmetric or rectangular matrices, which cannot be diagonalized directly using eigendecomposition. The SVD aims to find the following decomposition of a real-valued matrix $A$: $$A = U\\Sigma V^T,$$ where $U$ and $V$ are orthogonal (orthonormal) matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are called the left singular vectors of $A$, the columns of $V$ are called the right singular vectors, and the diagonal elements of $\\Sigma$ are called the singular values. Despite its widespread applications in areas such as data compression, noise reduction, and machine learning, SVD is often perceived as challenging to grasp. Many people find the mathematical intricacies daunting, even though there are numerous tutorials and explanations available. I remember struggling to fully understand SVD during my undergraduate studies, despite spending significant time on it. The complexity often arises from the abstract nature of the concepts involved, such as the interplay between eigenvectors, eigenvalues, and matrix decompositions. However, understanding SVD is crucial for many advanced techniques in data science and engineering. For instance, if the data matrix $A$ is close to being of low rank, it is often desirable to find a low-rank matrix that approximates the original data matrix well. As we will see, the singular value decomposition of $A$ provides the best low-rank approximation of $A$. The SVD process involves finding the eigenvalues and eigenvectors of the matrices $AA^T$ and $A^TA$. Since $A$ is generally not symmetric, it does not have orthogonal eigenvectors or guaranteed real eigenvalues, which complicates the process of SVD. However, the matrix $A^TA$ is guaranteed to be symmetric, as $$(A^TA)^T=A^TA,$$ and positive semi-definite, meaning all its eigenvalues are non-negative. Symmetric matrices have real eigenvalues and orthogonal eigenvectors, simplifying the decomposition process. These properties ensure that $A^TA$ can be diagonalized using an orthogonal matrix, which is crucial for deriving the SVD. The eigenvectors of $A^TA$ form the columns of $V$. We can diagonalize $A^TA$ as follows: $$A^TA = V\\Lambda V^T = \\sum_{i=1}^{n}\\lambda_i v_iv_i^T = \\sum_{i=1}^{n}\\sigma_i^2v_iv_i^T,$$ where the singular values of $A$ are defined as $\\sigma_i = \\sqrt{\\lambda_i}$. Since $A^TA$ is a symmetric matrix, its eigenvalues are non-negative. The matrix $\\Sigma$ in the SVD is a diagonal matrix whose diagonal entries are the singular values $\\sigma_1, \\dots, \\sigma_r$, where $r$ is the rank of $A$. Note that $rank(A) = rank(A^TA)$, and these singular values appear in the first $r$ positions on the diagonal of $\\Sigma$. For the $i$-th eigenvector-eigenvalue pair, we have $$A^TAv_i = \\sigma_i^2v_i.$$ Now, let‚Äôs derive the eigenvectors of $U$: \\begin{align*} A A^T (Av_i) \u0026= A (\\lambda_i v_i)\\\\ \u0026= \\lambda_i (A v_i). \\end{align*} Thus, $Av_i$ is an eigenvector of $AA^T$. However, to ensure that the matrix $U$ is orthonormal, we need to normalize these vectors as follows: \\begin{align*} u_i \u0026= \\frac{Av_i}{\\lVert Av_i\\rVert} \\\\ \u0026= \\frac{Av_i}{\\sqrt{(Av_i)^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^TA^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^T\\lambda_i v_i}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i \\underbrace{\\lVert v_i\\rVert}_{=1}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i}. \\end{align*} We can express $U$ as follows: $$U= \\left[\\frac{Av_1}{\\sigma_1}, \\dots, \\frac{Av_r}{\\sigma_r}, \\dots, \\frac{Av_n}{\\sigma_n}\\right].$$ Then, we have $$U\\Sigma = AV.$$ By rearranging, we get $$A = U\\Sigma V^{-1}.$$ Since the inverse of an orthogonal matrix $V$ is its transpose, $V^T$, the final form of the SVD is: $$A = U\\Sigma V^T.$$ ","date":"2024-08-15","objectID":"/svd/:0:0","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/svd/"},{"categories":["machine learning"],"content":"Introduction to Regression: Recursive Least Squares (Part 3)","date":"2024-08-12","objectID":"/recursive-least-square/","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Deep Dive into Regression: Recursive Least Squares Explained (Part 3) ","date":"2024-08-12","objectID":"/recursive-least-square/:0:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Introduction to Recursive Least Squares Ordinary least squares assumes that all data is available at once, but in practice, this isn‚Äôt always the case. Often, measurements are obtained sequentially, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where Recursive Least Squares (RLS) comes into play. RLS allows us to update our estimates efficiently as new measurements are obtained, without having to recompute everything from scratch. Suppose we have an estimate $\\boldsymbol{\\theta}_{k-1}$ after $k-1$ measurements and we receive a new measurement $\\mathbf{y}_k$. We want to update our estimate $\\boldsymbol{\\theta}_k$ using the following linear recursive model: \\begin{align*} \\mathbf{y}_k\u0026=\\mathbf{X}_k\\boldsymbol{\\theta} + \\boldsymbol{\\eta}_k\\\\ \\boldsymbol{\\theta}_k\u0026=\\boldsymbol{\\theta}_{k-1} + K_k (\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1}) \\end{align*} where $\\mathbf{X}_k$ is the observation matrix, $K_k$ is the gain matrix, and $\\boldsymbol{\\eta}_k$ represents the measurement error. The term $(\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is the correction term that adjusts our previous estimate using the new data. Also, $\\boldsymbol{\\eta}_k$ is the measurement error. The new estimate is modified from the previous estimate $\\boldsymbol{\\theta}_{k-1}$ with a correction via the gain matrix. To update the estimate optimally, we need to calculate the gain matrix $K_k$. This involves minimizing the variance of the estimation errors at time $k$. The error at step $k$ can be expressed as: \\begin{align*} \\boldsymbol{\\epsilon}_k \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k \\\\ \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1} - K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k (\\mathbf{X}_k\\boldsymbol{\\theta}+\\boldsymbol{\\eta}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k \\mathbf{X}_k(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1})-K_k\\boldsymbol{\\eta}_k\\\\ \u0026= (I-K_k \\mathbf{X}_k)\\boldsymbol{\\epsilon}_{k-1}-K_k\\boldsymbol{\\eta}_k, \\end{align*} where $I$ is the $d\\times d$ identity matrix. The mean of this error is then \\begin{align*} \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}] = (I-K_k \\mathbf{X}_k)\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]-K_k\\mathbb{E}[\\boldsymbol{\\eta}_{k}] \\end{align*} If $\\mathbb{E}[\\boldsymbol{\\eta}_{k}]=0$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]=0$, then $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\\boldsymbol{\\theta}$ is set equal to its expected value, then $\\boldsymbol{\\theta}_k=\\boldsymbol{\\theta}_k, \\forall k$. This property tells us that the estimator $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is unbiased. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\\boldsymbol{\\theta}$ on average. The key task is to find the optimal $K_k$ by minimizing the trace of the estimation-error covariance matrix $P_k = \\mathbb{E}[\\boldsymbol{\\epsilon}_k \\boldsymbol{\\epsilon}_k^T]$. This optimization leads to the following expression for ( K_k ): \\begin{align*} J_k \u0026= \\mathbb{E}[\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k\\rVert^2]\\\\ \u0026= \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}^T\\boldsymbol{\\epsilon}_{k}]\\\\ \u0026= \\mathbb{E}[tr(\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T)]\\\\ \u0026= tr(P_k), \\end{align*} where $P_k=\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T]$ is the estimation-error covariance (i.e., covariance matrix). Note that the third line holds by the trace of a product (i.e., cyclic property) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain ","date":"2024-08-12","objectID":"/recursive-least-square/:1:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Alternative Formulations Sometimes alternate forms of the equations for $P_k$ and $K_k$ are useful for computational purposes. Let‚Äôs first set $\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k = S_k$, then we get $$K_k = P_{k-1}\\mathbf{X}_k^TS_k^{-1}.$$ By putting this into $P_k$, we can obtain \\begin{align*} P_k \u0026= (I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)P_{k-1}(I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)^T+P_{k-1}\\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\\mathbf{X}_kP_{k-1}\\\\ \u0026\\quad \\vdots\\\\ \u0026= P_{k-1}-P_{k-1}\\mathbf{X}_k^TS_k^{-1}\\mathbf{X}_k^TP_{k-1}\\\\ \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}. \\end{align*} Note that $P_k$ is symmetric (c.f., $P_k=\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$. Then, we take the inverse of both sides of $$P_{k-1}^{-1} = \\bigg(\\underbrace{P_{k-1}}_{A}-\\underbrace{P_{k-1}\\mathbf{X}_k^T}_{B}\\big(\\underbrace{\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T}_{D}\\big)^{-1}\\underbrace{\\mathbf{X}_kP_{k-1}}_{C}\\bigg)^{-1}.$$ Next, we apply the matrix inversion lemma which is known as Sherman-Morrison-Woodbury matrix identity (or matrix inversion lemma) identity: $$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$ Then, rewrite $P_k^{-1}$ as follows: \\begin{align*} P_k^{-1} \u0026= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\\mathbf{X}_k^T\\big((\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)-\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\\mathbf{X}_k^T)\\big)^{-1}\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\\\ \u0026= P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k \\end{align*} This yields an alternative expression for the covariance matrix: \\begin{align*} P_k = \\big(P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\big)^{-1} \\end{align*} We can also obtain \\begin{align*} K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1} \\end{align*} By \\begin{align*} P_k \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}\\\\ P_kP_{k-1}^{-1} \u0026= (I-K_k\\mathbf{X}_k)\\\\ P_kP_k^{-1} \u0026= P_kP_{k-1}^{-1}+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k=I\\\\ I \u0026= (I-K_k\\mathbf{X}_k)+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\\\ K_k \u0026= P_{k}\\mathbf{X}_k^TR_{k}^{-1}. \\end{align*} ","date":"2024-08-12","objectID":"/recursive-least-square/:1:1","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Summary of RLS To summarize, the RLS algorithm can be updated as follows: Gain Matrix Update: $K_k = P_{k-1}\\mathbf{X}_k^T(\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)^{-1}$ or $K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1}$ Estimate Update: $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ Covariance Matrix Update: $P_k = (I-K_k\\mathbf{X}_k)P_{k-1}$. $P_k = (I-K_k \\mathbf{X}_k)P_{k-1}(I-K_k \\mathbf{X}_k)^T+K_kR_kK_k^T,$ ","date":"2024-08-12","objectID":"/recursive-least-square/:1:2","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Alternate Derivation of RLS This chapter will be posted soon. Stay tuned for updates! References: Simon Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-12","objectID":"/recursive-least-square/:2:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/recursive-least-square/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 2)","date":"2024-08-11","objectID":"/regression2/","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 2) ","date":"2024-08-11","objectID":"/regression2/:0:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model. Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data. To address these issues, regularization techniques are often used. Regularization involves adding a penalty term to the model‚Äôs objective function, which helps control the complexity of the model and prevents it from overfitting. ","date":"2024-08-11","objectID":"/regression2/:1:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Overdetermined and Underdetermined Problems Recall that linear regression is fundamentally an optimization problem where we aim to find the optimal parameter vector $\\boldsymbol{\\theta}_{opt}$ that minimizes the residual sum of squares between the observed data and the model‚Äôs predictions. Mathematically, this is expressed as: $$\\boldsymbol{\\theta}_{opt} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d}\\lVert y-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2.$$ ","date":"2024-08-11","objectID":"/regression2/:2:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Overdetermined Systems An optimization problem is termed overdetermined when the design matrix (or data matrix) $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}$ has more rows than columns, i.e., $m\u003ed$. This configuration means that there are more equations than unknowns, typically leading to a unique solution. In other words, there is more information available than the number of unknowns. The unique solution can be found using the formula: $$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$ The solution exists if and only if $\\mathbf{X}^T\\mathbf{X}$ is invertible, which is true when the columns of $\\mathbf{X}$ are linearly independent, meaning $\\mathbf{X}$ is full rank. ","date":"2024-08-11","objectID":"/regression2/:2:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Underdetermined Systems In contrast, when $\\mathbf{X}$ is fat and short (i.e., $m\u003cd$), the problem is called underdetermined. In this scenario, there are more unknowns than equations, leading to infinitely many solutions. This occurs because the system has less information than the number of unknowns. Among these, the solution that minimizes the squared norm of the parameters is preferred. This solution is known as the minimum-norm least-squares solution. For an underdetermined linear regression problem, the objective can be written as: \\begin{align*} \\boldsymbol{\\theta} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d} \\lVert \\boldsymbol{\\theta}\\rVert^2, \\quad \\textrm{subject to}\\ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} Here, $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}, \\boldsymbol{\\theta}\\in \\mathbb{R}^d,$ and $\\mathbf{y}\\in \\mathbb{R}^m$. If the matrix has rank$(\\mathbf{X})=m$, then the linear regression problem will have a unique global minimum \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} This solution is called the minimum-norm least-squares solution. The proof of this solution is given by: \\begin{align*} \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = \\lVert\\boldsymbol{\\theta}\\rVert^2 + \\boldsymbol{\\lambda}^T(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}), \\end{align*} where $\\boldsymbol{\\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives with respec to $\\boldsymbol{\\lambda}$ and $\\boldsymbol{\\theta}$ and setting them to zero: \\begin{align*} \\nabla_{\\boldsymbol{\\theta}} \u0026= 2 \\boldsymbol{\\theta} + \\mathbf{X}^T\\boldsymbol{\\lambda} = 0\\\\ \\nabla_{\\boldsymbol{\\lambda}} \u0026= \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y} = 0 \\end{align*} The first equation gives us $\\boldsymbol{\\theta} = -\\mathbf{X}^T\\boldsymbol{\\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\\mathbf{X})=N$ so that $\\mathbf{X}^T\\mathbf{X}$ is invertible, we have $\\boldsymbol{\\lambda} = -2 (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}.$ Thus, we have \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} Note that $\\mathbf{X}\\mathbf{X}^T$ is often called a Gram matrix, $\\mathbf{G}$. ","date":"2024-08-11","objectID":"/regression2/:2:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Regularization and Ridge Regression Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term that encourages the parameters to behave better, effectively controlling their magnitude. Ridge regression is a widely-used regularization technique that adds a penalty proportional to the square of the magnitude of the model parameters. The objective function for ridge regression is formulated as: \\begin{align*} J(\\boldsymbol{\\theta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2 \\end{align*} This can be expanded as: \\begin{align*} J(\\boldsymbol{\\theta}) = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) + \\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\end{align*} Breaking it down further: \\begin{align*} J(\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\lambda\\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta} \\end{align*} To minimize the objective function $J(\\boldsymbol{\\theta})$, we take the derivative with respect to $\\boldsymbol{\\theta}$ and set it to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + 2\\lambda\\mathbf{I}\\boldsymbol{\\theta} = 0 \\end{align*} Solving for $\\boldsymbol{\\theta}$, we obtain the ridge regression solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{align*} ","date":"2024-08-11","objectID":"/regression2/:3:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Understanding the Role of $\\lambda$ When $\\lambda$ approaches 0, the regularization term $\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ becomes negligible, making ridge regression equivalent to ordinary least squares (OLS), which can lead to overfitting if the model is too complex. If $\\lambda\\to 0$, then $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\underbrace{\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2}_{=0}$ As $\\lambda$ increases towards infinity, the regularization term dominates, forcing $\\boldsymbol{\\theta}$ towards zero. In this case, the solution becomes overly simplistic, effectively shrinking the model parameters to zero, which may result in underfitting. $\\lambda\\to \\infty$, then $\\underbrace{\\frac{1}{\\lambda}\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2}_{=0} + \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ Since what we want to do is to minimize the objective function, we can divide it by $\\lambda$. Therefore, the solution will be $\\boldsymbol{\\theta}=0$, because it is the smallest value the squared function can achieve. ","date":"2024-08-11","objectID":"/regression2/:3:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Spectral Decomposition and Invertibility It‚Äôs important to note that $\\mathbf{X}^T\\mathbf{X}$ is always symmetric. According to the Spectral theorem, this matrix can be decomposed as $\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$, where $\\mathbf{Q}$ is the eigenvector matrix, and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues. This allows us to express the inverse operation in ridge regression as: \\begin{align*} \\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{I}\\\\ \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{Q}\\mathbf{Q}^T\\\\ \u0026= \\mathbf{Q}(\\mathbf{\\Lambda}+\\lambda\\mathbf{I})\\mathbf{Q}^T. \\end{align*} Even if $\\mathbf{X}^T\\mathbf{X}$ is not invertible (or is close to being non-invertible), the regularization constant $\\lambda$ ensures invertibility by making the matrix full-rank. ","date":"2024-08-11","objectID":"/regression2/:3:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Dual Form of Ridge Regression Ridge regression can also be expressed in its dual form, which is particularly useful for solving underdetermined problems: \\begin{align*} (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= \\mathbf{X}^T\\mathbf{y}\\\\ \\boldsymbol{\\theta} \u0026= \\lambda^{-1}\\mathbf{I}(\\mathbf{X}^T\\mathbf{y}-\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\lambda^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\alpha\\\\ \\lambda\\alpha \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\mathbf{X}^T\\alpha)\\\\ \\mathbf{y} \u0026= (\\mathbf{X}\\mathbf{X}^T\\alpha+\\lambda\\alpha)\\\\ \\alpha \u0026= (\\mathbf{X}\\mathbf{X}^T+\\lambda)^{-1}\\mathbf{y}\\\\ \\alpha \u0026= (\\mathbf{G}+\\lambda)^{-1}\\mathbf{y}. \\end{align*} Here, $\\alpha$ represents the dual coefficients, and $\\mathbf{G}$ is the Gram matrix. This formulation is especially powerful when dealing with high-dimensional data, where the number of features exceeds the number of samples. ","date":"2024-08-11","objectID":"/regression2/:3:3","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Considering Varying Confidence in Measurements: Weighted Regression Up to this point, we have assumed that all measurements are equally reliable. However, in practice, the confidence in different measurements may vary. To account for this, we can consider the situation where the noise associated with each measurement has a zero mean and is independent across measurements. Under these conditions, the covariance matrix for the measurement noise can be expressed as follows: \\begin{align*} R \u0026= \\mathbb{E}(\\eta\\eta^T)\\\\ \u0026= \\begin{bmatrix} \\sigma_1^2 \u0026 \\dots \u0026 0\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ 0 \u0026 \\dots \u0026 \\sigma_l^2 \\end{bmatrix} \\end{align*} By denoting the error vector $\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}$ as $\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements: \\begin{align*} J(\\tilde{\\mathbf{x}}) \u0026= \\boldsymbol{\\epsilon}^TR^{-1}\\boldsymbol{\\epsilon}=\\frac{\\boldsymbol{\\epsilon}_1^2}{\\sigma_1^2}+\\dots+\\frac{\\boldsymbol{\\epsilon}_l^2}{\\sigma_l^2}\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^TR^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) \\end{align*} Note that by dividing each residual by its variance, we effectively equalize the influence of each data point on the overall fitting process. Subsequently, by taking the partial derivative of $J$ with respect to $\\boldsymbol{\\theta}$, we get the best estimate of the parameter, which is given by $$\\boldsymbol{\\theta} = (\\mathbf{X}^TR^{-1}\\mathbf{X})^{-1}\\mathbf{X}^TR^{-1}\\mathbf{y}.$$ Note that the measurement noise matrix $R$ must be non-singular for a solution to exist. To learn more, please take a look at this note! This article continues in Part 3. References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-11","objectID":"/regression2/:4:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/regression2/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 1)","date":"2024-08-10","objectID":"/regression1/","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/regression1/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 1) Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That‚Äôs why I‚Äôve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction. ","date":"2024-08-10","objectID":"/regression1/:0:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/regression1/"},{"categories":["machine learning"],"content":"Linear Regression Regression is a method used to identify the relationship between input and output variables. In a regression problem, we are given a set of noisy measurements (or output data) $\\mathbf{y} = [y_1, \\dots, y_d]^T$, which are affected by measurement noise $\\boldsymbol{\\eta} = [\\eta_1, \\dots, \\eta_d]^T$. The corresponding input data is denoted by $\\mathbf{x} = [x_1, \\dots, x_d]$. We refer to the collection of these input-output pairs as the training data, $\\mathcal{D} = {(\\mathbf{x}_1, \\mathbf{y}_1), \\dots, (\\mathbf{x}_m, \\mathbf{y}_m)}$. The true relationship between the input and output data is unknown and is represented by a function $f(\\cdot)$ that maps $\\mathbf{x}_n$ to $y_n$, i.e., $$ \\mathbf{y} = f(\\mathbf{x}). $$ Determining the exact function $f(\\cdot)$ from a finite set of data points $\\mathcal{D}$ is not feasible because there are infinitely many possible mappings for each $\\mathbf{x}_i$. The idea of regression is to introduce structure to the problem. Instead of trying to find the true $f(\\cdot)$, we seek an approximate model $g_\\theta(\\cdot)$, which is parameterized by $\\boldsymbol{\\theta} = [\\theta_1,\\dots,\\theta_d]^T$. For example, we might assume a linear relationship between $(\\mathbf{x}_n, \\mathbf{y}_n)$: \\begin{align*} g_{\\boldsymbol{\\theta}}(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}, \\end{align*} where $\\mathbf{X}$ is an $m \\times d$ input matrix derived from our observations. Since the true relationship is unknown, any chosen model is essentially a hypothesis. However, we can quantify the error in our model. Given a parameter $\\boldsymbol{\\theta}$, the error between the noisy measurements and the estimated values is: \\begin{align*} \\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} The goal of regression is to find the best $\\boldsymbol{\\theta}$ that minimizes this error. This leads us to the following objective function: \\begin{align*} J(\\boldsymbol{\\theta}) = \\boldsymbol{\\epsilon}^T \\boldsymbol{\\epsilon}. \\end{align*} This objective function is equivalent to minimizing the mean squared error (MSE): \\begin{align*} MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{x}_i \\boldsymbol{\\theta})^2. \\end{align*} We can optimize this function in closed form as follows: \\begin{align*} J(\\boldsymbol{\\theta}) \u0026= \\lVert\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}\\rVert^2_2 \\\\ \u0026= (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) \\\\ \u0026= \\mathbf{y}^T \\mathbf{y} - \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}. \\end{align*} To find the $\\boldsymbol{\\theta}$ that minimizes the objective function, we compute the derivative of the function and set it equal to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} = 0, \\end{align*} which simplifies to: \\begin{align*} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0, \\end{align*} leading to the solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}. \\end{align*} The equation $\\mathbf{X}^T(\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0$ is known as the normal equation. ","date":"2024-08-10","objectID":"/regression1/:1:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/regression1/"},{"categories":["machine learning"],"content":"Python Code Let‚Äôs implement a simple regression in Python: import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.randn(N) w_1 = 3.4 # True Parameter w_0 = 0.9 # True Parameter y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) W = np.array([w_1, w_0]) # From Scratch XtX = np.dot(X.T, X) XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m W_best = np.dot(XtXinvX, y.T) print(f\"W_best: {W_best}\") # Pythonic Approach theta = np.linalg.lstsq(X, y, rcond=None)[0] print(f\"Theta: {theta}\") t = np.linspace(0, 1, 200) y_pred = W_best[0]*t+W_best[1] yhat = theta[0]*t+theta[1] plt.plot(x, y, 'o') plt.plot(t, y_pred, 'r', linewidth=4) plt.show() To learn more, please take a look at this note! This article continues in Part 2. References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 ","date":"2024-08-10","objectID":"/regression1/:1:1","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/regression1/"},{"categories":["gpg"],"content":"Guide to encrypt/decrypt data using GPG","date":"2024-08-04","objectID":"/gpg-encryption/","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"Securing Your Privacy The importance of securing your data has become critical in the modern digital era. This post explores a versatile tool called GnuPG, or GNU Privacy Guard, which allows you to encrypt your data and communications, ensuring that only the intended recipients can access them. ","date":"2024-08-04","objectID":"/gpg-encryption/:0:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"Asymmetric Encryption Before looking at GPG, let‚Äôs first review some encryption approaches. A very naive approach to sharing encrypted files is to use the same secret key between a sender and a receiver. This approach is known as symmetric encryption. However, the symmetric approach has a limitation in that it requires a secure method for key exchange. To address this issue, asymmetric encryption is preferred. Asymmetric encryption, also known as public-key cryptography, is a method of encryption that uses a pair of keys: a public key and a private key, to encrypt and decrypt data. The public key is used for encryption. It is openly shared and can be distributed to anyone, allowing anyone to encrypt a message intended for the key owner. The private key is used for decryption. It is kept secret and known only to the owner, allowing the key owner to decrypt messages that were encrypted with the corresponding public key. This is how asymmetric encryption works: Key Pair Generation: A user generates a pair of keys: one public and one private. The public key is shared widely, while the private key is kept secure. Encryption: When someone wants to send a secure message, they use the recipient‚Äôs public key to encrypt the message. This ensures that only the recipient, who has the corresponding private key, can decrypt and read the message. Decryption: The recipient uses their private key to decrypt the received message. The private key is the only key that can decrypt the message encrypted with the corresponding public key. ","date":"2024-08-04","objectID":"/gpg-encryption/:1:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"How to Use GPG? Installation: sudo apt-get install gnupg # Ubuntu sudo pacman -S gnupg # Arch Generate a GPG Key: gpg --full-gen-key Export Public Key: gpg --export --armor your-email@example.com \u003e publickey.asc Import Public Key: gpg --import publickey.asc Encrypt a File: gpg --output encryptedfile.gpg --encrypt --recipient recipient@example.com file.txt Decrypt a File: gpg --output file.txt --decrypt encryptedfile.gpg ","date":"2024-08-04","objectID":"/gpg-encryption/:2:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"Singinig/Verifying Files A detached signature is a separate file that contains the signature of the original file. gpg --output file.sig --detach-sign file.txt file.txt is the file you want to sign. file.sig is the signature file generated. To verify the detached signature: gpg --verify file.sig file.txt To sign text files, gpg --clearsign \u003cfile name\u003e This will create a file called file.txt.asc which contains the original text and the signature. To verify gpg --verify \u003cfile.asc\u003e ","date":"2024-08-04","objectID":"/gpg-encryption/:3:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"A Simple GitHub Verification using ssh with gpg ","date":"2024-08-04","objectID":"/gpg-encryption/:4:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["gpg"],"content":"Generate SSH key ssh-keygen -t rsa or simply ssh-keygen id_rsa : private key id_rsa.pub : public key Go to SSH and GPG keys in the setting menu of GitHub Just paste your public key ","date":"2024-08-04","objectID":"/gpg-encryption/:4:1","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/gpg-encryption/"},{"categories":["Tools"],"content":"Guide to manage your tasks using TaskSpooler","date":"2024-07-13","objectID":"/taskspooler/","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"What is TaskSpooler? TaskSpooler (ts) is a lightweight job scheduler that allows you to queue up your tasks and execute them in order. It‚Äôs particularly useful for environments where tasks need to be managed sequentially or with a controlled degree of parallelism. Unlike more complex systems like SLURM, TaskSpooler is designed for simplicity and ease of use, making it accessible for individual researchers and small teams. ","date":"2024-07-13","objectID":"/taskspooler/:0:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"Efficient Job Scheduling for ML/DL Researchers with Taskspooler In the dynamic field of Machine Learning (ML) and Deep Learning (DL), managing and optimizing computational resources is crucial. For researchers frequently running numerous experiments, an efficient job scheduler can be a game-changer. Enter Taskspooler, a powerful yet user-friendly job scheduler for Linux, designed to help you manage and schedule your jobs in a queue. Taskspooler is a simpler alternative to SLURM, providing many benefits for ML/DL researchers, especially when it comes to utilizing GPUs efficiently. ","date":"2024-07-13","objectID":"/taskspooler/:1:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"TL;DR Job Queuing: Easily queue up multiple jobs, specifying the order of execution. Resource Management: Find and allocate empty GPUs to your tasks, maximizing resource utilization. Monitoring: Track the status of your jobs in real-time. Simplicity: A straightforward command-line interface that requires minimal setup and configuration. Parallel Execution: Control the number of jobs running simultaneously, which is essential for managing GPU workloads effectively. ","date":"2024-07-13","objectID":"/taskspooler/:1:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"Dive into TaskSpooler ","date":"2024-07-13","objectID":"/taskspooler/:2:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"Installation First, clone the repository: git clone https://github.com/justanhduc/task-spooler Install GPU Version To set up Task Spooler with GPU support, run the provided script: ./install_make Alternatively, to use CMake: ./install_cmake If Task Spooler is already installed, and you want to reinstall or upgrade, run: ./reinstall Install CPU Version If you would like to install only the CPU version, use the following commands (recommended): make cpu sudo make install or via CMake: mkdir build \u0026\u0026 cd build cmake .. -DTASK_SPOOLER_COMPILE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release make sudo make install ","date":"2024-07-13","objectID":"/taskspooler/:2:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"Basic Usage Let‚Äôs first put your task into a queue: ts python main.py This command queues up your python main.py. Keeping track of running and pending jobs is crucial for optimizing your workflow. Taskspooler provides real-time updates on job status, allowing you to make informed decisions and adjustments on the fly. To check the overall queue status, simply type: ts This returns your jobs with the job ID, state, time, and the command you typed. To track the status of your jobs: ts -c \u003cjob-id\u003e You can delete finished jobs in the job list by: ts -C To set the size of your job queue (i.e., to limit/expand the number of parallel running processes): ts -S \u003cqueue-size\u003e ","date":"2024-07-13","objectID":"/taskspooler/:3:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"GPU Utilization For ML/DL researchers, GPUs are invaluable but often limited resources. Taskspooler helps you find available GPUs and assign tasks to them efficiently. This ensures that your experiments run smoothly without unnecessary delays due to resource contention. To specify GPU indices for a job without checking whether they are free, use: ts -g [id,...] python main.py For instance: ts -g 1 python main.py This allows you to run your model on GPU 1. To get the GPU usage: ts -g ","date":"2024-07-13","objectID":"/taskspooler/:4:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Tools"],"content":"Conclusion Taskspooler offers a simple yet powerful solution for job scheduling and resource management, making it an excellent tool for ML/DL researchers. By effectively queuing your tasks and optimizing GPU usage, you can streamline your workflow and focus more on the research itself rather than managing computational resources. Whether you‚Äôre working on a single machine or a small cluster, Taskspooler can significantly enhance your productivity and efficiency. If you want to know more visit its official github repo ","date":"2024-07-13","objectID":"/taskspooler/:5:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/taskspooler/"},{"categories":["Python"],"content":"Guide to keep manage dependencies in Python ","date":"2024-07-07","objectID":"/poetry/","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Introduction Poetry is a dependency management and packaging tool in Python, aiming to improve how you define, install, and manage project dependencies. Installation: You can install Poetry through its custom installer script or using package managers. The recommended way is to use their installer script to ensure you get the latest version. Creating a New Project: Use poetry new \u003cproject-name\u003e to create a new project with a standard layout. Adding Dependencies: Add new dependencies directly to your project using poetry add \u003cpackage\u003e. Poetry will resolve the dependencies and update the pyproject.toml and poetry.lock files. Installing Dependencies: Running poetry install in your project directory will install all dependencies defined in your pyproject.toml file. ","date":"2024-07-07","objectID":"/poetry/:0:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Poetry Example ","date":"2024-07-07","objectID":"/poetry/:1:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Setting Up a New Project To create a new project named example_project with Poetry and manage its dependencies: poetry new example_project This command creates a new directory named example_project with some initial files, including a pyproject.toml file for configuration. The pyproject.toml file is what is the most important here. This will orchestrate your project and its dependencies. For now, it looks like this: [tool.poetry] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [\"author \u003cauthor@xxxxx.xxx\u003e\"] readme = \"README.md\" packages = [{include = \"poetry_demo\"}] [tool.poetry.dependencies] python = \"^3.7\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" we are allowing any version of Python 3 that is greater than 3.7.0. If you want to use Poetry only for dependency management but not for packaging, you can use the non-package modei: [tool.poetry] package-mode = false ","date":"2024-07-07","objectID":"/poetry/:1:1","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Add and Install Dependencies Suppose your project depends on requests for making HTTP requests and pytest for testing. To add these: poetry add requests poetry add pytest --dev The --dev flag indicates that pytest is a development dependency, not required for production. To remove a package, poetry remove \u003cpackage\u003e Running the command below will install all dependencies listed in your pyproject.toml: poetry install This also creates a virtual environment for your project if it doesn‚Äôt already exist. ","date":"2024-07-07","objectID":"/poetry/:1:2","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Run Your Project Run directly To run a script or start your project within the Poetry-managed virtual environment: poetry run python my_script.py This command ensures that python and any other commands are run within the context of your project‚Äôs virtual environment, using the correct versions of Python and all dependencies. Entry Point In Poetry, an entry point is a way to specify which Python script should be executed when your package is run. This is particularly useful for creating command-line applications or defining executable scripts that can be run directly from the command line after your package is installed. To define an entry point in a Poetry project, you use the [tool.poetry.scripts] section in your pyproject.toml file. This section allows you to map a command name to a Python function, which will be executed when the command is run. Edit your pyproject.toml file to include the [tool.poetry.scripts] section. This is where you define your entry point. Add the following lines to the pyproject.toml file: [tool.poetry.scripts] greet-app = \"greet_app.cli:main\" Here, greet-app is the command name, and greet_app.cli:main specifies the main function in the cli.py module inside the greet_app package. Shell Using Virtual Environment Shell: If you frequently run commands, you might find it convenient to activate the Poetry-managed virtual environment shell: poetry shell This will activate the virtual environment, and you can run your command without prefixing it with poetry run: greet-app To exit this new shell type exit. To deactivate the virtual environment without leaving the shell use deactivate. ","date":"2024-07-07","objectID":"/poetry/:1:3","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Init Method Instead of creating a new project, Poetry can be used to ‚Äòinitialise‚Äô a pre-populated directory. To interactively create a pyproject.toml file in directory pre-existing-project: poetry init Then, install it by poetry install To see the information about the project poetry env info -p: prints a path of the virtual environment By setting a config, you can generate a virtual environment in your projects: poetry config virtualenvs.in-project true ","date":"2024-07-07","objectID":"/poetry/:1:4","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Python"],"content":"Use with Git Poetry automatically creates a .gitignore file for you. It should include entries to ignore the virtual environment directory (.venv if you use Poetry‚Äôs built-in virtual environment management). Whenever you add or update dependencies, make sure to commit the pyproject.toml and poetry.lock files: To clone your project: git clone \u003cyour-repository-url\u003e cd my-project poetry install ","date":"2024-07-07","objectID":"/poetry/:2:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/poetry/"},{"categories":["Linux"],"content":"Pacman tutorial","date":"2024-05-01","objectID":"/pacman/","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Pacman, the package manager for Arch Linux, is known for its simple binary package format and easy-to-use build system. The primary aim of Pacman is to facilitate straightforward management of packages from both the official repositories and user-generated builds. Pacman ensures your system remains updated by synchronizing the package lists with the master server. This client/server model simplifies the process of downloading and installing packages, along with all their dependencies, using basic commands. ","date":"2024-05-01","objectID":"/pacman/:0:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Installing and Upgrading Packages Install a Package: sudo pacman -S \u003cpackage-name\u003e Full System Upgrade: sudo pacman -Syu -y synchronizes the database, similar to sudo apt-get update. -u upgrades all out-of-date packages, akin to sudo apt-get upgrade. Installing Packages from Git: Clone the package and install: git clone \u003crepository-url\u003e makepkg -si ","date":"2024-05-01","objectID":"/pacman/:0:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Removing Packages Remove a Specific Package: sudo pacman -R \u003cpackage-name\u003e Using -s removes dependencies not required by other packages, but be cautious as it may affect dependencies needed by other programs. Best Practice for Removing Packages: sudo pacman -Rns \u003cpackage-name\u003e Remove Obsolete Packages: sudo pacman -Sc ","date":"2024-05-01","objectID":"/pacman/:0:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Managing Package Lists List All Installed Packages: sudo pacman -Q List Manually Installed Packages: sudo pacman -Qe List Installed AUR Packages: sudo pacman -Qm List Unneeded Dependencies: sudo pacman -Qdt ","date":"2024-05-01","objectID":"/pacman/:0:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Getting Started with Pacman Creating a List of Installed Packages: Generate a list to easily reinstall packages on a new system: pacman -Qqen \u003e pkglist.txt To reinstall packages from the list: pacman -S - \u003c pkglist.txt ","date":"2024-05-01","objectID":"/pacman/:1:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Rollback to Previous Versions List Cached Packages: ls /var/cache/pacman/pkg/ To downgrade a package: sudo pacman -U \u003cpackage-file\u003e ","date":"2024-05-01","objectID":"/pacman/:1:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Managing Mirror Lists Edit and Refresh Mirror List: sudo vim /etc/pacman.d/mirrorlist sudo pacman -Syy Use -yy to force a refresh of the package databases, even if they are up to date. ","date":"2024-05-01","objectID":"/pacman/:1:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Linux"],"content":"Configuration Tips Enable Parallel Downloads: Open your configuration file: sudo vim /etc/pacman.conf Then uncomment or add the following line to enable multiple simultaneous downloads: ParallelDownloads=5 Ignore Specific Packages: Add the following line to /etc/pacman.conf to prevent specific packages from being updated: IgnorePkg = postgresql* ","date":"2024-05-01","objectID":"/pacman/:1:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/pacman/"},{"categories":["Vim"],"content":"The Appeal of Vim in Modern Programming","date":"2024-05-01","objectID":"/vim/","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/vim/"},{"categories":["Vim"],"content":"While it may look somewhat obsolete in an era dominated by graphically rich IDEs, Vim remains not just a highly relevant and effective tool for today‚Äôs programmers but also a badge of coolness in the tech world. Those who master its commands are often seen as coding wizards. With its unique advantages in speed, efficiency, and customizability, Vim is an invaluable asset in software development environments, proving that old-school can still be trendy. Vim is celebrated for its minimalistic approach, using fewer system resources, which facilitates fast and responsive editing. This efficiency is further enhanced by Vim‚Äôs command-driven interface, allowing developers to perform complex edits quickly through simple keystrokes. This minimizes the need for mouse use, thereby reducing the risk of repetitive strain injuries on the wrists. Here‚Äôs an example of how to yank (copy) and paste in Vim: To yank (copy) a line in Vim, just press yy. To paste the yanked text, simply move the cursor to where you want to insert the copied text and press p to paste after the cursor position or P to paste before it. The universal Ctrl+C to copy and Ctrl+V to paste are straightforward and familiar to most users. However, this often requires alternating between the keyboard and mouse, which can slow down the editing process and increase physical strain on the wrists. Vim‚Äôs approach is designed to keep your fingers on the keyboard, reducing the need to switch to a mouse and enhancing focus and productivity. This is especially beneficial in coding and scripting environments where rapid navigation and changes are common. Beyond its capabilities as a programming editor, Vim excels as a general-purpose text editor. It‚Äôs an excellent tool for note-taking and managing documentation, thanks to its lightweight nature and fast operation. Moreover, Vim is highly effective for composing complex documents in LaTeX (See my machine learning study notes!), allowing users to edit large amounts of text with ease and precision. The ability to customize Vim with plugins and scripts also extends its functionality into areas like PDF viewing and reference management. Customizability is another cornerstone of Vim‚Äôs design. Developers can adjust nearly every aspect of the editor to fit their workflow, from key bindings to complex integrations. Vim‚Äôs adaptability extends to broader system configurations, seamlessly integrating with tools like the i3 window manager (i3wm) and file browsers like LF and Ranger. This integration allows for a more unified and efficient desktop environment, where everything from file management to window resizing is streamlined through Vim-like commands. Mastering Vim not only boosts your coding efficiency but also earns you some cool points among tech peers who appreciate the power and elegance of classic tools. I will post more Vim tips soon! ","date":"2024-05-01","objectID":"/vim/:0:0","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/vim/"},{"categories":["Python"],"content":"Why Use Python's `pdb` Debugger Over an IDE?","date":"2024-04-27","objectID":"/pdb/","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"When it comes to debugging Python code, most programmers reach for an Integrated Development Environment (IDE) because of its convenience and powerful features. However, there‚Äôs a classic, built-in tool that shouldn‚Äôt be overlooked: Python‚Äôs own debugger, pdb. This tool might seem basic at first glance, but it offers some compelling advantages, especially in scenarios where an IDE might be less effective. Here‚Äôs why you might consider using pdb for debugging your Python projects: ","date":"2024-04-27","objectID":"/pdb/:0:0","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"Simplicity pdb comes as part of Python‚Äôs standard library, which means it‚Äôs ready to use out of the box‚Äîno installation or complex setup required. If you‚Äôre working on a simple script or need a quick debugging session, pdb is just a few keystrokes away. pdb offers an interactive session that lets you control the flow of your program. You can step through your code line by line, inspect and modify variables, and execute Python commands on the fly. This hands-on control can make finding and fixing bugs much clearer and sometimes even faster. ","date":"2024-04-27","objectID":"/pdb/:0:1","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"Environment Independence One of pdb‚Äôs greatest strengths is its versatility. Whether you‚Äôre coding on a local machine, a remote server, or even in a container, pdb works just the same. This universal compatibility is a huge plus, particularly when dealing with production environments where installing a full-fledged IDE isn‚Äôt feasible. Also, pdb operates entirely in the terminal, it‚Äôs perfect for low-resource environments or situations where a graphical interface might slow you down. This makes pdb incredibly efficient and responsive, even over network connections like SSH. ","date":"2024-04-27","objectID":"/pdb/:0:2","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"Flexibility in Use You can start pdb in several ways: directly from the command line, by inserting a breakpoint in your code, or as a module. This flexibility allows you to adapt your debugging approach to the needs of each specific project or problem. For developers who prefer working in text editors like Vim or Emacs, pdb integrates smoothly, enabling powerful debugging without leaving your editor. This integration supports a streamlined workflow, particularly for those who favor a more textual or minimalist development environment. ","date":"2024-04-27","objectID":"/pdb/:0:3","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"Conclusion While modern IDEs are undeniably powerful and user-friendly, pdb holds its own with features that are particularly suited to debugging in a variety of environments and situations. It‚Äôs a tool that encourages mastery of debugging by getting you close to the code in a way that GUI tools sometimes can‚Äôt match. Whether you‚Äôre a beginner looking to understand the inner workings of Python or an experienced developer needing a reliable tool on a remote server, pdb is worth exploring. ","date":"2024-04-27","objectID":"/pdb/:0:4","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/pdb/"},{"categories":["Python"],"content":"A tutorial for Enum ","date":"2024-04-26","objectID":"/enum/","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Enum is a way that Python enumerate variables. The enum module allows for the creation of enumerated constants‚Äîunique, immutable data types that are useful for representing a fixed set of values. These values, which are usually related by their context, are known as enumeration members. Enum provides‚Ä¶ Uniqueness - Each member of an Enum is unique within its definition, meaning no two members can have the same value. Attempting to define two members with the same value will result in an error unless you explicitly allow aliases. Immutability - Enum members are immutable. Once the Enum class is defined, you cannot change the members or their values. Iterability and Comparability - Enum classes support iteration over their members and can be compared using identity and equality checks. Accessing Members - You can access enumeration members by their names or values: Auto - If you want to automatically assign values to enum members, you can use the auto() function from the same module: from enum import Enum class State(Enum): PLAYING=0 PAUSED=1 GAME_OVER=2 If we just want to make sure them to be unique and automatically assigned, then use auto() from enum import Enum, auto class State(Enum): PLAYING=auto() PAUSED=auto() GAME_OVER=auto() print(State.PLAYING) print(State.PLAYING.value) Or simply, from enum import Enum, auto class State(Enum): PLAYING, PAUSED, GAME_OVER=range(3) print(State.PLAYING) print(State.PLAYING.value) However, this hard codes numbers, which can create an issue in the future. ","date":"2024-04-26","objectID":"/enum/:0:0","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Iterating over Enum Members You can iterate over the members of an enum: for state in State: print(state) ","date":"2024-04-26","objectID":"/enum/:0:1","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Comparison of Enum Members Enum members are singleton objects, so comparison is possible by identity: if State.PLAYING is State.PLAYING: print(\"RED is indeed RED\") ","date":"2024-04-26","objectID":"/enum/:0:2","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Using Enum as a Type Hint Enums can be used as type hints, enhancing code readability and correctness: def paint(color: Color): print(f\"Painting with {color.name}\") ","date":"2024-04-26","objectID":"/enum/:0:3","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Extending Enums: IntEnum and StrEnum For enums where the members are specifically integers or strings, you can inherit from IntEnum or StrEnum for additional benefits, like being able to compare members to integers or strings directly. from enum import IntEnum class Priority(IntEnum): LOW = 1 MEDIUM = 2 HIGH = 3 # Direct comparison with integers if Priority.LOW \u003c Priority.HIGH: print(\"Low priority is less than high\") ","date":"2024-04-26","objectID":"/enum/:0:4","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Unique Constraint To ensure that all enum values are unique, you can use the @unique decorator: from enum import Enum, unique @unique class StatusCode(Enum): OK = 200 NOT_FOUND = 404 ERROR = 500 Using @unique will raise a ValueError if any duplicate values are detected. ","date":"2024-04-26","objectID":"/enum/:0:5","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"Conclusion Enums in Python are useful for defining sets of named constants that are related and have a fixed set of members. They improve code readability, prevent errors related to using incorrect literal values, and can simplify type checking and validation in your programs. ","date":"2024-04-26","objectID":"/enum/:0:6","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/enum/"},{"categories":["Python"],"content":"A tutorial for Pytest","date":"2024-04-26","objectID":"/unit-tests/","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/unit-tests/"},{"categories":["Python"],"content":"Unit testing involves testing individual components of software in isolation to ensure they function correctly. Automated frameworks facilitate this process, which is integral to ensuring that new changes do not disrupt existing functionality. Unit tests also serve as practical documentation and encourage better software design. This testing method boosts development speed and confidence by confirming component reliability before integration. Early bug detection through unit testing also helps minimize future repair costs and efforts. pytest Pytest is one of the best tools that you can use to boost your testing productivity for Python codes. ","date":"2024-04-26","objectID":"/unit-tests/:0:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/unit-tests/"},{"categories":["Python"],"content":"Install pip install pytest pip install pytest-cov pytest --cov: this returns a coverage of test functions coverage html: log test results in html format ","date":"2024-04-26","objectID":"/unit-tests/:1:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/unit-tests/"},{"categories":["Python"],"content":"Example pytest is a libarary for testing. You can run your unit test code by pytest test_function.py If you wanna create a directory with several testing files, then just create __init__.py and put it inside the test dir. Then, just run pytest test_dir from calc import square import pytest def square(n): return n*n # This is a convention test_{func} def test_negative(): assert square(-2)==4 assert square(-3)==9 def test_positive(): assert square(2)==4 assert square(3)==9 def test_zero(): assert square(0)==0 def test_str(): # Write down what I expect to get # If it successfully raised the error that I expected # Then, it will pass the test with pytest.raises(TypeError): square(\"cat\") def main(): x = int(input(\"What's x? \")) print(\"x squared is\", square(x)) if __name__==\"__main__\": main() If you want to intentially raise an error, then you can do it by pytest.raises(\"SomeErrorType\") Note that you can write a warning message like assert x == \u003ccond\u003e, \u003cMSG\u003e ","date":"2024-04-26","objectID":"/unit-tests/:2:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/unit-tests/"},{"categories":["Programming"],"content":"A gentle introduction to bash scripting","date":"2024-04-21","objectID":"/shell-script-tutorial/","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Let‚Äôs create our first simple shell script #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Unix that the file is to be executed by /bin/sh. This is the standard location of the Bourne shell on just about every Unix system. If you‚Äôre using GNU/Linux, /bin/sh is normally a symbolic link to bash (or, more recently, dash). The second line begins with a special symbol: #. This marks the line as a comment, and it is ignored completely by the shell. The only exception is when the very first line of the file starts with #! (shebang) - as ours does. This is a special directive which Unix treats specially. It means that even if you are using csh, ksh, or anything else as your interactive shell, that what follows should be interpreted by the Bourne shell. Similarly, a Perl script may start with the line #!/usr/bin/perl to tell your interactive shell that the program which follows should be executed by perl. For Bourne shell programming, we shall stick to #!/bin/sh. The third line runs a command: echo, with two parameters, or arguments - the first is \"Hello\"; the second is \"World\". Note that echo will automatically put a single space between its parameters. To make it executable, run chmod +rx \u003cfilename\u003e ","date":"2024-04-21","objectID":"/shell-script-tutorial/:0:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables Let‚Äôs look back at our first Hello World example. This could be done using variables. Note that there must be no spaces around the ‚Äú=‚Äù sign: VAR=value works; VAR = value doesn‚Äôt work. In the first case, the shell sees the ‚Äú=‚Äù symbol and treats the command as a variable assignment. In the second case, the shell assumes that VAR must be the name of a command and tries to execute it. #!/bin/sh MY_MESSAGE=\"Hello World\" echo $MY_MESSAGE This assigns the string ‚ÄúHello World‚Äù to the variable MY_MESSAGE then echoes out the value of the variable. Note that we need the quotes around the string Hello World. Whereas we could get away with echo Hello World because echo will take any number of parameters, a variable can only hold one value, so a string with spaces must be quoted so that the shell knows to treat it all as one. Otherwise, the shell will try to execute the command World after assigning MY_MESSAGE=Hello The shell does not care about types of variables; they may store strings, integers, real numbers - anything you like. We can interactively set variable names using the read command; the following script asks you for your name then greets you personally #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME - hope you're well.\" ","date":"2024-04-21","objectID":"/shell-script-tutorial/:1:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Escape Characters Certain characters are significant to the shell; for example, that the use of double quotes (\") characters affect how spaces and TAB characters are treated, for example: $ echo Hello World Hello World $ echo \"Hello World\" Hello World So how do we display: Hello¬†\"World\" ? $ echo \"Hello \\\"World\\\"\" The first and last \" characters wrap the whole lot into one parameter passed to echo so that the spacing between the two words is kept as is. But the code: $ echo \"Hello \" World \"\" would be interpreted as three parameters: ‚ÄúHello¬†\" World \"‚Äù So the output would be Hello World Note that we lose the quotes entirely. This is because the first and second quotes mark off the Hello and following spaces; the second argument is an unquoted ‚ÄúWorld‚Äù and the third argument is the empty string; ‚Äú‚Äù. ","date":"2024-04-21","objectID":"/shell-script-tutorial/:2:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Loop ","date":"2024-04-21","objectID":"/shell-script-tutorial/:3:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"For Loop #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i\" done #!/bin/sh for i in hello 1 * 2 goodbye do echo \"Looping ... i is set to $i\" done The output of the above code is Looping ... i is set to hello Looping ... i is set to 1 Looping ... i is set to (name of first file in current directory) ... etc ... Looping ... i is set to (name of last file in current directory) Looping ... i is set to 2 Looping ... i is set to goodbye This is well worth trying. Make sure that you understand what is happening here. Try it without the * and grasp the idea, then re-read the Wildcards section and try it again with the * in place. Try it also in different directories, and with the * surrounded by double quotes, and try it preceded by a backslash (\\*) ","date":"2024-04-21","objectID":"/shell-script-tutorial/:3:1","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"While Loop #!/bin/sh INPUT_STRING=hello while [ \"$INPUT_STRING\" != \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done #!/bin/sh while : do echo \"Please type something in (^C to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done The colon (:) always evaluates to true; whilst using this can be necessary sometimes, it is often preferable to use a real exit condition. Compare quitting the above loop with the one below; see which is the more elegant. Also think of some situations in which each one would be more useful than the other: #!/bin/sh while read input_text do case $input_text in hello) echo English ;; howdy) echo American ;; gday) echo Australian ;; bonjour) echo French ;; \"guten tag\") echo German ;; *) echo Unknown Language: $input_text ;; esac done \u003c myfile.txt This reads the file ‚Äúmyfile.txt‚Äù, one line at a time, into the variable ‚Äú$input_text‚Äù. The case statement then checks the value of $input_text. If the word that was read from myfile.txt was ‚Äúhello‚Äù then it echoes the word ‚ÄúEnglish‚Äù. If it was ‚Äúgday‚Äù then it will echo Australian. If the word (or words) read from a line of myfile.txt don‚Äôt match any of the provided patterns, then the catch-all ‚Äú*‚Äù default will display the message ‚ÄúUnknown Language: $input_text‚Äù - where of course ‚Äú$input_text‚Äù is the value of the line that it read in from myfile.txt. A handy Bash (but not Bourne Shell) tip I learned recently from the Linux From Scratch project is: mkdir rc{0,1,2,3,4,5,6,S}.d instead of the more cumbersome: for runlevel in 0 1 2 3 4 5 6 S do mkdir rc${runlevel}.d done And ls can be done recursively, too: $ cd / $ ls -ld {,usr,usr/local}/{bin,sbin,lib} drwxr-xr-x 2 root root 4096 Oct 26 01:00 /bin drwxr-xr-x 6 root root 4096 Jan 16 17:09 /lib drwxr-xr-x 2 root root 4096 Oct 27 00:02 /sbin drwxr-xr-x 2 root root 40960 Jan 16 19:35 usr/bin drwxr-xr-x 83 root root 49152 Jan 16 17:23 usr/lib drwxr-xr-x 2 root root 4096 Jan 16 22:22 usr/local/bin drwxr-xr-x 3 root root 4096 Jan 16 19:17 usr/local/lib drwxr-xr-x 2 root root 4096 Dec 28 00:44 usr/local/sbin drwxr-xr-x 2 root root 8192 Dec 27 02:10 usr/sbin ","date":"2024-04-21","objectID":"/shell-script-tutorial/:3:2","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Test Test is used by virtually every shell script written. It may not seem that way, because test is not often called directly. test is more frequently called as [. [ is a symbolic link to test, just to make shell programs more readable. It is also normally a shell builtin (which means that the shell itself will interpret [ as meaning test, even if your Unix environment is set up differently): $ type [ [ is a shell builtin $ which [ /usr/bin/[ $ ls -l /usr/bin/[ lrwxrwxrwx 1 root root 4 Mar 27 2000 /usr/bin/[ -\u003e test $ ls -l /usr/bin/test -rwxr-xr-x 1 root root 35368 Mar 27 2000 /usr/bin/test This means that ‚Äò[‚Äô is actually a program, just like ls and other programs, so it must be surrounded by spaces: if [$foo = \"bar\" ] will not work; it is interpreted as if test$foo = \"bar\" ], which is a ‚Äò]‚Äô without a beginning ‚Äò[‚Äô. Put spaces around all your operators. I‚Äôve highlighted the mandatory spaces with the word ‚ÄòSPACE‚Äô . Note: Some shells also accept ‚Äú==‚Äù for string comparison; this is not portable, a single ‚Äú=‚Äù should be used for strings, or ‚Äú-eq‚Äù for integers. Test is a simple but powerful comparison utility. For full details, run man test on your system, but here are some usages and typical examples. Test is most often invoked indirectly via the if and while statements. It is also the reason you will come into difficulties if you create a program called test and try to run it, as this shell builtin will be called instead of your program! The syntax for if...then...else... is: if [ ... ] then # if-code else # else-code fi Also, be aware of the syntax - the ‚Äúif [ ... ]‚Äù and the ‚Äúthen‚Äù commands must be on different lines. Alternatively, the semicolon ‚Äú;‚Äù can separate them: if [ ... ]; then # do something fi You can also use the elif, like this: if [ something ]; then echo \"Something\" elif [ something_else ]; then echo \"Something else\" else echo \"None of the above\" fi This will echo \"Something\" if the [ something ] test succeeds, otherwise it will test [ something_else ], and echo \"Something else\" if that succeeds. If all else fails, it will echo \"None of the above\". ","date":"2024-04-21","objectID":"/shell-script-tutorial/:4:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Case The case statsement saves going through a whole set of if ... then ... else statements. Its syntax is really simple: #!/bin/sh echo \"Please talk to me ...\" while : do read INPUT_STRING case $INPUT_STRING in hello) echo \"Hello yourself!\" ;; bye) echo \"See you again!\" break ;; *) echo \"Sorry I don't understand\" ;; esac done ","date":"2024-04-21","objectID":"/shell-script-tutorial/:5:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables 2 The first set of variables we will look at are $0 ... $9 and $#. The variable $0 is the basename of the program as it was called. $1...$9 are the first 9 additional parameters the script was called with. The variable $@ is all parameters. The variable $* is similar, but does not preserve any whitespace and quoting, so ‚ÄúFile with spaces‚Äù becomes ‚ÄúFile‚Äù, ‚Äúwith‚Äù, and ‚Äúspaces‚Äù. $# is the number of parameters the script was called with. #!/bin/sh echo \"I was called with $# parameters\" echo \"My name is $0\" echo \"My first parameter is $1\" The othere two main variables set are $$ and $!. These are both process numbers. The $$ variable is the PID of the currently running shell. This can be useful for creating temporary files, such as /tmp/my-script.$$ which is useful if many instances of the script could be run at the same time, and they all need their own temporary files. The $! variable is the PID of the last run background processd. This is useful to keep track of the process as it gets on with its job. Another interesting vardfiable is IFS. This is the Interfal Field Separator. The default value is SPACE TAB NEWLINE, but if you are changing it, it‚Äôs easier to take a copy as shown: #!/bin/sh old_IFS=\"$IFS\" IFS=: # Set IFS as colon echo \"Please input some data separated by colons\" read x y z IFS=$old_IFS echo \"x is $x y is $y z is $z\" ","date":"2024-04-21","objectID":"/shell-script-tutorial/:6:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Functions #!/bin/sh add_a_user() { USER=$1 PASSWORD=$2 COMMENTS=$@ echo \"Adding user $USER\" echo useradd -c \"$COMMENTS\" $USER echo passwd $USER $PASSWORD } ","date":"2024-04-21","objectID":"/shell-script-tutorial/:7:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Programming"],"content":"Reference shellscript ","date":"2024-04-21","objectID":"/shell-script-tutorial/:8:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/shell-script-tutorial/"},{"categories":["Linux"],"content":"You should use Linux!","date":"2024-04-21","objectID":"/why-linux/","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/why-linux/"},{"categories":["Linux"],"content":"Linux, A Path to Digital Simplicity In an age dominated by digital clutter and overwhelming software choices, the minimalist philosophy stands out as a beacon for those seeking simplicity and efficiency. This approach not only applies to physical possessions but extends into the digital realm, where Linux has become a preferred tool for minimalists. Linux, an open-source operating system, embodies the principles of minimalism by offering users control over their digital environments. Unlike mainstream operating systems that often come loaded with non-essential features and bloatware, Linux allows users to select only the components they need, creating a lean and efficient system. The minimalist appeal of Linux is evident in its customizable nature. Users can choose from a variety of distributions, each tailored for different needs. For instance, distributions like Arch Linux provide barebone setup that users can expand as needed. This customization extends to the user interface, where options range from feature-rich desktop environments to more austere ones like i3wm or DWM, which use fewer resources and maintain a clean, unobtrusive design. Furthermore, Linux‚Äôs robust command-line interface is a minimalist‚Äôs dream. It enables users to perform tasks efficiently without the graphical overhead, which is particularly appealing to those who favor functionality and speed over visual elements. Moreover, Linux supports the concept of free software, which resonates with minimalists‚Äô preference for authenticity and freedom from commercial constraints. Users are free to modify, improve, and redistribute their software in ways that proprietary systems do not allow. Linux offers a compelling choice for anyone looking to embrace a minimalist digital lifestyle. It provides the tools to create a personalized and simple digital environment, encourages the efficient use of resources, and upholds values of freedom and simplicity. For those seeking to reduce their digital footprint while maximizing functionality, Linux proves that less can indeed be more. ","date":"2024-04-21","objectID":"/why-linux/:0:1","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/why-linux/"},{"categories":["Python"],"content":"Guide to keep sensitive data in Python","date":"2024-04-20","objectID":"/hide-sensitive-data/","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"An app‚Äôs config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes: Resource handles to the database, Memcached, and other backing services Credentials to external services such as Amazon S3 or Twitter Per-deploy values such as the canonical hostname for the deploy Apps sometimes store config as constants in the code. This is a violation of twelve-factor, which requires strict separation of config from code. Config varies substantially across deploys, code does not. A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. In a twelve-factor app, env vars are granular controls, each fully orthogonal to other env vars. They are never grouped together as ‚Äúenvironments‚Äù, but instead are independently managed for each deploy. This is a model that scales up smoothly as the app naturally expands into more deploys over its lifetime. ","date":"2024-04-20","objectID":"/hide-sensitive-data/:0:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"Environment Variable For example, you shouldn‚Äôt put information directly in your code. db_user = 'my_db_user' db_password = 'my_db_pass_123!' Let‚Äôs keep the sensitive information in .bash_profile as follows: export DB_USER=\"my_db_user\" export DB_PASS='my_db_pass_123!' Then, we can call them by import os db_user = os.environ.get('DB_USER') db_password = os.environ.get('DB_PASS') ","date":"2024-04-20","objectID":"/hide-sensitive-data/:1:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"dotenv ","date":"2024-04-20","objectID":"/hide-sensitive-data/:2:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"Introduction Python-dotenv reads key-value pairs from a .env file and can set them as environment variables. It helps in the development of applications following the 12-factor principles. ","date":"2024-04-20","objectID":"/hide-sensitive-data/:2:1","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"Basics Installation: pip install python-dotenv Create .env file in your project directory. Put the data (or variables) in the .env file. e.g, API_KEY=\"dafjei304aldkjf20akj\" To load your key, First, use load_dotenv() with os.getenv(\"[Your variable]\") e.g., API_KEY=os.getenv(\"API_KEY\") Make sure to update .gitignore to exclude the .env file. from dotenv import load_dotenv load_dotenv() API_KEY = os.getenv(\"API_KEY\") or \"\" ","date":"2024-04-20","objectID":"/hide-sensitive-data/:2:2","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"Reference The Twelve Factor App ","date":"2024-04-20","objectID":"/hide-sensitive-data/:3:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/hide-sensitive-data/"},{"categories":["Python"],"content":"Guide to understand type hint in Python.","date":"2024-04-20","objectID":"/type-hint/","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Type hinting is not mandatory, but it can make your code easier to understand and debug by Improved readability Better IDE support: IDEs and linters can use type hints to check your code for potential errors before runtime. While type hints can be simple classes like float or str, they can also be more complex. The typing module provides a vocabulary of more advanced type hints. ","date":"2024-04-20","objectID":"/type-hint/:0:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Basics # This is how you declare the type of a variable age: int = 1 # You don't need to initialize a variable to annotate it a: int # Ok (no value at runtime until assigned) # Doing so can be useful in conditional branches child: bool if age \u003c 18: child = True else: child = False x: int = 1 x: float = 1.0 x: bool = True x: str = \"test\" x: bytes = b\"test\" # For collections on Python 3.9+, the type of the collection item is in brackets x: list[int] = [1] x: set[int] = {6, 7} # For mappings, we need the types of both keys and values x: dict[str, float] = {\"field\": 2.0} # Python 3.9+ # For tuples of fixed size, we specify the types of all the elements x: tuple[int, str, float] = (3, \"yes\", 7.5) # Python 3.9+ # For tuples of variable size, we use one type and ellipsis x: tuple[int, ...] = (1, 2, 3) # Python 3.9+ # On Python 3.8 and earlier, the name of the collection type is # capitalized, and the type is imported from the 'typing' module from typing import List, Set, Dict, Tuple x: List[int] = [1] x: Set[int] = {6, 7} x: Dict[str, float] = {\"field\": 2.0} x: Tuple[int, str, float] = (3, \"yes\", 7.5) x: Tuple[int, ...] = (1, 2, 3) ","date":"2024-04-20","objectID":"/type-hint/:0:1","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Union Union is for multiple types def process_message(msg: Union[str, bytes, None]) -\u003e str: ... # On Python 3.10+, use the | operator when something could be one of a few types x: list[int | str] = [3, 5, \"test\", \"fun\"] # Python 3.10+ # On earlier versions, use Union x: list[Union[int, str]] = [3, 5, \"test\", \"fun\"] ","date":"2024-04-20","objectID":"/type-hint/:0:2","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Optional # food can be either str or None. def eat_food(food: Optional[str]) -\u003e None: ... # Use Optional[X] for a value that could be None # Optional[X] is the same as X | None or Union[X, None] x: Optional[str] = \"something\" if some_condition() else None if x is not None: # Mypy understands x won't be None here because of the if-statement print(x.upper()) # If you know a value can never be None due to some logic that mypy doesn't # understand, use an assert assert x is not None print(x.upper()) ","date":"2024-04-20","objectID":"/type-hint/:0:3","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Any Any is a special type hint in Python that indicates that a variable can be of any type. It essentially disables static type checking for that variable. It‚Äôs typically used when you want to explicitly indicate that a certain variable can have any type, or when dealing with dynamically typed code where the type of the variable cannot be easily inferred. While Any provides flexibility, it also sacrifices the benefits of static type checking, as type errors related to variables annotated as Any won‚Äôt be caught by type checkers. ","date":"2024-04-20","objectID":"/type-hint/:0:4","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Functions: Callable Types Callable type hint can define types for callable functions. from typing import Callable Callable[[Parameter types, ...], return_types] Callable objects are functions, classes, and so on. Type [input types] and return types def on_some_event_happened(callback: Callable[[int, str, str], int]) -\u003e None: ... def do_this(a: int, b: str, c:str) -\u003e int: ... on_some_event_happened(do_this) # This is how you annotate a callable (function) value x: Callable[[int, float], float] = f def register(callback: Callable[[str], int]) -\u003e None: ... # A generator function that yields ints is secretly just a function that # returns an iterator of ints, so that's how we annotate it def gen(n: int) -\u003e Iterator[int]: i = 0 while i \u003c n: yield i i += 1 # You can of course split a function annotation over multiple lines def send_email(address: Union[str, list[str]], sender: str, cc: Optional[list[str]], bcc: Optional[list[str]], subject: str = '', body: Optional[list[str]] = None ) -\u003e bool: ... ","date":"2024-04-20","objectID":"/type-hint/:0:5","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Classes class BankAccount: # The \"__init__\" method doesn't return anything, so it gets return # type \"None\" just like any other method that doesn't return anything def __init__(self, account_name: str, initial_balance: int = 0) -\u003e None: # mypy will infer the correct types for these instance variables # based on the types of the parameters. self.account_name = account_name self.balance = initial_balance # For instance methods, omit type for \"self\" def deposit(self, amount: int) -\u003e None: self.balance += amount def withdraw(self, amount: int) -\u003e None: self.balance -= amount # User-defined classes are valid as types in annotations account: BankAccount = BankAccount(\"Alice\", 400) def transfer(src: BankAccount, dst: BankAccount, amount: int) -\u003e None: src.withdraw(amount) dst.deposit(amount) ","date":"2024-04-20","objectID":"/type-hint/:0:6","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Annotated Annotated in python allows developers to declare type of a reference and and also to provide additional information related to it. name = Annotated[str, \"first letter is capital\"] This tells that name is of type str and that name[0] is a capital letter. On its own Annotated does not do anything other than assigning extra information (metadata) to a reference. It is up to another code, which can be a library, framework or your own code, to interpret the metadata and make use of it. For example FastAPI uses Annotated for data validation: def read_items(q: Annotated[str, Query(max_length=50)]) Here the parameter q is of type str with a maximum length of 50. This information was communicated to FastAPI (or any other underlying library) using the Annotated keyword. Annotated[\u003ctype\u003e, \u003cmetadata\u003e] Here is an example of how you might use Annotated to add metadata to type annotations if you were doing range analysis: @dataclass class ValueRange: lo: int hi: int T1 = Annotated[int, ValueRange(-10, 5)] T2 = Annotated[T1, ValueRange(-20, 3)] ","date":"2024-04-20","objectID":"/type-hint/:0:7","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"TypeVar This is a special type for generic types. from typing import Sequence, TypeVar, Iterable T = TypeVar(\"T\") # `T` is typically used to represent a generic type variable def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Since the generic type is used, batch_iter function can take any type of Sequence type data. For instance, Sequence[int], Sequence[str], Sequence[Person] If we use bound, then we can restrict the generic type. For example, from typing import Sequence, TypeVar, Iterable, Union T = TypeVar(\"T\", bound=Union[int, str, bytes]) def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Thus, the following code will show an error as it takes a list of float numbers: batch_iter([1.1, 1.3, 2.5, 4.2, 5.5], 2) Note that in Python 3.12, generic type hint has been changed ","date":"2024-04-20","objectID":"/type-hint/:1:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":["Python"],"content":"Reference ArjanCodes Type hint cheat sheet ","date":"2024-04-20","objectID":"/type-hint/:2:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/type-hint/"},{"categories":null,"content":"About Han","date":"2025-09-07","objectID":"/about/","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Han Cheol Moon üìë Download CV (PDF) Staff AI/LLM Engineer with a Ph.D. in Computer Science (NTU-NLP Group), specializing in large language models, robustness, and applied NLP systems. I combine cutting-edge research with production-grade deployment expertise, having led the development of NL2SQL systems, enterprise-scale document QA pipelines, and LLM safety frameworks at Samsung AI Center. Experienced in bridging academic research (ACL, KDD, EMNLP publications) with scalable LLM-based services, I bring deep technical knowledge, strong system design skills, and a proven record of delivering reliable, high-impact AI solutions. ","date":"2025-09-07","objectID":"/about/:0:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Research \u0026 Engineering Interests Natural Language Processing: Large Language Models (LLM), LLM Robustness, NL2SQL Machine/Deep Learning: Reinforcement Learning, Generative Models, Kernel Methods Engineering \u0026 Systems: LLM Ops, LLM-based Services, Scalable API Design for ML/DL Services ","date":"2025-09-07","objectID":"/about/:1:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Professional Employments Samsung Electronics, AI Center ‚Äî Staff ML/DL Engineer (Sept 2023 ‚Äì Present) Led the design and deployment of NL2SQL systems converting natural language queries into SQL, with agentic workflows, schema linking, and fine-tuning on custom databases. Architected scalable document-based question-answering and enterprise translation/summarization services using large language models, with pipelines for parsing, chunking, vector indexing, and API controls. Designed and implemented sensitive data leakage prevention systems, including prompt filtering, model output analysis, and red-teaming strategies, integrated into production AI pipelines. Managed academic collaborations with Seoul National University, supervising student researchers and organizing joint workshops. ","date":"2025-09-07","objectID":"/about/:2:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Education Nanyang Technological University, Singapore ‚Äî Ph.D. in Computer Science (2019‚Äì2023) Yonsei University, Korea ‚Äî M.S. in Electrical \u0026 Electronic Engineering (2016‚Äì2018) Chung-Ang University, Korea ‚Äî B.S. in Electrical \u0026 Electronic Engineering (2009‚Äì2016) Completed mandatory Military Service (2009‚Äì2012) during studies ","date":"2025-09-07","objectID":"/about/:3:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Publications Han Cheol Moon and Shafiq Joty. Reinforced Momentum Update for Textual Adversarial Attack, 2024. (In Progress) Han Cheol Moon. Toward Robust Natural Language Systems, Ph.D. Thesis, 2023. Han Cheol Moon, Shafiq Joty, Ruochen Zhao, Megh Thakkar, and Xu Chi. Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. In Proceedings of the Association for Computational Linguistics (ACL ‚Äô23), Toronto, Canada, 2023. Han Cheol Moon, Shafiq Joty, and Xu Chi. GradMask: Gradient-Guided Token Masking for Textual Adversarial Example Detection. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô22), Washington, DC, ACM, August 14‚Äì18, 2022. Junsik Jung, Han-Cheol Moon, Jooyoung Kim, Donghyun Kim, and Kar-Ann Toh. Wi-Fi Based User Identification Using In-Air Handwritten Signature. In IEEE Access, vol. 9, pp. 53548‚Äì53565, 2021. Han Cheol Moon, Tasnim Mohiuddin, Shafiq Joty, and Chi Xu. A Unified Neural Coherence Model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2262‚Äì2272, Hong Kong, China, 2019. Han-Cheol Moon, Se-In Jang, Kangrok Oh, and Kar-Ann Toh. An In-Air Signature Verification System Using Wi-Fi Signals. In Proceedings of the 2017 4th International Conference on Biomedical and Bioinformatics Engineering (ICBBE 2017), 2017. ","date":"2025-09-07","objectID":"/about/:4:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Skills Strong background in ML research and scalable LLM system development, with expertise in PyTorch, FastAPI, LangChain, Redis, RabbitMQ, and vector databases. ML/DL Research: PyTorch, TensorFlow, HuggingFace, SciPy, NumPy, Scikit-learn, Pandas ML/LLM System Development: LangChain / LangGraph, Pydantic AI, FastAPI, Redis, RabbitMQ, K6, Ansible, Vagrant Databases: PostgreSQL, MongoDB, SQLite (SQLAlchemy, Alembic), Vector Databases (Qdrant) Programming: Python (primary), Go, C/C++, MATLAB, Bash/Shell Workflow \u0026 Tools: Linux (Debian \u0026 Arch), Docker, Git, Async/Multiprocessing, LaTeX, Vim + i3 Other: Unit Testing, PlantUML, Documentation, Presentations, GIMP, Inkscape, Hugo Soft Skills: Clear Documentation, Engaging Presentations, Team Collaboration, Time Management, Problem-Solving ","date":"2025-09-07","objectID":"/about/:5:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching Internal Lecturer ‚Äî Samsung AI Center (2023‚ÄìPresent) Led a study group and delivered in-depth lectures and workshops on Machine Learning, Reinforcement Learning, Large Language Models (LLMs), and LLM service design for Samsung engineers and researchers. Covered theory, applications, and advanced topics such as policy gradients, prompt engineering, and fine-tuning. (See lecture materials). Teaching Assistant ‚Äî Nanyang Technological University, Singapore (2019‚Äì2020) DeepNLP ‚Äî Assisted in tutorials, grading, and mentoring students Teaching Assistant ‚Äî Yonsei University, Korea Digital Logic Circuits (Spring 2018) ‚Äî Assisted in tutorials and grading Multimedia Signal Processing (Fall 2017) ‚Äî Assisted in labs and coursework support Signals and Systems (Fall 2016) ‚Äî Assisted in tutorials and exam preparation ","date":"2025-09-07","objectID":"/about/:6:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Honors \u0026 Awards Singapore International Graduate Award (SINGA) ‚Äî 2019‚Äì2023 Top of Class Scholarship, Chung-Ang University ‚Äî 2015 Dean‚Äôs List, Chung-Ang University ‚Äî 2013‚Äì2014 ","date":"2025-09-07","objectID":"/about/:7:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Service \u0026 Leadership Republic of Korea Army ‚Äî Capital Defense Command ‚ÄúSHIELD‚Äù, 1st Security Group (2009‚Äì2012) Served as a Sergeant, guarding the presidential residence and performing security duties Developed discipline, responsibility, and leadership through service ","date":"2025-09-07","objectID":"/about/:8:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Technical Interests Technical Writing \u0026 Knowledge Sharing Consistently producing study notes and articles since my master‚Äôs program, covering LLM system design, deep learning, mathematics, and programming. I maintain a curated collection on GitHub and regularly share articles on my Blog, demonstrating my long-term commitment to continuous learning and knowledge sharing. Linux Ricing Passionate about customizing Linux environments for efficiency and aesthetics. Since 2016, I have explored Arch Linux, i3, and Vim-based setups, creating highly optimized workflows tailored to development and research. This long-term practice reflects both my technical curiosity and my persistence in mastering complex systems. ","date":"2025-09-07","objectID":"/about/:9:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":" --\u003e ","date":"0001-01-01","objectID":"/drafts/how_to_hugo/:0:0","tags":null,"title":"","uri":"/drafts/how_to_hugo/"},{"categories":null,"content":"Writing clean, efficient, and well-structured functions in Python is a skill that takes time to master. I‚Äôve learned so much about this craft, thanks to ArjanCodes, a YouTube channel that has been like a Python guru to me. ","date":"0001-01-01","objectID":"/drafts/how_to_write_functions/:0:0","tags":null,"title":"","uri":"/drafts/how_to_write_functions/"},{"categories":null,"content":"Python Naming Conventions: A Guide to Writing Clean and Readable Code When writing Python code, adhering to consistent naming conventions is crucial for readability and maintainability. Python‚Äôs official style guide, PEP 8, provides clear guidelines on how to name variables, functions, classes, and other elements in your code. In this blog post, we‚Äôll explore these conventions and provide examples to help you write clean, professional Python code. Note: PEP 8 is a document that provides various guidelines to write the readable in Python. PEP 8 describes how the developer can write beautiful code. ","date":"0001-01-01","objectID":"/drafts/pep8/:0:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"Why Naming Conventions Matter Naming conventions are more than just a formality‚Äîthey make your code easier to read, understand, and collaborate on. Consistent naming helps you and others quickly identify the purpose of a variable, function, or class. It also reduces the likelihood of errors and makes your codebase more maintainable. ","date":"0001-01-01","objectID":"/drafts/pep8/:1:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"PEP 8 Naming Conventions ","date":"0001-01-01","objectID":"/drafts/pep8/:2:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"1. Variables and Functions Use snake_case for variable and function names. Names should be lowercase, with words separated by underscores. Choose descriptive and concise names that reflect the purpose of the variable or function. # Good user_name = \"JohnDoe\" def calculate_total_price(items): pass # Bad UserName = \"JohnDoe\" # Pascal case is not recommended for variables def CalculateTotalPrice(items): # Pascal case is not recommended for functions pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:1","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"2. Constants Use UPPER_SNAKE_CASE for constants. Constants are typically defined at the module level and are intended to remain unchanged. # Good MAX_CONNECTIONS = 100 DEFAULT_TIMEOUT = 30 # Bad maxConnections = 100 # Not in uppercase default_timeout = 30 # Not in uppercase ","date":"0001-01-01","objectID":"/drafts/pep8/:2:2","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"3. Classes Use PascalCase (also known as CamelCase) for class names. Class names should be nouns and should clearly describe the object they represent. # Good class UserProfile: pass class DatabaseConnection: pass # Bad class user_profile: # Snake case is not recommended for classes pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:3","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"4. Methods Methods follow the same naming convention as functions: snake_case. Method names should be verbs or verb phrases that describe the action they perform. # Good class User: def get_name(self): pass def update_profile(self, new_data): pass # Bad class User: def GetName(self): # Pascal case is not recommended for methods pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:4","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"5. Modules and Packages Use lowercase names for modules and packages. Keep names short and descriptive. Underscores are acceptable, especially if they improve readability. Avoid underscores in module names if possible, but they are acceptable for readability. # Good import utilities from data_processing import analyzer # Bad import Utilities # Uppercase is not recommended from DataProcessing import Analyzer # Pascal case is not recommended ","date":"0001-01-01","objectID":"/drafts/pep8/:2:5","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"6. Private and Protected Members Use a single leading underscore (_) for non-public methods and variables. Use a double leading underscore (__) for name mangling (to make an attribute private to its class). class MyClass: def __init__(self): self._protected_variable = 42 # Protected self.__private_variable = 100 # Private def _protected_method(self): pass def __private_method(self): pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:6","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"7. Avoid Single-Letter Names Avoid using single-letter variable names except for trivial loop counters or mathematical variables. # Good for index in range(10): print(index) # Bad for i in range(10): # Not descriptive print(i) ","date":"0001-01-01","objectID":"/drafts/pep8/:2:7","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"8. Avoid Reserved Keywords Do not use Python reserved keywords (e.g., class, def, import) as variable or function names. # Bad class = \"MyClass\" # This will raise a SyntaxError ","date":"0001-01-01","objectID":"/drafts/pep8/:2:8","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"Best Practices for Naming Be Descriptive: Choose names that clearly describe the purpose of the variable, function, or class. Keep It Short but Meaningful: Avoid overly long names, but ensure they are descriptive enough. Be Consistent: Stick to the same naming conventions throughout your codebase. Avoid Ambiguity: Use names that are unambiguous and easy to understand. ","date":"0001-01-01","objectID":"/drafts/pep8/:3:0","tags":null,"title":"","uri":"/drafts/pep8/"}]