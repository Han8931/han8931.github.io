[{"categories":["programming","linux"],"content":"Setting Up DL/ML Environments","date":"2025-01-11","objectID":"/20250111_pytorch_container/","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Setting Up DL Experiment Environments ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:0:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"A Challenge for Arch Linux Users If you’ve ever tried to set up a new experiment environment for deep learning on Arch Linux, you’re probably familiar with the challenges involved. Arch Linux, renowned for its rolling-release model and cutting-edge updates, provides unparalleled flexibility and control over your system. However, this same flexibility can often lead to headaches when setting up complex environments for machine learning or deep learning experiments. Dependency conflicts, missing libraries, and version mismatches are all too common. One particular pain point is the setup of Python environments using tools like Conda or virtual environments. While these tools work seamlessly on many systems, Arch Linux users often encounter version conflicts. Installing Conda itself can be tricky and painful. For researchers and developers, this process can feel like a significant barrier to productivity. Instead of focusing on model development or data analysis, hours are spent troubleshooting environment issues. On Arch Linux, where package versions are always at the bleeding edge, finding compatibility between your system and the tools required by frameworks like PyTorch can be very challenging. This is where Docker steps in to save the day. By using Docker containers, you can create isolated, portable environments that encapsulate all the dependencies you need, regardless of the host operating system. For PyTorch users who rely on a CPU-only setup for studying DL/ML and testing PyTorch code, Docker offers a streamlined solution to avoid the usual hassle of configuring local environments on Arch Linux. In this blog post, I will go over the process of setting up a PyTorch container using Docker, exploring how it simplifies the creation of a reproducible environment for your deep learning experiments. ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:1:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Install Docker on Arch sudo pacman -S docker docker-compose docker-buildx Then, sudo systemctl enable --now docker.service ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:2:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"PyTorch Container ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:3:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Steps for Using PyTorch with CPU-Only Docker Image Pull the CPU-Only PyTorch Docker Image The latest PyTorch images without CUDA can be pulled using the following command: docker pull pytorch/pytorch:latest If you prefer a specific version, for example, PyTorch 1.13.1, use: docker pull pytorch/pytorch:1.13.1-cpu Run the PyTorch Container Start the container interactively: docker run -it --rm pytorch/pytorch:latest This will launch a Python environment with PyTorch installed. Mount Your Project Files (Optional) If you want to access your local project files inside the container, mount a directory: docker run -it --rm -v $(pwd):/workspace pytorch/pytorch:latest -it -i: Keeps STDIN open, allowing you to interact with the container (important for running interactive shells or REPLs). -t: Allocates a pseudo-TTY, which is useful for interactive sessions. --rm: This flag automatically removes the container when it stops. It’s useful to avoid cluttering your system with stopped containers. -v $(pwd):/workspace: This is the volume flag for mounting a directory. Here’s how it works: $(pwd) refers to the current working directory on your host machine (outside the container). /workspace is the directory inside the container where the mounted files will be accessible. The files inside /workspace in the container are directly linked to the files in your host machine’s current directory. If you modify a script in your host machine, the changes will be visible inside the container immediately. Similarly, if you create or edit a file inside the /workspace directory in the container, the changes will reflect on your host machine. Install Additional Python Libraries (If Needed) Install any extra libraries required for your project: pip install \u003cpackage-name\u003e To save this setup for future use, create a custom Docker image with these dependencies pre-installed. Write and Test PyTorch Code Create a simple PyTorch script (e.g., test.py): import torch # Check if CUDA is available print(\"CUDA Available:\", torch.cuda.is_available()) # Perform a simple tensor operation x = torch.tensor([5.0, 10.0, 15.0]) print(\"Tensor:\", x) Run it inside the container: python test.py The output should confirm that CUDA is not available and display the tensor. Save and Exit To persist changes, save your code in the mounted directory (e.g., /workspace). You can also commit the container if you’ve made extensive modifications: exit docker ps -a # Find the container ID docker commit \u003ccontainer_id\u003e my_pytorch_cpu_image Note that the container would not appear if you run the container with --rm. ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:3:1","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","python"],"content":"Introduction to asyncio library","date":"2025-01-05","objectID":"/20250105_asyncio/","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"For the past few months, I’ve been working on an exciting internal project at my company: taking users’ documents and running them through LLM APIs to translate and summarize their content, somewhat similar to DeepL . The output is a collection of translated documents, each overlaid with the newly translated text. Our goal is to provide a stable service that can handle large files efficiently for thousands of employees at Samsung—no small task! To achieve this, we needed a concurrency strategy that supports high throughput while remaining responsive. That’s where Asyncio comes in. In this post, we’ll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We’ll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle large-scale I/O more effectively. Whether you’re building an internal service for employees or creating a high-performance web server, Asyncio’s approach to concurrency might just be the key to unlocking the scalability you need. A Deep Dive into Asynchronous I/O Modern software frequently needs to handle large volumes of input/output (I/O) operations. For instance, you might be retrieving data from web services, communicating with microservices over a network, or running multiple database queries simultaneously. These tasks can often take hundreds of milliseconds—or even seconds—if the network is under heavy load or the database is busy. If you approach these operations in a strictly synchronous manner—doing one after another—each I/O request can block the execution of your entire application. When you have many such requests, total execution time can balloon significantly. Picture having to process 100 requests, each taking 1 second. Doing that sequentially results in a 100-second runtime. However, if you can handle them concurrently, you might complete all in roughly the same amount of time as a single request. In this post, we’ll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We’ll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle I/O more efficiently. ","date":"2025-01-05","objectID":"/20250105_asyncio/:0:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Why Concurrency Matters ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Synchronous Bottleneck In a synchronous application, each line of code must complete before moving on to the next. This is usually acceptable for simple tasks but becomes problematic if a single operation is slow or unresponsive. While any operation can block an application, many applications will be stuck waiting for I/O. I/O refers to a computer’s input and output devices such as a keyboard, hard drive, and, most commonly, a network card. A classic example is a web server that processes each request in series; if one request takes longer than expected, all subsequent requests are delayed. Users of a slow website or client application may experience hang-ups, timeouts, or sluggish responsiveness due to this “queue” of operations. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Concurrency as a Solution Concurrency allows multiple tasks to be in progress simultaneously. In code terms, this often means starting multiple operations and then efficiently switching between them, so the application doesn’t grind to a halt waiting on just one task. For I/O-bound tasks, concurrency can provide remarkable speedups because while one operation is waiting on a response, your program can continue working on other tasks. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Concurrency vs. Parallelism It’s important to distinguish concurrency from parallelism: Concurrency means you can have multiple tasks in progress at once, but they are not necessarily all running at the exact same moment. Parallelism means two or more tasks truly run at the same time, which requires at least as many CPU cores as the number of tasks you want to run in parallel. Even on a single-core machine, you can achieve concurrency by rapidly switching (or time slicing) between tasks. However, true parallelism requires multiple CPU cores, letting tasks run simultaneously without interrupting each other. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:3","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The I/O-Bound vs. CPU-Bound Distinction When we label a particular operation as I/O-bound or CPU-bound, we’re describing what fundamentally limits its performance. I/O-Bound: The operation spends most of its time waiting for I/O devices such as hard drives or network interfaces. Examples include fetching a remote web page, reading from a file, or waiting on a database query. These tasks can benefit significantly from concurrency, because while one operation waits, the program can do other work. CPU-Bound: The task is primarily gated by processor speed. Examples include computing the nth Fibonacci number using a recursive function, performing complex data analysis, or running CPU-intensive algorithms. Concurrency alone may not help here, especially in Python, because of the Global Interpreter Lock (GIL). ","date":"2025-01-05","objectID":"/20250105_asyncio/:2:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"A Primer on Processes, Threads, and the GIL ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Processes A process is a running instance of an application with its own memory space. An example of creating a Python process would be running a simple “hello world” application or typing python at the command line to start up the REPL (read eval print loop). Modern operating systems allow multiple processes to run at once. If your CPU has multiple cores, it can execute processes truly in parallel. Otherwise, the OS uses time slicing to rapidly switch among processes. ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Threads A thread is a more lightweight form of concurrency that runs within a single process, sharing the parent process’s memory space. Threads have no their own memory. A process will always have at least one thread associated with it, usually known as the main thread. A process can also create other threads, which are more commonly known as worker or background threads. These threads can perform other work concurrently alongside the main thread. Threads, much like processes, can run alongside one another on a multi-core CPU, and the operating system can also switch between them via time slicing. When we run a normal Python application, we create a process as well as a main thread that will be responsible for running our Python application. import os import threading print(f'Python process with process id: {os.getpid()}') num_threads = threading.active_count() thread_name = threading.current_thread().name print(f'{num_threads} thread(s) are running') print(f'The current thread is {thread_name}') Multithreading in Python You might assume that starting multiple threads automatically takes advantage of multi-core systems. However, Python has a key constraint called the Global Interpreter Lock (GIL). The GIL ensures that only one thread can run one Python instruction at a time. This means that even on a multi-core machine, your Python code cannot run more than one CPU-bound thread simultaneously within the same process. So, are threads useless in Python? Far from it. Threads do provide genuine concurrency for I/O-bound tasks because Python releases the GIL during I/O operations. This allows you to overlap network calls, file reads, etc., effectively improving throughput. Yet for CPU-bound tasks, you won’t get true parallelism using just threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Global Interpreter Lock (GIL) in More Detail The Global Interpreter Lock is often regarded as a tricky limitation in Python. At a high level, the GIL: Prevents multiple native threads from executing Python bytecode simultaneously. Releases the lock when code interacts with the operating system for I/O (e.g., network or disk). Reacquires the lock once I/O completes and Python bytecode needs to be executed again. Why does it exist? The main reason is memory safety in the CPython implementation, which relies heavily on reference counting to manage objects. While convenient, reference counting can become unsafe when multiple threads mutate the same objects without careful synchronization. For I/O-bound code—like sending concurrent HTTP requests—this arrangement works well. You start multiple threads, each waiting on different I/O operations, and the GIL is periodically released while those operations happen, giving an overall speedup. For CPU-bound tasks—like computing Fibonacci numbers with a naive recursion—threads won’t help much because the lock is rarely released. Instead, you might use multiprocessing or specialized libraries that bypass the GIL for compute-intensive work. ","date":"2025-01-05","objectID":"/20250105_asyncio/:4:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Enter Asyncio: Asynchronous I/O in a Single Thread Asyncio is Python’s built-in library (introduced in Python 3.4 and improved in Python 3.5 with the async and await keywords) that focuses on concurrent I/O without the overhead of managing threads or processes. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Coroutines The foundation of Asyncio is the concept of a coroutine—a special function that can pause itself (await) while waiting for an I/O operation, and then resume right where it left off once the operation completes. While one coroutine is waiting, other coroutines can continue running, effectively achieving concurrency within a single thread. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Event Loop At the core of every Asyncio program is the event loop. Think of it as a manager that schedules coroutines. The event loop steps through coroutines one by one: A coroutine starts running until it hits an await for an I/O operation. The coroutine “pauses,” returning control to the event loop. The event loop checks if there’s another coroutine ready to run. If so, it switches to that coroutine immediately. Meanwhile, the operating system handles the actual I/O. Once the I/O is ready (e.g., the network has responded), the event loop “wakes up” the paused coroutine and resumes its execution. Because only one thread is responsible for executing Python code, the GIL is never contended between multiple threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Where Asyncio Shines—and Where It Doesn’t ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Sweet Spot: I/O-Bound Work Asyncio is incredibly useful when you’re dealing with a large number of concurrent I/O operations. Common examples include: Building high-performance web servers that handle thousands of simultaneous connections. Writing web scrapers that fetch and parse dozens or hundreds of pages concurrently. Coordinating multiple microservice requests in a single workflow without blocking. In these scenarios, Asyncio’s single-threaded event loop can handle many I/O-bound coroutines, each pausing when it must wait for data. This often results in a dramatic improvement in throughput compared to a purely synchronous approach. ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Handling CPU-Bound Work What if your task is mainly compute-heavy? Asyncio won’t magically run CPU-bound code in parallel because the GIL still applies to Python bytecode, and you’re still on a single thread. For CPU-bound tasks—like image processing, machine learning, or large-scale data transformations—you’d likely want to offload work to another process or leverage special libraries that release the GIL. That said, Asyncio does provide interoperability with threading and multiprocessing; you can combine CPU-intensive tasks with your I/O-bound coroutines. For instance, you can use asyncio.to_thread (in Python 3.9+) to run a CPU-bound function in a separate thread or harness a process pool executor for true parallelism at the CPU level. ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Putting It All Together: An Example Below is a simplified comparison of synchronous, multithreaded, and Asyncio-based approaches to fetching two web pages: ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Synchronous Approach import time import requests def fetch_example(): response = requests.get('https://www.example.com') return response.status_code def sync_fetch(): start = time.time() status_one = fetch_example() status_two = fetch_example() end = time.time() print(f\"Synchronous: {status_one}, {status_two}, time={end - start:.4f}s\") if __name__ == \"__main__\": sync_fetch() Synchronous mode fetches one URL at a time. If each request blocks for one second, the total time is roughly two seconds. Here, Python will release the GIL while waiting for the network, letting both threads run concurrently. The total time is potentially cut almost in half, assuming the responses come back quickly. ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Asyncio Approach import time import asyncio import aiohttp async def async_fetch_example(): async with aiohttp.ClientSession() as session: async with session.get('https://www.example.com') as response: status = response.status print(status) async def main(): start = time.time() await asyncio.gather(async_fetch_example(), async_fetch_example()) end = time.time() print(f\"Asyncio: time={end - start:.4f}s\") if __name__ == \"__main__\": asyncio.run(main()) With Asyncio, both fetch operations are initiated concurrently in the same thread, with the event loop switching between them whenever one is waiting for I/O. Like multithreading, you should see a meaningful speedup compared to the synchronous approach—but without the complexities of shared data across threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["linux"],"content":"How to install Arch Linux","date":"2025-01-05","objectID":"/20250105_arch/","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"I recently bought a mini PC because I wanted a lightweight machine that I can easily carry anywhere. Arch Linux’s minimalistic, rolling-release approach aligns perfectly with my love for a Vim-based workflow and a highly customizable setup. While the process can seem intimidating at first, it’s an incredibly rewarding experience that offers complete control over your system. Installing Arch Linux (UEFI or BIOS) Arch Linux is well-known for giving users full control over their system. This guide walks you through a fresh Arch Linux installation. While it is detailed, always refer to the official Arch Wiki for up-to-date information. ","date":"2025-01-05","objectID":"/20250105_arch/:0:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Contents Prerequisites Creating a Bootable USB Initial Setup Check UEFI or BIOS Wi-Fi Connection Check Internet Time \u0026 NTP Partitioning Using fdisk or cfdisk BIOS Partition Scheme UEFI Partition Scheme Formatting and Mounting Creating File Systems Mounting Partitions Installing the Base System Fast Mirror Selection pacstrap / basestrap Generating fstab Chroot Configuration Setting up Network Manager Installing and Configuring GRUB Root Password Locale and Timezone Hostname Final Steps Post-Installation Creating a New User Sudoers Configuration Installing X.org and a Window Manager Fonts Enabling a Display Manager (Optional) Sound Setup Installing Yay (AUR Helper) ","date":"2025-01-05","objectID":"/20250105_arch/:1:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Prerequisites A working internet connection on another device (in case you need help or to follow the Arch Wiki). A USB drive of at least 2 GB capacity. Familiarity with the command line. Important: Installing Arch Linux involves formatting drives, which is destructive. Back up all important data before proceeding. ","date":"2025-01-05","objectID":"/20250105_arch/:2:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating a Bootable USB ","date":"2025-01-05","objectID":"/20250105_arch/:3:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"On Linux sudo dd if=\u003cpath-to-arch-iso\u003e of=\u003cpath-to-usb\u003e status=progress if = input file (the ISO file). of = output file (usually something like /dev/sdb). Be sure to confirm the correct USB path using lsblk or fdisk -l before running the command. ","date":"2025-01-05","objectID":"/20250105_arch/:3:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"On Windows Use Rufus . It’s a straightforward tool that avoids many potential pitfalls. ","date":"2025-01-05","objectID":"/20250105_arch/:3:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Initial Setup Boot from your USB and select the Arch Linux USB in your system’s boot menu. You should see a command-line shell once Arch boots. ","date":"2025-01-05","objectID":"/20250105_arch/:4:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Check UEFI or BIOS ls /sys/firmware/efi/efivars If you get an error, you’re in BIOS (Legacy) mode. If you see contents, you’re in UEFI mode. ","date":"2025-01-05","objectID":"/20250105_arch/:4:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Wi-Fi Connection If you’re on Wi-Fi, use iwctl: iwctl device list station \u003cwlan\u003e scan station \u003cwlan\u003e get-networks station \u003cwlan\u003e connect \u003cwifi-name\u003e station \u003cwlan\u003e show exit ","date":"2025-01-05","objectID":"/20250105_arch/:4:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Check Internet ping google.com If you have no connection, re-check your Wi-Fi settings or use a wired connection. ","date":"2025-01-05","objectID":"/20250105_arch/:4:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Time \u0026 NTP timedatectl set-ntp true This ensures your system clock stays synchronized. ","date":"2025-01-05","objectID":"/20250105_arch/:4:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Partitioning ","date":"2025-01-05","objectID":"/20250105_arch/:5:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Using fdisk or cfdisk Identify target disk: lsblk Open the disk utility: fdisk /dev/sda or cfdisk /dev/sda ","date":"2025-01-05","objectID":"/20250105_arch/:5:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"BIOS Partition Scheme A common layout might be: Boot: +200M Swap: typically 50% of your RAM size (+8G for 16 GB RAM) Root: at least +25G Home: rest of the disk space Press w to write changes and exit. ","date":"2025-01-05","objectID":"/20250105_arch/:5:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"UEFI Partition Scheme EFI: around +550M and formatted as FAT32. Swap: 50% of RAM or as needed. Root: Minimum +25G or more. Home: Rest of the disk (if desired on a separate partition). ","date":"2025-01-05","objectID":"/20250105_arch/:5:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Formatting and Mounting ","date":"2025-01-05","objectID":"/20250105_arch/:6:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating File Systems Example commands (adjust partitions to suit your layout): mkfs.ext4 /dev/sda1 # For /boot or /root or /home mkfs.fat -F32 /dev/sda1 # For UEFI partition if using UEFI mkswap /dev/sda2 # Swap partition ","date":"2025-01-05","objectID":"/20250105_arch/:6:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Mounting Partitions Swap: swapon /dev/sda2 Root: mount /dev/sda3 /mnt Boot (UEFI): mkdir /mnt/boot mount /dev/sda1 /mnt/boot Home (if separate): mkdir /mnt/home mount /dev/sda4 /mnt/home ","date":"2025-01-05","objectID":"/20250105_arch/:6:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing the Base System ","date":"2025-01-05","objectID":"/20250105_arch/:7:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Fast Mirror Selection Choose the fastest mirrors by editing /etc/pacman.d/mirrorlist. Move the closest/fastest mirrors to the top. This significantly speeds up package downloads. ","date":"2025-01-05","objectID":"/20250105_arch/:7:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"pacstrap / basestrap For Arch Linux: pacstrap /mnt base base-devel linux linux-firmware vim git For Artix Linux (example): basestrap -i /mnt base base-devel runit elogind-runit linux linux-firmware \\ grub networkmanager networkmanager-runit cryptsetup lvm2 lvm2-runit neovim vim ","date":"2025-01-05","objectID":"/20250105_arch/:7:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Generating fstab genfstab -U /mnt \u003e\u003e /mnt/etc/fstab Check the file to ensure correct entries: vim /mnt/etc/fstab ","date":"2025-01-05","objectID":"/20250105_arch/:7:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Chroot Now “enter” the new system: Arch: arch-chroot /mnt Artix: artix-chroot /mnt bash ","date":"2025-01-05","objectID":"/20250105_arch/:7:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Configuration ","date":"2025-01-05","objectID":"/20250105_arch/:8:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Setting up Network Manager pacman -S networkmanager systemctl enable NetworkManager (In Artix, you would enable the corresponding runit service instead.) ","date":"2025-01-05","objectID":"/20250105_arch/:8:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing and Configuring GRUB For BIOS pacman -S grub grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg For UEFI pacman -S grub efibootmgr grub-install --target=x86_64-efi --efi-directory=/boot grub-mkconfig -o /boot/grub/grub.cfg ","date":"2025-01-05","objectID":"/20250105_arch/:8:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Root Password passwd Set a strong password for the root user. ","date":"2025-01-05","objectID":"/20250105_arch/:8:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Locale and Timezone Edit /etc/locale.gen and uncomment your locale lines (e.g., en_US.UTF-8). Generate them: locale-gen Create /etc/locale.conf: echo \"LANG=en_US.UTF-8\" \u003e /etc/locale.conf Set your timezone: ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # or use tzselect ","date":"2025-01-05","objectID":"/20250105_arch/:8:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Hostname echo \"myhostname\" \u003e /etc/hostname ","date":"2025-01-05","objectID":"/20250105_arch/:8:5","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Final Steps Exit the chroot environment: exit umount -R /mnt reboot Remove your USB before booting, and the system should start from the newly installed Arch Linux. ","date":"2025-01-05","objectID":"/20250105_arch/:8:6","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Post-Installation ","date":"2025-01-05","objectID":"/20250105_arch/:9:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating a New User Log in as root. Create a user: useradd -m -g wheel \u003cusername\u003e passwd han Add additional groups if needed: usermod -aG audio,video,storage han ","date":"2025-01-05","objectID":"/20250105_arch/:9:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Sudoers Configuration Edit /etc/sudoers: visudo Add or uncomment a line to allow members of the wheel group to use sudo: %wheel ALL=(ALL:ALL) ALL ","date":"2025-01-05","objectID":"/20250105_arch/:9:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing X.org and a Window Manager Install Xorg: pacman -S xorg-server xorg-xinit Minimal Window Manager: pacman -S i3 dmenu rxvt-unicode Start X (for testing): startx For an automated start, add exec i3 in your ~/.xinitrc. ","date":"2025-01-05","objectID":"/20250105_arch/:10:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Fonts sudo pacman -S noto-fonts noto-fonts-cjk noto-fonts-emoji noto-fonts-extra Or any other font packages that suit your language preferences. For powerline or devicons, install Nerd Fonts: yay -S nerd-fonts-hack ","date":"2025-01-05","objectID":"/20250105_arch/:11:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Enabling a Display Manager (Optional) If you prefer a graphical login screen: sudo pacman -S lightdm lightdm-gtk-greeter sudo systemctl enable lightdm.service (Again, adapt for runit or other init systems.) ","date":"2025-01-05","objectID":"/20250105_arch/:12:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Sound Setup ","date":"2025-01-05","objectID":"/20250105_arch/:13:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Alsa sudo pacman -S alsa-utils alsa-plugins amixer Use M to unmute any channels. ","date":"2025-01-05","objectID":"/20250105_arch/:13:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"PulseAudio sudo pacman -S pulseaudio pulsemixer pulseaudio --start ","date":"2025-01-05","objectID":"/20250105_arch/:13:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing Yay (AUR Helper) Clone the Yay repository: git clone https://aur.archlinux.org/yay.git Build and install: cd yay makepkg -si Yay lets you install packages from both the official repositories and the AUR. ","date":"2025-01-05","objectID":"/20250105_arch/:14:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Conclusion Congratulations! Your Arch Linux system is now up and running. From here, you can customize it with whatever software and configurations you like. Remember, the Arch Wiki is your best friend for finding detailed guides and troubleshooting tips. Happy hacking! ","date":"2025-01-05","objectID":"/20250105_arch/:15:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels","date":"2024-09-01","objectID":"/20240902_svm3/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels Recall that the dual form of LS-SVM is given by \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way. Asymmetric kernels are particularly useful in capturing directional relationships in data that symmetric kernels cannot. For instance, in scenarios involving directed graphs or conditional probabilities, the relationship from $x$ to $y$ is inherently different from the relationship from $y$ to $x$. ","date":"2024-09-01","objectID":"/20240902_svm3/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"AsK-LS Primal Problem Formulation We first define a generalized kernel trick for an inner product of two mappings $\\phi_s$ and $\\phi_t$. \\begin{align*} K(\\mathbf{u}, \\mathbf{v}) = \\langle \\phi_s(\\mathbf{u}), \\phi_t(\\mathbf{v})\\rangle, \\forall \\mathbf{u} \\in \\mathbb{R}^{d_s}, \\mathbf{v} \\in \\mathbb{R}^{d_t}, \\end{align*} where $\\phi_s: \\mathbb{R}^{d_s}\\to \\mathbb{R}^{p}$, $\\phi_t: \\mathbb{R}^{d_t}\\to \\mathbb{R}^{p}$, and $\\mathbb{R}^p$ is a high-dimensional or even an infinite-dimensional space. Note that $d_s$ and $d_t$ can be different. This formulation is closely related to the traditional LS-SVM but extends it by simultaneously considering both source and target feature spaces. The optimization goal is to find the weight vectors $ \\omega $ and $ \\nu $, and bias terms $ b_1 $ and $ b_2 $, that minimize the following objective function: \\begin{align*} \\min_{\\omega, \\nu, b_1, b_2, e, h} \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2, \\end{align*} subject to the constraints: \\begin{align*} \u0026 y_i (\\omega^T \\phi_s(x_i) + b_1) = 1 - e_i\\ \u0026 y_i (\\nu^T \\phi_t(x_i) + b_2) = 1 - h_i \\end{align*} Here: $ \\omega $ and $ \\nu $ are weight vectors for the source and target features. $ \\phi_s(x) $ and $ \\phi_t(x) $ are the source and target feature mappings. $ e_i $ and $ h_i $ are error terms for the source and target constraints. $ \\gamma $ is a regularization parameter. Note that this formulation is almost the same as the LS-SVM except that this considers both the source and target feature spaces simultaneously. ","date":"2024-09-01","objectID":"/20240902_svm3/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Dual Form Let’s transform it into a dual form. The dual problem involves solving a system of linear equations derived from the primal problem’s Lagrangian. The Lagrangian function for the primal problem is: \\begin{align*} \\mathcal{L}( \\omega, \\nu, b_1, b_2, e, h, \\alpha, \\beta) \u0026= \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2\\\\ + \\sum_{i=1}^m \\alpha_i (1 - e_i \u0026- y_i (\\omega^T \\phi_s(x_i) + b_1)) + \\sum_{i=1}^m \\beta_i (1 - h_i - y_i (\\nu^T \\phi_t(x_i) + b_2)) \\end{align*} The KKT conditions are derived by setting the partial derivatives of the Lagrangian with respect to $ \\omega, \\nu, b_1, b_2, e, $ and $ h $ to zero. The dual problem leads to the following linear system: \\begin{align*} \\begin{bmatrix} 0 \u0026 0 \u0026 Y^T \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 Y^T \\\\ Y \u0026 0 \u0026 \\frac{I}{\\gamma} \u0026 H \\\\ 0 \u0026 Y \u0026 H^T \u0026 \\frac{I}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\alpha \\\\ \\beta \\end{bmatrix} =\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} where: $ Y $ is a vector of class labels. $ H $ is the kernel matrix with elements $ H_{ij} = y_i K(x_i, x_j) y_j $, where $ K(x_i, x_j) = \\langle \\phi_s(x_i), \\phi_t(x_j) \\rangle $ is the asymmetric kernel function. For an asymmetric kernel $ K $, the kernel function $ K(x_i, x_j) \\neq K(x_j, x_i) $. This asymmetry is directly incorporated into the matrix $ H $, where: \\begin{align*} H_{ij} \u0026= y_i K(x_i, x_j) y_j \\\\ H_{ji} \u0026= y_j K(x_j, x_i) y_i \\end{align*} AsK-LS uses two different feature mappings $ \\phi_s $ and $ \\phi_t $ for the source and target features. This approach allows capturing more information compared to symmetric kernels. The dual solution provides weight vectors $ \\omega $ and $ \\nu $, which span the target and source feature spaces, respectively. The decision functions for classification from the source and target perspectives are given by \\begin{align*} f_s(x) \u0026= \\sum_{i=1}^m \\beta_i y_i K(x, x_i) + b_1\\\\ f_t(x) \u0026= \\sum_{i=1}^m \\alpha_i y_i K(x_i, x) + b_2 \\end{align*} These decision functions leverage the learned asymmetric relationships in the data, providing a more nuanced classification model. ","date":"2024-09-01","objectID":"/20240902_svm3/:2:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 2.","date":"2024-08-31","objectID":"/20240825_svm2/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction to Least-Square SVM ","date":"2024-08-31","objectID":"/20240825_svm2/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a least squares cost function. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve. ","date":"2024-08-31","objectID":"/20240825_svm2/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Optimization Problem (Primal Problem) \\begin{align*} \u0026\\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2,\\\\ \u0026\\text{subject to } y_i (w^T \\phi(x_i) + b) = 1 - e_i, \\ \\forall i \\end{align*} where: $w$ is the weight vector. $b$ is the bias term. $e_i$ are the error variables. $\\gamma$ is a regularization parameter. $\\phi(x_i)$ is the feature mapping function. Note that $y_i^{-1} = y_i$, since $y_i = \\pm 1$. ","date":"2024-08-31","objectID":"/20240825_svm2/:1:1","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Lagrangian Function To solve the constraint optimization problem, we define the Lagrangian function: \\begin{align*} L(w, b, e, \\alpha) = \\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2 - \\sum_{i=1}^n \\alpha_i \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right], \\end{align*} where $\\alpha_i$ are Lagrange multipliers. Then, by setting the partial derivatives of the Lagrangian with respect to $w$, $b$, $e$, and $\\alpha$ to zero, we get the KKT conditions. $w$: \\begin{align*} \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) = 0 \\implies w = \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\end{align*} $b$: \\begin{align*} \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^n \\alpha_i y_i = 0 \\end{align*} $e_i$: \\begin{align*} \\frac{\\partial L}{\\partial e_i} = \\gamma e_i - \\alpha_i = 0 \\implies \\alpha_i = \\gamma e_i \\end{align*} Thus, $e_i = \\frac{\\alpha_i}{\\gamma}$ $\\alpha_i$: \\begin{align*} \\frac{\\partial L}{\\partial \\alpha_i} = - \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right] = 0 \\implies y_i (w^T \\phi(x_i) + b) = 1 - e_i, i=1,\\dots, N. \\end{align*} Let’s substitute $w$ and $e$: $K$: kernel matrix $\\alpha = [\\alpha_1, \\alpha_2, \\ldots, \\alpha_n]^T$: $N\\times 1$ $y = [y_1, y_2, \\ldots, y_n]^T$. $\\Omega = YKY^T$, where $\\Omega_{kl}= y_ky_l\\phi(x_k)^T\\phi(x_l)$ $b$: $1\\times 1$ Then, we can express it compactly \\begin{align*} \u0026 Y(KY^T\\alpha+b\\mathbf{1})-\\mathbf{1}+\\frac{\\alpha}{2\\gamma} = 0\\\\ \u0026 \\mathbf{1}^TY\\alpha = 0. \\end{align*} \\begin{align*} \\end{align*} By using the expression of $\\alpha$ and $b$, we get \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1_n \\end{bmatrix} \\end{align*} Note that the dimension of the matrix on the left-hand side is $(N+1)\\times (N+1)$. Once we have $b$ and $\\alpha$ by solving the linear system, the decision function for a new input $x$ can be obtained by: \\begin{align*} f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b. \\end{align*} Example Suppose we have three training examples with feature vectors $x_1, x_2$, and $x_3$, and corresponding labels $y_1, y_2$, and $y_3$. The kernel matrix $\\Omega$ is defined as: \\begin{align*} \\Omega_{ij} = y_i y_j K(x_i, x_j) \\end{align*} The dual form is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} $y = \\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\end{bmatrix}$ $\\alpha = \\begin{bmatrix} \\alpha_1\\ \\alpha_2 \\ \\alpha_3 \\end{bmatrix} $ $e = \\begin{bmatrix} 1 \\ 1 \\ 1 \\end{bmatrix}$ $I$ is a $3 \\times 3$ identity matrix Then, the $\\Omega$ is given by \\begin{align*} \\Omega = \\begin{bmatrix} y_1 y_1 K(x_1, x_1) \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 y_1 K(x_2, x_1) \u0026 y_2 y_2 K(x_2, x_2) \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3 y_3 K(x_3, x_3) \\end{bmatrix} \\end{align*} Now, the complete matrix equation is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 \\Omega_{11} + \\frac{1}{\\gamma} \u0026 \\Omega_{12} \u0026 \\Omega_{13} \\\\ y_2 \u0026 \\Omega_{21} \u0026 \\Omega_{22} + \\frac{1}{\\gamma} \u0026 \\Omega_{23} \\\\ y_3 \u0026 \\Omega_{31} \u0026 \\Omega_{32} \u0026 \\Omega_{33} + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} This can be written explicitly as: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 y_1^2 K(x_1, x_1) + \\frac{1}{\\gamma} \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 \u0026 y_2 y_1 K(x_2, x_1) \u0026 y_2^2 K(x_2, x_2) + \\frac{1}{\\gamma} \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 \u0026 y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3^2 K(x_3, x_3) + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} The solution to this matrix equation provides the values of $b$ and $ \\alpha_1, \\alp","date":"2024-08-31","objectID":"/20240825_svm2/:1:2","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 1.","date":"2024-08-25","objectID":"/20240825_svm1/","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Support Vector Machine ","date":"2024-08-25","objectID":"/20240825_svm1/:0:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Introduction Support Vector Machines (SVMs) are among the most effective and versatile tools in machine learning, widely used for various tasks. SVMs work by finding the optimal boundary, or hyperplane, that separates different classes of data with the maximum margin, making them highly reliable for classification, especially with complex datasets. What truly sets SVMs apart is their ability to handle both linear and non-linear data through the kernel trick, allowing them to adapt to a wide range of problems with impressive accuracy. In this blog post, we’ll delve into how SVMs work and gently explore the mathematical foundations behind their powerful performance. ","date":"2024-08-25","objectID":"/20240825_svm1/:1:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Orthogonal Projection When working with vectors $x$ and $y$, finding the orthogonal projection of $x$ onto $y$ is a common task in linear algebra. The projection is a way to express how much of $x$ lies in the direction of $y$. By definition, the magnitude of the projection $z$ of $x$ onto $y$ is given by:: $$\\lVert z\\rVert = \\lVert x\\rVert cos(\\theta).$$ Here, $\\theta$ is the angle between $x$ and $y$. To connect this with the dot product, recall that: $$x\\cdot y = \\lVert x\\rVert \\ \\lVert y\\rVert cos(\\theta).$$ This formula allows us to replace the cosine term: $$\\lVert z\\rVert = \\lVert x\\rVert \\frac{x\\cdot y}{\\lVert x\\rVert \\cdot\\lVert y\\rVert }.$$ Simplifying further, we express the magnitude of $z$ as: $$\\lVert z\\rVert = u\\cdot x,$$ where $u$ is an unit vector of $y$. Since $z$ is in the direction of $y$, we can write: $$z = \\lVert z\\rVert \\cdot u,$$ Then, \\begin{align*} z \u0026= (u\\cdot x)\\cdot u. \\end{align*} This gives us the final expression for the orthogonal projection of $x$ onto $y$: \\begin{align*} \\textrm{Proj}_yx \u0026= (u\\cdot x)\\cdot u\\\\ \u0026= \\Bigg(\\frac{y\\cdot x}{\\lVert y\\rVert ^2}\\Bigg)y\\\\ \u0026= \\Bigg(\\frac{y\\cdot x}{\\lVert y\\rVert }\\Bigg)\\frac{y}{\\lVert y\\rVert } \\end{align*} In this formula, the projection $\\textrm{Proj}_yx$ represents the component of $x$ that lies along the direction of $y$. ","date":"2024-08-25","objectID":"/20240825_svm1/:1:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Decision Boundary with Margin A hyperplane(or decision surface) is used to separate data points belonging to different classes. The goal of SVM is to find the optimal separating hyperplane. However, what is the optimal separating hyperplanes? The optimal hyperplane is the one which maximizes the distance from the hyperplane to the nearest data point of any class. Support vectors are the data points that lie closest to the hyperplane. The distance is referred to as the margin. SVMs maximize the margin around the separating hyperplane. The equation of a hyperplane in $\\mathbb{R}^p$ can be expressed as: $$\\mathbf{w}\\cdot \\mathbf{x}+b=0.$$ Here, $\\mathbf{w}$ is the normal vector to the hyperplane. It is clear by expressing it $$\\mathbf{w}(\\mathbf{x}-\\mathbf{x}_0)=0,$$ where $b = \\mathbf{w}\\cdot\\mathbf{x}_0$. % The support vectors are directly related to the optimal hyperplane. The decision function is fully specified by a subset of training samples, the support vectors. Let’s consider a simple scenario, where training data is linearly separable: $$\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) | \\mathbf{x}_i \\in \\mathbb{R}^p,\\ y_i \\in {-1,1}\\}_{i=1}^N.$$ Then, we can build two hyperplanes separating the data with no points between them: $H_1:\\mathbf{w}\\cdot \\mathbf{x}+b=1$ $H_2:\\mathbf{w}\\cdot \\mathbf{x}+b=-1$ All samples have to satisfy one of two constraints: $\\mathbf{w}\\cdot \\mathbf{x}+b\\geq1$ $\\mathbf{w}\\cdot \\mathbf{x}+b\\leq-1$ These constraints can be combined into a single expression: $$y(\\mathbf{w}\\cdot \\mathbf{x}+b)\\geq 1.$$ To maximize the margin, we can consider a unit vector $\\mathbf{u} = \\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}$, which is perpendicular to the hyperplanes and a point $x_0$ on the hyperplane $H_2$. If we scale $u$ from $x_0$, we get $z = x_0+ru$. If we assume $z$ is on $H_1$, then $\\mathbf{w}\\cdot z +b=1$. This is equivalent to \\begin{align*} \\mathbf{w}\\cdot (x_0+ru)+b=1\\\\ \\mathbf{w}x_0+\\mathbf{w}r\\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}+b=1\\\\ \\mathbf{w}x_0+r\\lVert \\mathbf{w}\\rVert +b=1\\\\ \\mathbf{w}x_0+b=1-r\\lVert \\mathbf{w}\\rVert \\end{align*} As $x_0$ is on $H_2$, we get $\\mathbf{w}x_0+b=-1$. Finally, we obtain \\begin{align*} -1=1-r\\lVert \\mathbf{w}\\rVert \\\\ r=\\frac{2}{\\lVert \\mathbf{w}\\rVert }. \\end{align*} Note that the scaled unit vector $ru$’s magnitude is $r$. Thus, the maximization of margin is equivalent to maximize $r$. To maximize $r$, we have to minimize $\\lVert \\mathbf{w} \\rVert$. Thus, finding the optimal hyperplane reduces to solving the following optimization problem: \\begin{align*} \u0026\\min \\lVert \\mathbf{w}\\rVert ,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Equivalently, \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert w\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Now, we have convex quadratic optimization problem. The solution of this problem gives us the optimal hyperplane that maximizes the margin (Details are in the following section). However, in practice, the data may not be perfectly separable. To account for this, we introduce a soft margin that allows for some misclassification. This is done by admitting small errors in classification and potentially using a more complex, nonlinear decision boundary, improving the generalization of the model. \\section{Error Handling in SVM} In practice, it’s unrealistic to expect a perfect separation of data, especially when the data is noisy or not linearly separable. To address this, we can allow for some prediction errors while still striving to find an optimal decision boundary. One approach is to minimize the norm of the weight vector, while penalizing the number of errors $N_e$. The optimization problem can be formulated as follows: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 +C\\cdot N_{e},\\quad \\text{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad \\forall i. \\end{align*} Here, $C$ is a regularization para","date":"2024-08-25","objectID":"/20240825_svm1/:1:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"SVM Optimization: Lagrange Multipliers ","date":"2024-08-25","objectID":"/20240825_svm1/:2:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Lagrange Multipliers Consider the optimization problem: \\begin{align*} \u0026\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\ \u0026\\text{subject to}\\quad g(\\mathbf{x})=0. \\end{align*} To find the minimum of $f$ under the constraint $g(\\mathbf{x})$, we use the method of Lagrange multipliers. The key idea is that at the optimal point, the gradient of $f(\\mathbf{x})$ must be parallel to the gradient of $g(\\mathbf{x})$. Mathematically, this condition is expressed as: $$\\nabla f(\\mathbf{x}) = \\lambda\\nabla g(\\mathbf{x}).$$ Example: Consider a simple 2D example where you want to minimize the function $f(x,y)=x^2+y^2$, which represents a circle centered at the origin. This function increases as you move away from the origin, so the minimum is at the origin. Now, consider the constraint: $g(x,y)=x+y-1=0$. This constraint is a line that runs through the $xy$-plane. Our goal is to find the point on this line that minimizes $f(x,y)$. A Lagrange multiplier is like a balancing factor that adjusts the direction and magnitude of your search along the constraint. As you move along the constraint line $g(x,y)$, $\\lambda$ ensures that the solution also respects the shape of the function $f(x,y)$ that you are trying to minimize. To solve the constraint optimization problem, we define the Lagrangian function: $$\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x}).$$ To find the minimum, we take the partial derivatives of $\\mathcal{L}(\\mathbf{x}, \\lambda)$ with respect to both $\\mathbf{x}$ and $\\lambda$, and set them equal to zero. \\subsection{SVM Optimization} Recall that we want to solve the following convex quadratic optimization problem: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} The objective is to find the optimal hyperplane that maximizes the margin between two classes of data points. We can reformulate this optimization problem using the method of Lagrange multipliers, which introduces a set of multipliers $\\alpha_i$ (one for each constraint). The Lagrangian function for this problem is given by: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} ","date":"2024-08-25","objectID":"/20240825_svm1/:2:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Duality and the Lagrangian Problem While we could attempt to solve the primal optimization problem directly, it is often more practical, especially for large datasets, to reformulate the problem using the duality principle. The dual form is advantageous because it depends only on the inner products of the data points, which allows the use of kernel methods for non-linear classification. To find the solution to the primal problem, we solve the following problem: \\begin{align*} \u0026\\max_{\\mathbf{w}, b}\\min_\\alpha \\mathcal{L}(\\mathbf{w}, b, \\alpha)\\\\ \u0026\\textrm{subject to}\\quad \\alpha_i\\geq 0, \\forall i. \\end{align*} Here, we maximize the Lagrangian with respect to the multipliers $\\alpha_i$, while minimizing with respect to the primal variables $\\mathbf{w}$ and $b$. ","date":"2024-08-25","objectID":"/20240825_svm1/:2:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Handling Inequality Constraints with KKT Conditions You may observe that the method of Lagrange multipliers is used for equality constraints. However, it can be extended to handle inequality constraints through the use of additional conditions known as the Karush-Kuhn-Tucker (KKT) conditions. These conditions ensure that the solution satisfies the necessary optimality criteria for problems with inequality constraints. ","date":"2024-08-25","objectID":"/20240825_svm1/:2:3","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"The Wolfe Dual Problem The Lagrangian problem for SVM optimization involves $N$ inequality constraints, where $N$ is the number of training examples. This problem is typically tacked using its dual form. The duality principle provides a powerful framework, stating that an optimization problem can be approached from two perspectives: The primal problem, which in our context is a minimization problem. The dual problem, which is a maximization problem. An important aspect of duality is that the maximum value of the dual problem is always less than or equal to the minimum value of the primal problem. This relationship means that the dual problem provides a lower bound to the solution of the primal problem. In the context of SVM optimization, we are dealing with a convex optimization problem. According to Slater’s condition, which applies to problems with affine constraints, strong duality holds. Strong duality implies that the optimal values of the primal and dual problems are equal, meaning the maximum value of the dual problem equals the minimum value of the primal problem. This equality allows us to solve the dual problem instead of the primal problem, often leading to computational advantages. Recall that we aim to solve the following optimization problem: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} The minimization problem involves solving the partial derivatives of $\\mathcal{L}$ with respect to $\\rvw$ and $b$ and set them equal to zero: \\begin{align*} \u0026\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\mathbf{w} - \\sum_i \\alpha_i y_i \\mathbf{x}_i\\\\ \u0026 \\nabla_b\\mathcal{L}(\\mathbf{w}, b, \\alpha) = -\\sum_i \\alpha_i y_i \\end{align*} Form the first equation, we obtain: \\begin{align*} \u0026\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i\\ \\end{align*} Next, we substitute the objective function with $\\rvw$: \\begin{align*} \\mathbf{W}(\\alpha, b) \u0026= \\frac{1}{2}\\left(\\sum_i \\alpha_i y_i \\mathbf{x}_i\\right)\\cdot \\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\left[y_i\\left(\\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\cdot \\mathbf{x}_i+b\\right)-1\\right]\\\\ \u0026= \\frac{1}{2}\\Big(\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j\\Big)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\Bigg[y_i\\Bigg(\\Big(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\Big)\\cdot \\mathbf{x}_i+b\\Bigg)\\Bigg]+\\sum_i \\alpha_i \\\\ \u0026= \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j - \\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i \\cdot \\mathbf{x}_j\\\\ \u0026\\quad -\\sum_i \\alpha_i y_i b+\\sum_i \\alpha_i \\\\ \u0026= \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j-\\sum_i \\alpha_i y_i b \\end{align*} Note that we use two indices, $i$ and $j$ when substituting $\\mathbf{W}$. This is obvious if we consider a simple example. Imagine you have two data points: \\begin{align*} \\mathbf{x}_1,y1\u0026=(1,2),1\\\\ \\mathbf{x}_2,y2\u0026=(2,1),−1 \\end{align*} Then, \\begin{align*} \\lVert \\mathbf{w}\\rVert^2=\\mathbf{w}\\cdot \\mathbf{w}=\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_i}\\cdot\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_j}. \\end{align*} This simplification shows that the optimization problem can be reformulated purely in terms of the Lagrange multipliers $\\alpha_i$. Note that the term involving $b$ can be removed by setting $b=0$, simplifying our equation further: \\begin{align*} \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j (\\mathbf{x}_i\\cdot \\mathbf{x}_j) \\end{align*} This expression is known as the Wolfe dual Lagrangian function. We have transformed the problem into one involving only the multipliers $\\alpha_i$, resulting in a quadratic programming problem, commonly referred to as the Wolfe dual problem: \\begin{align*} \u0026\\max_\\alpha \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j ","date":"2024-08-25","objectID":"/20240825_svm1/:3:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Karush-Kuhn-Tucker conditions When dealing with optimization problems that involve inequality constraints, such as those encountered in Support Vector Machines (SVMs), an additional requirement must be met: the solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The KKT conditions are a set of first-order necessary conditions that must be satisfied for a solution to be optimal. These conditions extend the method of Lagrange multipliers to handle inequality constraints and are particularly useful in non-linear programming. For the KKT conditions to apply, the problem must also satisfy certain regularity conditions. Fortunately, one of these regularity conditions is Slater’s condition, which we have already established holds true for SVMs. ","date":"2024-08-25","objectID":"/20240825_svm1/:4:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"KKT Conditions and SVM Optimization In the context of SVMs, the optimization problem is convex, meaning that the KKT conditions are not only necessary but also sufficient for optimality. This implies that if a solution satisfies the KKT conditions, it is guaranteed to be the optimal solution for both the primal and dual problems. Moreover, in this case, there is no duality gap, meaning the optimal values of the primal and dual problems are equal. Reference Alexandre Kowalczyk, Support Vector Machines Succinctly, 2017 ","date":"2024-08-25","objectID":"/20240825_svm1/:4:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"On the direction of gradient descent update","date":"2024-08-19","objectID":"/20240819_gradient_descent/","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"On Gradient Descent Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the function’s minimum value. It is a fundamental concept in machine learning, particularly in training models such as neural networks. The gradient is a vector that represents the direction of the steepest increase of the function at a given point. For example, for a convex function $z = ax^2 + by^2$, the gradient is $[2ax, 2by]$, which points in the direction of the steepest ascent. In gradient descent, the goal is to minimize the function, so the algorithm moves in the opposite direction of the gradient, which is $[-2ax, -2by]$. This opposite direction is chosen because it is the direction of the steepest decrease in the function value. But how do we know that moving in this direction will strictly decrease the function value? ","date":"2024-08-19","objectID":"/20240819_gradient_descent/:0:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"Direction of Gradient Descent Let’s investigate the direction of gradient descent. The derivative of the objective function $f(\\mathbf{x})$ provides the slope of $f(\\mathbf{x})$ at the point $f(\\mathbf{x})$. It tells us how to change $\\mathbf{x}$ in order to make a small improvement in our goal. A function $f(\\mathbf{x})$ can be approximated by its first-order Taylor expansion at $\\bar{\\mathbf{x}}$: $$f(\\mathbf{x})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T(x-\\bar{\\mathbf{x}})$$ Now let $\\mathbf{d}\\neq0, |\\mathbf{d}|=1$ be a direction, and in consideration of a new point $\\mathbf{x}:=\\bar{\\mathbf{x}}+\\mathbf{d}$, we define: $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}$$ We would like to choose $\\mathbf{d}$ that minimizes the function $f$. From the Cauchy-Schwarz inequality1 , we know that $$|\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}|\\leq \\lVert\\nabla f(\\bar{\\mathbf{x}})\\rVert \\ \\lVert\\mathbf{d}\\rVert.$$ The equality holds if and only if $\\mathbf{d}=\\lambda \\nabla f(\\bar{\\mathbf{x}})$, where $\\lambda\\in \\mathbb{R}$. Since we want to minimize the function $f$, we negate the steepest direction $\\mathbf{d}^{*}$, then $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}}).$$ Since $\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always positive, the term $-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always negative. Therefore, by updating $\\mathbf{x}$ $$\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\lambda \\nabla f(\\mathbf{x}^{(k)}),$$ we get $$f(\\mathbf{x}^{(k+1)}) \u003c f(\\mathbf{x}^{(k)}).$$ Cauchy-Schwarz Inequaility: $|\\mathbf{a}\\cdot \\mathbf{b}|\\leq \\lVert\\mathbf{a}\\rVert \\ \\lVert\\mathbf{b}\\rVert$. Equality holds if and only if either $\\mathbf{a}$ or $\\mathbf{b}$ is a multiple of the other. ↩︎ ","date":"2024-08-19","objectID":"/20240819_gradient_descent/:1:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"Latent variable tutorial","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Latent Variable Modeling ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:0:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Motivation of Latent Variable Modeling Let’s say we want to classify some data. If we had access to a corresponding latent variable for each observation $ \\mathbf{x}_i $, modeling would be more straightforward. To illustrate this, consider the challenge of finding the latent variable (i.e., the true class of $ \\mathbf{x} $). It can be expressed like $ z^* = \\argmax_{z} p(\\mathbf{x} | z) $. It is hard to identify the true clusters without prior knowledge about them. For example, we can cluster like Fig. (b) or (c). Consider modeling the complete data set $ p(\\mathbf{x} | z) $ under the assumption that the observations are independent and identically distributed (i.i.d.). Based on the above Fig. (c), the joint distribution for a single observation $ (\\mathbf{x}_i, \\mathbf{z}_i) $ given the model parameters $ \\boldsymbol{\\theta} $ can be expressed: \\begin{align*} p(\\mathbf{x}_i, \\mathbf{z}_i | \\boldsymbol{\\theta}) = \\begin{cases} p(\\mathcal{C}_1) p(\\mathbf{x}_i | \\mathcal{C}_1) \u0026 \\text{if } z_i = 0 \\\\ p(\\mathcal{C}_2) p(\\mathbf{x}_i | \\mathcal{C}_2) \u0026 \\text{if } z_i = 1 \\\\ p(\\mathcal{C}_3) p(\\mathbf{x}_i | \\mathcal{C}_3) \u0026 \\text{if } z_i = 2 \\\\ \\end{cases} \\end{align*} Given $ N $ observations, the joint distribution for the entire dataset $ { \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N } $ along with their corresponding latent variables $ { \\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_N } $ is: \\begin{align*} p(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N, \\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_N | \\boldsymbol{\\theta}) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\pi_k^{z_{nk}} \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_{nk}} \\end{align*} Here, $ \\pi_k = p(\\mathcal{C}_k) $ represents the prior probability of the $ k $-th component, and $ p(\\mathbf{x}_n | \\mathcal{C}_k) = \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ denotes the Gaussian distribution associated with component $ \\mathcal{C}_k $. However, in practice, the latent variables $ \\mathbf{z}_k $ are often not directly observable, which complicates the modeling process. In the following sections, we present various methods for identifying and handling these latent variables to improve the classification and modeling of data. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:1:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"K-Means Clustering Suppose that we have a data set $\\mathbf{X} = {\\mathbf{x}_1,\\dots, \\mathbf{x}_n}$ consisting of $N$ observations of a random $D$-dimensional variable $\\mathbf{x}\\in \\mathbb{R}^{D}$. Our goal is to partition the data into $K$ of clusters. Intuitively, a cluster can be thought as a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This notion can be formalized by introducing a set of $D$-dimensional vectors $\\boldsymbol{\\mu}_k$, which represents the centers of the clusters. Our goal is to find an assignment of data points to clusters, as well as a set of vectors ${\\boldsymbol{\\mu}_k}$. Objective function of $K$-means clustering (distortion measure) can be defined as follows: $$J = \\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}\\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k\\rVert^2$$ , where $r_{nk}\\in{0,1}$ is a binary indicator variable which represents the membership of data $\\mathbf{x}_n$. It can be expressed as follows: % The $r_{nk}$ can be optimized in a closed-form solution as follows: \\begin{align*} r_{nk}=\\begin{cases} 1 \u0026 \\text{if } k=\\argmin_{j} \\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_j\\rVert^2\\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\end{align*} Our goal is to find values for the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$ that minimize $J$. We can minimize $J$ through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$ First we choose some initial values for the $\\boldsymbol{\\mu}_k$. Then in the first phase we minimize $J$ with respect to the $r_{nk}$, keeping the $\\boldsymbol{\\mu}_k$ fixed. In the second phase we minimize $J$ with respect to the $\\boldsymbol{\\mu}_k$, keeping $r_{nk}$ fixed. This two-stage optimization is then repeated until convergence. Now consider the optimization of the $\\boldsymbol{\\mu}_k$ with the $r_{nk}$ held fixed. The objective function $J$ is a quadratic function of $\\boldsymbol{\\mu}_k$, and it can be minimized by setting its derivative with respect to $\\boldsymbol{\\mu}_k$ to zero giving \\begin{align*} 2\\sum_{n=1}^{N}r_{nk}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k) = 0. \\end{align*} We can arrange as \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{\\sum_n r_{nk}\\boldsymbol{x}_n}{\\sum_n r_{nk}}. \\end{align*} The denominator of $\\boldsymbol{\\mu}_k$ is equal to the number of points assigned to cluster $k$. The mean of cluster $k$ is essentially the same as the mean of data points $\\mathbf{x}_n$ assigned to cluster $k$. For this reason, the procedure is known as the $K$-means clustering algorithm. The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments. These two phases reduce the value of the objective function $J$, so the convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of $J$. We can also sequentially update the $\\mu_k$ as follows: \\begin{align*} \\mu_{k+1} = \\mu_{k} + \\eta(\\mathbf{x}_k-\\mu_{k}) \\end{align*} There are some properties to note: It is a hard clustering algorithm ($\\leftrightarrow$ soft clustering) It is sensitive to the initialization of centroid. The number of clusters is uncertain. Sensitive to distance metrics (e.g., Euclidean?) ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:2:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Gaussian Mixture Models $K$-means clustering is a form of hard clustering, where each data point is assigned to exactly one cluster. However, in some cases, soft clustering—where data points can belong to multiple clusters with varying degrees of membership—provides a better model in practice. A Gaussian Mixture Model (GMM) assumes a linear superposition of Gaussian components, offering a richer class of density models than a single Gaussian distribution. In essence, rather than assuming that all data points are generated by a single Gaussian distribution, we assume that the data is generated by a mixture of $ K $ different Gaussian distributions, where each Gaussian represents a different component in the mixture. For a single sample, the Gaussian Mixture Model can be expressed as a weighted sum of these individual Gaussian distributions: \\begin{align*} p(\\mathbf{x}) \u0026= \\sum_\\mathbf{z} p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) \\\\ \u0026= \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\end{align*} Here, $ \\mathbf{x} $ is a data point, $ \\pi_k $ represents the mixing coefficients, $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is a Gaussian distribution with mean $ \\boldsymbol{\\mu}_k $ and covariance $ \\boldsymbol{\\Sigma}_k $, and $ K $ is the number of Gaussian components. A key quantity in GMMs is the conditional probability of $ \\mathbf{z} $ given $ \\mathbf{x} $, denoted as $ p(z_k = 1 | \\mathbf{x}) $ or $ \\gamma(z_k) $. This is also known as the responsibility or assignment probability, which represents the probability that a given data point $ \\mathbf{x} $ belongs to component $ k $ of the mixture. Essentially, this can be thought of as the \\textbf{classification result} for $ \\mathbf{x} $. This responsibility is updated using Bayes’ Theorem, and can be expressed as: \\begin{align*} \\gamma(z_k) \\equiv p(z_k=1|\\mathbf{x}) \u0026 \\equiv \\frac{p(z_k=1)p(\\mathbf{x}|z_k=1)}{\\sum_{j=1}^{K}p(z_j=1)p(\\mathbf{x}|z_j=1)} \\\\ \u0026 = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\end{align*} In this expression, $ \\pi_k $ is the prior probability (or mixing coefficient) for component $ k $, and $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is the likelihood of the data point $ \\mathbf{x} $ under the Gaussian distribution corresponding to component $ k $. The denominator is a normalization factor that ensures the responsibilities sum to 1 across all components for a given data point. This framework allows for a soft classification of data points, where each point is associated with a probability of belonging to each cluster, rather than being strictly assigned to a single cluster as in $K$-means. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Maximum Likelihood Suppose we have a data set of observations $\\mathbf{X}=\\{ \\mathbf{x}_1,\\dots,\\mathbf{x}_n \\}^{T}\\in\\mathbb{R}^{N\\times D}$ and we want to model the data distribution $p(\\mathbf{X})$ using GMM. Assuming the data is independent and identically distributed (i.i.d.), the likelihood of the entire dataset can be expressed as: \\begin{align*} p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\prod_{n=1}^{N}\\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg). \\end{align*} To simplify the optimization process, we consider the log-likelihood function, which is given by: \\begin{align*} \\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \u0026= \\sum_{n=1}^{N}\\ln \\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg) \\end{align*} To solve the Maximum Likelihood Estimation (MLE) for Gaussian Mixture Models (GMMs), we typically consider the iterative Expectation-Maximization (EM) algorithm due to the non-convex nature of the problem. Before discussing how to maximize the likelihood, it is important to emphasize two significant issues that arise in GMMs: singularities and identifiability. Singularity A major challenge in applying the maximum likelihood framework to Gaussian Mixture Models is the presence of singularities. This problem arises because the likelihood function can become unbounded under certain conditions, leading to an ill-posed optimization problem. For simplicity, consider a Gaussian mixture model where each component has a covariance matrix of the form $ \\Sigma_k = \\sigma^2_k I $, where $ I $ is the identity matrix. Suppose one of the mixture components, say the $ j $-th component, has its mean $ \\boldsymbol{\\mu}_j $ exactly equal to one of the data points $ \\mathbf{x}_n $, so that $ \\boldsymbol{\\mu}_j = \\mathbf{x}_n $ for some value of $ n $. The contribution of this data point to the likelihood function would then be: \\begin{align*} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{x}_n, \\sigma^2_j I) = \\frac{1}{\\sqrt{2\\pi} \\sigma_j} \\cdot \\exp^0 \\end{align*} As $ \\sigma_j $ approaches 0, this term goes to infinity, causing the log-likelihood function to also diverge to infinity. Therefore, maximizing the log-likelihood function becomes an ill-posed problem because such singularities can always be present. These singularities occur whenever one of the Gaussian components collapses onto a specific data point, leading to a covariance matrix with a determinant approaching zero. This issue did not arise with a single Gaussian distribution because the variance cannot be zero by definition. Identifiability Another issue in finding MLE solutions for GMMs is related to identifiability. For any given maximum likelihood solution, a GMM with $ K $ components has a total of $ K! $ equivalent solutions. This arises from the fact that the $ K! $ different ways of permuting the $ K $ sets of parameters (means, covariances, and mixing coefficients) yield the same likelihood. In other words, for any point in the parameter space that represents a maximum likelihood solution, there are $ K! - 1 $ additional points that produce exactly the same probability distribution. This lack of identifiability means that the solution is not unique, complicating both the interpretation of the model and the optimization process. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:1","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Expectation Maximization for GMM The goal of the Expectation-Maximization (EM) algorithm is to find maximum likelihood solutions for models that involve latent variables. Suppose that directly optimizing the likelihood $p(\\mathbf{X}|\\boldsymbol{\\theta})$ is difficult. However, it is easier to optimize the complete-data likelihood function $p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})$ as as discussed in the previous sections. In such cases, we can use the EM algorithm. EM algorithm is a general technique for finding maximum likelihood solutions in latent variable models. Let’s begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. By setting the derivatives of $\\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with respect to the means $\\boldsymbol{\\mu}_k$ of the Gaussian components to zero, we obtain \\begin{align*} 0 = -\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}\\boldsymbol{\\Sigma}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k) \\end{align*} Multiplying by $\\boldsymbol{\\Sigma}_k^{-1}$ (which we assume to be non-singular) and rearranging we obtain \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\gamma(z_{nk})\\mathbf{x}_n, \\end{align*} where we have defined \\begin{align*} N_k = \\sum_{n=1}^{N}\\gamma(z_{nk}). \\end{align*} We can interpret $N_k$ as the effective number of points assigned to cluster $k$. We can obtain the MLE solutions for other variables similarly. References: Christoper M. Bishop, Pattern Recognition and Machine Learning, 2006 ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:2","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular value decomposition tutorial","date":"2024-08-15","objectID":"/20240815_svd/","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular Value Decomposition In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square matrix by extending the concept to asymmetric or rectangular matrices, which cannot be diagonalized directly using eigendecomposition. The SVD aims to find the following decomposition of a real-valued matrix $A$: $$A = U\\Sigma V^T,$$ where $U$ and $V$ are orthogonal (orthonormal) matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are called the left singular vectors of $A$, the columns of $V$ are called the right singular vectors, and the diagonal elements of $\\Sigma$ are called the singular values. Despite its widespread applications in areas such as data compression, noise reduction, and machine learning, SVD is often perceived as challenging to grasp. Many people find the mathematical intricacies daunting, even though there are numerous tutorials and explanations available. I remember struggling to fully understand SVD during my undergraduate studies, despite spending significant time on it. The complexity often arises from the abstract nature of the concepts involved, such as the interplay between eigenvectors, eigenvalues, and matrix decompositions. However, understanding SVD is crucial for many advanced techniques in data science and engineering. For instance, if the data matrix $A$ is close to being of low rank, it is often desirable to find a low-rank matrix that approximates the original data matrix well. As we will see, the singular value decomposition of $A$ provides the best low-rank approximation of $A$. The SVD process involves finding the eigenvalues and eigenvectors of the matrices $AA^T$ and $A^TA$. Since $A$ is generally not symmetric, it does not have orthogonal eigenvectors or guaranteed real eigenvalues, which complicates the process of SVD. However, the matrix $A^TA$ is guaranteed to be symmetric, as $$(A^TA)^T=A^TA,$$ and positive semi-definite, meaning all its eigenvalues are non-negative. Symmetric matrices have real eigenvalues and orthogonal eigenvectors, simplifying the decomposition process. These properties ensure that $A^TA$ can be diagonalized using an orthogonal matrix, which is crucial for deriving the SVD. The eigenvectors of $A^TA$ form the columns of $V$. We can diagonalize $A^TA$ as follows: $$A^TA = V\\Lambda V^T = \\sum_{i=1}^{n}\\lambda_i v_iv_i^T = \\sum_{i=1}^{n}\\sigma_i^2v_iv_i^T,$$ where the singular values of $A$ are defined as $\\sigma_i = \\sqrt{\\lambda_i}$. Since $A^TA$ is a symmetric matrix, its eigenvalues are non-negative. The matrix $\\Sigma$ in the SVD is a diagonal matrix whose diagonal entries are the singular values $\\sigma_1, \\dots, \\sigma_r$, where $r$ is the rank of $A$. Note that $rank(A) = rank(A^TA)$, and these singular values appear in the first $r$ positions on the diagonal of $\\Sigma$. For the $i$-th eigenvector-eigenvalue pair, we have $$A^TAv_i = \\sigma_i^2v_i.$$ Now, let’s derive the eigenvectors of $U$: \\begin{align*} A A^T (Av_i) \u0026= A (\\lambda_i v_i)\\\\ \u0026= \\lambda_i (A v_i). \\end{align*} Thus, $Av_i$ is an eigenvector of $AA^T$. However, to ensure that the matrix $U$ is orthonormal, we need to normalize these vectors as follows: \\begin{align*} u_i \u0026= \\frac{Av_i}{\\lVert Av_i\\rVert} \\\\ \u0026= \\frac{Av_i}{\\sqrt{(Av_i)^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^TA^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^T\\lambda_i v_i}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i \\underbrace{\\lVert v_i\\rVert}_{=1}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i}. \\end{align*} We can express $U$ as follows: $$U= \\left[\\frac{Av_1}{\\sigma_1}, \\dots, \\frac{Av_r}{\\sigma_r}, \\dots, \\frac{Av_n}{\\sigma_n}\\right].$$ Then, we have $$U\\Sigma = AV.$$ By rearranging, we get $$A = U\\Sigma V^{-1}.$$ Since the inverse of an orthogonal matrix $V$ is its transpose, $V^T$, the final form of the SVD is: $$A = U\\Sigma V^T.$$ ","date":"2024-08-15","objectID":"/20240815_svd/:0:0","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning"],"content":"Introduction to Regression: Recursive Least Squares (Part 3)","date":"2024-08-12","objectID":"/20240812_recursive_least_square/","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Deep Dive into Regression: Recursive Least Squares Explained (Part 3) ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:0:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Recursive Least Squares Ordinary least squares assumes that all data is available at once, but in practice, this isn’t always the case. Often, measurements are obtained sequentially, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where Recursive Least Squares (RLS) comes into play. RLS allows us to update our estimates efficiently as new measurements are obtained, without having to recompute everything from scratch. Suppose we have an estimate $\\boldsymbol{\\theta}_{k-1}$ after $k-1$ measurements and we receive a new measurement $\\mathbf{y}_k$. We want to update our estimate $\\boldsymbol{\\theta}_k$ using the following linear recursive model: \\begin{align*} \\mathbf{y}_k\u0026=\\mathbf{X}_k\\boldsymbol{\\theta} + \\boldsymbol{\\eta}_k\\\\ \\boldsymbol{\\theta}_k\u0026=\\boldsymbol{\\theta}_{k-1} + K_k (\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1}) \\end{align*} where $\\mathbf{X}_k$ is the observation matrix, $K_k$ is the gain matrix, and $\\boldsymbol{\\eta}_k$ represents the measurement error. The term $(\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is the correction term that adjusts our previous estimate using the new data. Also, $\\boldsymbol{\\eta}_k$ is the measurement error. The new estimate is modified from the previous estimate $\\boldsymbol{\\theta}_{k-1}$ with a correction via the gain matrix. To update the estimate optimally, we need to calculate the gain matrix $K_k$. This involves minimizing the variance of the estimation errors at time $k$. The error at step $k$ can be expressed as: \\begin{align*} \\boldsymbol{\\epsilon}_k \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k \\\\ \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1} - K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k (\\mathbf{X}_k\\boldsymbol{\\theta}+\\boldsymbol{\\eta}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k \\mathbf{X}_k(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1})-K_k\\boldsymbol{\\eta}_k\\\\ \u0026= (I-K_k \\mathbf{X}_k)\\boldsymbol{\\epsilon}_{k-1}-K_k\\boldsymbol{\\eta}_k, \\end{align*} where $I$ is the $d\\times d$ identity matrix. The mean of this error is then \\begin{align*} \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}] = (I-K_k \\mathbf{X}_k)\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]-K_k\\mathbb{E}[\\boldsymbol{\\eta}_{k}] \\end{align*} If $\\mathbb{E}[\\boldsymbol{\\eta}_{k}]=0$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]=0$, then $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\\boldsymbol{\\theta}$ is set equal to its expected value, then $\\boldsymbol{\\theta}_k=\\boldsymbol{\\theta}_k, \\forall k$. This property tells us that the estimator $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is unbiased. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\\boldsymbol{\\theta}$ on average. The key task is to find the optimal $K_k$ by minimizing the trace of the estimation-error covariance matrix $P_k = \\mathbb{E}[\\boldsymbol{\\epsilon}_k \\boldsymbol{\\epsilon}_k^T]$. This optimization leads to the following expression for ( K_k ): \\begin{align*} J_k \u0026= \\mathbb{E}[\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k\\rVert^2]\\\\ \u0026= \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}^T\\boldsymbol{\\epsilon}_{k}]\\\\ \u0026= \\mathbb{E}[tr(\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T)]\\\\ \u0026= tr(P_k), \\end{align*} where $P_k=\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T]$ is the estimation-error covariance (i.e., covariance matrix). Note that the third line holds by the trace of a product (i.e., cyclic property) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternative Formulations Sometimes alternate forms of the equations for $P_k$ and $K_k$ are useful for computational purposes. Let’s first set $\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k = S_k$, then we get $$K_k = P_{k-1}\\mathbf{X}_k^TS_k^{-1}.$$ By putting this into $P_k$, we can obtain \\begin{align*} P_k \u0026= (I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)P_{k-1}(I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)^T+P_{k-1}\\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\\mathbf{X}_kP_{k-1}\\\\ \u0026\\quad \\vdots\\\\ \u0026= P_{k-1}-P_{k-1}\\mathbf{X}_k^TS_k^{-1}\\mathbf{X}_k^TP_{k-1}\\\\ \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}. \\end{align*} Note that $P_k$ is symmetric (c.f., $P_k=\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$. Then, we take the inverse of both sides of $$P_{k-1}^{-1} = \\bigg(\\underbrace{P_{k-1}}_{A}-\\underbrace{P_{k-1}\\mathbf{X}_k^T}_{B}\\big(\\underbrace{\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T}_{D}\\big)^{-1}\\underbrace{\\mathbf{X}_kP_{k-1}}_{C}\\bigg)^{-1}.$$ Next, we apply the matrix inversion lemma which is known as Sherman-Morrison-Woodbury matrix identity (or matrix inversion lemma) identity: $$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$ Then, rewrite $P_k^{-1}$ as follows: \\begin{align*} P_k^{-1} \u0026= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\\mathbf{X}_k^T\\big((\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)-\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\\mathbf{X}_k^T)\\big)^{-1}\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\\\ \u0026= P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k \\end{align*} This yields an alternative expression for the covariance matrix: \\begin{align*} P_k = \\big(P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\big)^{-1} \\end{align*} We can also obtain \\begin{align*} K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1} \\end{align*} By \\begin{align*} P_k \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}\\\\ P_kP_{k-1}^{-1} \u0026= (I-K_k\\mathbf{X}_k)\\\\ P_kP_k^{-1} \u0026= P_kP_{k-1}^{-1}+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k=I\\\\ I \u0026= (I-K_k\\mathbf{X}_k)+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\\\ K_k \u0026= P_{k}\\mathbf{X}_k^TR_{k}^{-1}. \\end{align*} ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:1","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Summary of RLS To summarize, the RLS algorithm can be updated as follows: Gain Matrix Update: $K_k = P_{k-1}\\mathbf{X}_k^T(\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)^{-1}$ or $K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1}$ Estimate Update: $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ Covariance Matrix Update: $P_k = (I-K_k\\mathbf{X}_k)P_{k-1}$. $P_k = (I-K_k \\mathbf{X}_k)P_{k-1}(I-K_k \\mathbf{X}_k)^T+K_kR_kK_k^T,$ ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:2","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternate Derivation of RLS This chapter will be posted soon. Stay tuned for updates! References: Simon Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:2:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 2)","date":"2024-08-11","objectID":"/20240811_regression2/","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 2) ","date":"2024-08-11","objectID":"/20240811_regression2/:0:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model. Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data. To address these issues, regularization techniques are often used. Regularization involves adding a penalty term to the model’s objective function, which helps control the complexity of the model and prevents it from overfitting. ","date":"2024-08-11","objectID":"/20240811_regression2/:1:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined and Underdetermined Problems Recall that linear regression is fundamentally an optimization problem where we aim to find the optimal parameter vector $\\boldsymbol{\\theta}_{opt}$ that minimizes the residual sum of squares between the observed data and the model’s predictions. Mathematically, this is expressed as: $$\\boldsymbol{\\theta}_{opt} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d}\\lVert y-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2.$$ ","date":"2024-08-11","objectID":"/20240811_regression2/:2:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined Systems An optimization problem is termed overdetermined when the design matrix (or data matrix) $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}$ has more rows than columns, i.e., $m\u003ed$. This configuration means that there are more equations than unknowns, typically leading to a unique solution. In other words, there is more information available than the number of unknowns. The unique solution can be found using the formula: $$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$ The solution exists if and only if $\\mathbf{X}^T\\mathbf{X}$ is invertible, which is true when the columns of $\\mathbf{X}$ are linearly independent, meaning $\\mathbf{X}$ is full rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Underdetermined Systems In contrast, when $\\mathbf{X}$ is fat and short (i.e., $m\u003cd$), the problem is called underdetermined. In this scenario, there are more unknowns than equations, leading to infinitely many solutions. This occurs because the system has less information than the number of unknowns. Among these, the solution that minimizes the squared norm of the parameters is preferred. This solution is known as the minimum-norm least-squares solution. For an underdetermined linear regression problem, the objective can be written as: \\begin{align*} \\boldsymbol{\\theta} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d} \\lVert \\boldsymbol{\\theta}\\rVert^2, \\quad \\textrm{subject to}\\ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} Here, $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}, \\boldsymbol{\\theta}\\in \\mathbb{R}^d,$ and $\\mathbf{y}\\in \\mathbb{R}^m$. If the matrix has rank$(\\mathbf{X})=m$, then the linear regression problem will have a unique global minimum \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} This solution is called the minimum-norm least-squares solution. The proof of this solution is given by: \\begin{align*} \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = \\lVert\\boldsymbol{\\theta}\\rVert^2 + \\boldsymbol{\\lambda}^T(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}), \\end{align*} where $\\boldsymbol{\\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives with respec to $\\boldsymbol{\\lambda}$ and $\\boldsymbol{\\theta}$ and setting them to zero: \\begin{align*} \\nabla_{\\boldsymbol{\\theta}} \u0026= 2 \\boldsymbol{\\theta} + \\mathbf{X}^T\\boldsymbol{\\lambda} = 0\\\\ \\nabla_{\\boldsymbol{\\lambda}} \u0026= \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y} = 0 \\end{align*} The first equation gives us $\\boldsymbol{\\theta} = -\\mathbf{X}^T\\boldsymbol{\\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\\mathbf{X})=N$ so that $\\mathbf{X}^T\\mathbf{X}$ is invertible, we have $\\boldsymbol{\\lambda} = -2 (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}.$ Thus, we have \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} Note that $\\mathbf{X}\\mathbf{X}^T$ is often called a Gram matrix, $\\mathbf{G}$. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Regularization and Ridge Regression Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term that encourages the parameters to behave better, effectively controlling their magnitude. Ridge regression is a widely-used regularization technique that adds a penalty proportional to the square of the magnitude of the model parameters. The objective function for ridge regression is formulated as: \\begin{align*} J(\\boldsymbol{\\theta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2 \\end{align*} This can be expanded as: \\begin{align*} J(\\boldsymbol{\\theta}) = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) + \\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\end{align*} Breaking it down further: \\begin{align*} J(\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\lambda\\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta} \\end{align*} To minimize the objective function $J(\\boldsymbol{\\theta})$, we take the derivative with respect to $\\boldsymbol{\\theta}$ and set it to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + 2\\lambda\\mathbf{I}\\boldsymbol{\\theta} = 0 \\end{align*} Solving for $\\boldsymbol{\\theta}$, we obtain the ridge regression solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{align*} ","date":"2024-08-11","objectID":"/20240811_regression2/:3:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding the Role of $\\lambda$ When $\\lambda$ approaches 0, the regularization term $\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ becomes negligible, making ridge regression equivalent to ordinary least squares (OLS), which can lead to overfitting if the model is too complex. If $\\lambda\\to 0$, then $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\underbrace{\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2}_{=0}$ As $\\lambda$ increases towards infinity, the regularization term dominates, forcing $\\boldsymbol{\\theta}$ towards zero. In this case, the solution becomes overly simplistic, effectively shrinking the model parameters to zero, which may result in underfitting. $\\lambda\\to \\infty$, then $\\underbrace{\\frac{1}{\\lambda}\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2}_{=0} + \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ Since what we want to do is to minimize the objective function, we can divide it by $\\lambda$. Therefore, the solution will be $\\boldsymbol{\\theta}=0$, because it is the smallest value the squared function can achieve. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Spectral Decomposition and Invertibility It’s important to note that $\\mathbf{X}^T\\mathbf{X}$ is always symmetric. According to the Spectral theorem, this matrix can be decomposed as $\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$, where $\\mathbf{Q}$ is the eigenvector matrix, and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues. This allows us to express the inverse operation in ridge regression as: \\begin{align*} \\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{I}\\\\ \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{Q}\\mathbf{Q}^T\\\\ \u0026= \\mathbf{Q}(\\mathbf{\\Lambda}+\\lambda\\mathbf{I})\\mathbf{Q}^T. \\end{align*} Even if $\\mathbf{X}^T\\mathbf{X}$ is not invertible (or is close to being non-invertible), the regularization constant $\\lambda$ ensures invertibility by making the matrix full-rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Dual Form of Ridge Regression Ridge regression can also be expressed in its dual form, which is particularly useful for solving underdetermined problems: \\begin{align*} (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= \\mathbf{X}^T\\mathbf{y}\\\\ \\boldsymbol{\\theta} \u0026= \\lambda^{-1}\\mathbf{I}(\\mathbf{X}^T\\mathbf{y}-\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\lambda^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\alpha\\\\ \\lambda\\alpha \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\mathbf{X}^T\\alpha)\\\\ \\mathbf{y} \u0026= (\\mathbf{X}\\mathbf{X}^T\\alpha+\\lambda\\alpha)\\\\ \\alpha \u0026= (\\mathbf{X}\\mathbf{X}^T+\\lambda)^{-1}\\mathbf{y}\\\\ \\alpha \u0026= (\\mathbf{G}+\\lambda)^{-1}\\mathbf{y}. \\end{align*} Here, $\\alpha$ represents the dual coefficients, and $\\mathbf{G}$ is the Gram matrix. This formulation is especially powerful when dealing with high-dimensional data, where the number of features exceeds the number of samples. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:3","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Considering Varying Confidence in Measurements: Weighted Regression Up to this point, we have assumed that all measurements are equally reliable. However, in practice, the confidence in different measurements may vary. To account for this, we can consider the situation where the noise associated with each measurement has a zero mean and is independent across measurements. Under these conditions, the covariance matrix for the measurement noise can be expressed as follows: \\begin{align*} R \u0026= \\mathbb{E}(\\eta\\eta^T)\\\\ \u0026= \\begin{bmatrix} \\sigma_1^2 \u0026 \\dots \u0026 0\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ 0 \u0026 \\dots \u0026 \\sigma_l^2 \\end{bmatrix} \\end{align*} By denoting the error vector $\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}$ as $\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements: \\begin{align*} J(\\tilde{\\mathbf{x}}) \u0026= \\boldsymbol{\\epsilon}^TR^{-1}\\boldsymbol{\\epsilon}=\\frac{\\boldsymbol{\\epsilon}_1^2}{\\sigma_1^2}+\\dots+\\frac{\\boldsymbol{\\epsilon}_l^2}{\\sigma_l^2}\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^TR^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) \\end{align*} Note that by dividing each residual by its variance, we effectively equalize the influence of each data point on the overall fitting process. Subsequently, by taking the partial derivative of $J$ with respect to $\\boldsymbol{\\theta}$, we get the best estimate of the parameter, which is given by $$\\boldsymbol{\\theta} = (\\mathbf{X}^TR^{-1}\\mathbf{X})^{-1}\\mathbf{X}^TR^{-1}\\mathbf{y}.$$ Note that the measurement noise matrix $R$ must be non-singular for a solution to exist. To learn more, please take a look at this note ! This article continues in Part 3. References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-11","objectID":"/20240811_regression2/:4:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 1)","date":"2024-08-10","objectID":"/20240810_regression1/","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 1) Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That’s why I’ve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction. ","date":"2024-08-10","objectID":"/20240810_regression1/:0:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Linear Regression Regression is a method used to identify the relationship between input and output variables. In a regression problem, we are given a set of noisy measurements (or output data) $\\mathbf{y} = [y_1, \\dots, y_d]^T$, which are affected by measurement noise $\\boldsymbol{\\eta} = [\\eta_1, \\dots, \\eta_d]^T$. The corresponding input data is denoted by $\\mathbf{x} = [x_1, \\dots, x_d]$. We refer to the collection of these input-output pairs as the training data, $\\mathcal{D} = {(\\mathbf{x}_1, \\mathbf{y}_1), \\dots, (\\mathbf{x}_m, \\mathbf{y}_m)}$. The true relationship between the input and output data is unknown and is represented by a function $f(\\cdot)$ that maps $\\mathbf{x}_n$ to $y_n$, i.e., $$ \\mathbf{y} = f(\\mathbf{x}). $$ Determining the exact function $f(\\cdot)$ from a finite set of data points $\\mathcal{D}$ is not feasible because there are infinitely many possible mappings for each $\\mathbf{x}_i$. The idea of regression is to introduce structure to the problem. Instead of trying to find the true $f(\\cdot)$, we seek an approximate model $g_\\theta(\\cdot)$, which is parameterized by $\\boldsymbol{\\theta} = [\\theta_1,\\dots,\\theta_d]^T$. For example, we might assume a linear relationship between $(\\mathbf{x}_n, \\mathbf{y}_n)$: \\begin{align*} g_{\\boldsymbol{\\theta}}(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}, \\end{align*} where $\\mathbf{X}$ is an $m \\times d$ input matrix derived from our observations. Since the true relationship is unknown, any chosen model is essentially a hypothesis. However, we can quantify the error in our model. Given a parameter $\\boldsymbol{\\theta}$, the error between the noisy measurements and the estimated values is: \\begin{align*} \\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} The goal of regression is to find the best $\\boldsymbol{\\theta}$ that minimizes this error. This leads us to the following objective function: \\begin{align*} J(\\boldsymbol{\\theta}) = \\boldsymbol{\\epsilon}^T \\boldsymbol{\\epsilon}. \\end{align*} This objective function is equivalent to minimizing the mean squared error (MSE): \\begin{align*} MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{x}_i \\boldsymbol{\\theta})^2. \\end{align*} We can optimize this function in closed form as follows: \\begin{align*} J(\\boldsymbol{\\theta}) \u0026= \\lVert\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}\\rVert^2_2 \\\\ \u0026= (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) \\\\ \u0026= \\mathbf{y}^T \\mathbf{y} - \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}. \\end{align*} To find the $\\boldsymbol{\\theta}$ that minimizes the objective function, we compute the derivative of the function and set it equal to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} = 0, \\end{align*} which simplifies to: \\begin{align*} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0, \\end{align*} leading to the solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}. \\end{align*} The equation $\\mathbf{X}^T(\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0$ is known as the normal equation. ","date":"2024-08-10","objectID":"/20240810_regression1/:1:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Python Code Let’s implement a simple regression in Python: import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.randn(N) w_1 = 3.4 # True Parameter w_0 = 0.9 # True Parameter y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) W = np.array([w_1, w_0]) # From Scratch XtX = np.dot(X.T, X) XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m W_best = np.dot(XtXinvX, y.T) print(f\"W_best: {W_best}\") # Pythonic Approach theta = np.linalg.lstsq(X, y, rcond=None)[0] print(f\"Theta: {theta}\") t = np.linspace(0, 1, 200) y_pred = W_best[0]*t+W_best[1] yhat = theta[0]*t+theta[1] plt.plot(x, y, 'o') plt.plot(t, y_pred, 'r', linewidth=4) plt.show() To learn more, please take a look at this note ! This article continues in Part 2 . References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 ","date":"2024-08-10","objectID":"/20240810_regression1/:1:1","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["gpg"],"content":"Guide to encrypt/decrypt data using GPG","date":"2024-08-04","objectID":"/20240804_gpg_encryption/","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Securing Your Privacy The importance of securing your data has become critical in the modern digital era. This post explores a versatile tool called GnuPG, or GNU Privacy Guard, which allows you to encrypt your data and communications, ensuring that only the intended recipients can access them. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:0:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Asymmetric Encryption Before looking at GPG, let’s first review some encryption approaches. A very naive approach to sharing encrypted files is to use the same secret key between a sender and a receiver. This approach is known as symmetric encryption. However, the symmetric approach has a limitation in that it requires a secure method for key exchange. To address this issue, asymmetric encryption is preferred. Asymmetric encryption, also known as public-key cryptography, is a method of encryption that uses a pair of keys: a public key and a private key, to encrypt and decrypt data. The public key is used for encryption. It is openly shared and can be distributed to anyone, allowing anyone to encrypt a message intended for the key owner. The private key is used for decryption. It is kept secret and known only to the owner, allowing the key owner to decrypt messages that were encrypted with the corresponding public key. This is how asymmetric encryption works: Key Pair Generation: A user generates a pair of keys: one public and one private. The public key is shared widely, while the private key is kept secure. Encryption: When someone wants to send a secure message, they use the recipient’s public key to encrypt the message. This ensures that only the recipient, who has the corresponding private key, can decrypt and read the message. Decryption: The recipient uses their private key to decrypt the received message. The private key is the only key that can decrypt the message encrypted with the corresponding public key. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:1:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"How to Use GPG? Installation: sudo apt-get install gnupg # Ubuntu sudo pacman -S gnupg # Arch Generate a GPG Key: gpg --full-gen-key Export Public Key: gpg --export --armor your-email@example.com \u003e publickey.asc Import Public Key: gpg --import publickey.asc Encrypt a File: gpg --output encryptedfile.gpg --encrypt --recipient recipient@example.com file.txt Decrypt a File: gpg --output file.txt --decrypt encryptedfile.gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:2:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Singinig/Verifying Files A detached signature is a separate file that contains the signature of the original file. gpg --output file.sig --detach-sign file.txt file.txt is the file you want to sign. file.sig is the signature file generated. To verify the detached signature: gpg --verify file.sig file.txt To sign text files, gpg --clearsign \u003cfile name\u003e This will create a file called file.txt.asc which contains the original text and the signature. To verify gpg --verify \u003cfile.asc\u003e ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:3:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"A Simple GitHub Verification using ssh with gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Generate SSH key ssh-keygen -t rsa or simply ssh-keygen id_rsa : private key id_rsa.pub : public key Go to SSH and GPG keys in the setting menu of GitHub Just paste your public key ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:1","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["Tools"],"content":"Guide to manage your tasks using TaskSpooler","date":"2024-07-13","objectID":"/20240713_taskspooler/","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"What is TaskSpooler ? TaskSpooler (ts) is a lightweight job scheduler that allows you to queue up your tasks and execute them in order. It’s particularly useful for environments where tasks need to be managed sequentially or with a controlled degree of parallelism. Unlike more complex systems like SLURM, TaskSpooler is designed for simplicity and ease of use, making it accessible for individual researchers and small teams. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:0:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Efficient Job Scheduling for ML/DL Researchers with Taskspooler In the dynamic field of Machine Learning (ML) and Deep Learning (DL), managing and optimizing computational resources is crucial. For researchers frequently running numerous experiments, an efficient job scheduler can be a game-changer. Enter Taskspooler, a powerful yet user-friendly job scheduler for Linux, designed to help you manage and schedule your jobs in a queue. Taskspooler is a simpler alternative to SLURM, providing many benefits for ML/DL researchers, especially when it comes to utilizing GPUs efficiently. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"TL;DR Job Queuing: Easily queue up multiple jobs, specifying the order of execution. Resource Management: Find and allocate empty GPUs to your tasks, maximizing resource utilization. Monitoring: Track the status of your jobs in real-time. Simplicity: A straightforward command-line interface that requires minimal setup and configuration. Parallel Execution: Control the number of jobs running simultaneously, which is essential for managing GPU workloads effectively. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Dive into TaskSpooler ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Installation First, clone the repository: git clone https://github.com/justanhduc/task-spooler Install GPU Version To set up Task Spooler with GPU support, run the provided script: ./install_make Alternatively, to use CMake: ./install_cmake If Task Spooler is already installed, and you want to reinstall or upgrade, run: ./reinstall Install CPU Version If you would like to install only the CPU version, use the following commands (recommended): make cpu sudo make install or via CMake: mkdir build \u0026\u0026 cd build cmake .. -DTASK_SPOOLER_COMPILE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release make sudo make install ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Basic Usage Let’s first put your task into a queue: ts python main.py This command queues up your python main.py. Keeping track of running and pending jobs is crucial for optimizing your workflow. Taskspooler provides real-time updates on job status, allowing you to make informed decisions and adjustments on the fly. To check the overall queue status, simply type: ts This returns your jobs with the job ID, state, time, and the command you typed. To track the status of your jobs: ts -c \u003cjob-id\u003e You can delete finished jobs in the job list by: ts -C To set the size of your job queue (i.e., to limit/expand the number of parallel running processes): ts -S \u003cqueue-size\u003e ","date":"2024-07-13","objectID":"/20240713_taskspooler/:3:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"GPU Utilization For ML/DL researchers, GPUs are invaluable but often limited resources. Taskspooler helps you find available GPUs and assign tasks to them efficiently. This ensures that your experiments run smoothly without unnecessary delays due to resource contention. To specify GPU indices for a job without checking whether they are free, use: ts -g [id,...] python main.py For instance: ts -g 1 python main.py This allows you to run your model on GPU 1. To get the GPU usage: ts -g ","date":"2024-07-13","objectID":"/20240713_taskspooler/:4:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Conclusion Taskspooler offers a simple yet powerful solution for job scheduling and resource management, making it an excellent tool for ML/DL researchers. By effectively queuing your tasks and optimizing GPU usage, you can streamline your workflow and focus more on the research itself rather than managing computational resources. Whether you’re working on a single machine or a small cluster, Taskspooler can significantly enhance your productivity and efficiency. If you want to know more visit its official github repo Happy experimenting! ","date":"2024-07-13","objectID":"/20240713_taskspooler/:5:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Python"],"content":"Guide to keep manage dependencies in Python","date":"2024-07-07","objectID":"/20240707_poetry/","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Introduction Poetry is a dependency management and packaging tool in Python, aiming to improve how you define, install, and manage project dependencies. Installation: You can install Poetry through its custom installer script or using package managers. The recommended way is to use their installer script to ensure you get the latest version. Creating a New Project: Use poetry new \u003cproject-name\u003e to create a new project with a standard layout. Adding Dependencies: Add new dependencies directly to your project using poetry add \u003cpackage\u003e. Poetry will resolve the dependencies and update the pyproject.toml and poetry.lock files. Installing Dependencies: Running poetry install in your project directory will install all dependencies defined in your pyproject.toml file. ","date":"2024-07-07","objectID":"/20240707_poetry/:0:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Poetry Example ","date":"2024-07-07","objectID":"/20240707_poetry/:1:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Setting Up a New Project To create a new project named example_project with Poetry and manage its dependencies: poetry new example_project This command creates a new directory named example_project with some initial files, including a pyproject.toml file for configuration. The pyproject.toml file is what is the most important here. This will orchestrate your project and its dependencies. For now, it looks like this: [tool.poetry] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [\"author \u003cauthor@xxxxx.xxx\u003e\"] readme = \"README.md\" packages = [{include = \"poetry_demo\"}] [tool.poetry.dependencies] python = \"^3.7\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" we are allowing any version of Python 3 that is greater than 3.7.0. If you want to use Poetry only for dependency management but not for packaging, you can use the non-package modei: [tool.poetry] package-mode = false ","date":"2024-07-07","objectID":"/20240707_poetry/:1:1","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Add and Install Dependencies Suppose your project depends on requests for making HTTP requests and pytest for testing. To add these: poetry add requests poetry add pytest --dev The --dev flag indicates that pytest is a development dependency, not required for production. To remove a package, poetry remove \u003cpackage\u003e Running the command below will install all dependencies listed in your pyproject.toml: poetry install This also creates a virtual environment for your project if it doesn’t already exist. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:2","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Run Your Project Run directly To run a script or start your project within the Poetry-managed virtual environment: poetry run python my_script.py This command ensures that python and any other commands are run within the context of your project’s virtual environment, using the correct versions of Python and all dependencies. Entry Point In Poetry, an entry point is a way to specify which Python script should be executed when your package is run. This is particularly useful for creating command-line applications or defining executable scripts that can be run directly from the command line after your package is installed. To define an entry point in a Poetry project, you use the [tool.poetry.scripts] section in your pyproject.toml file. This section allows you to map a command name to a Python function, which will be executed when the command is run. Edit your pyproject.toml file to include the [tool.poetry.scripts] section. This is where you define your entry point. Add the following lines to the pyproject.toml file: [tool.poetry.scripts] greet-app = \"greet_app.cli:main\" Here, greet-app is the command name, and greet_app.cli:main specifies the main function in the cli.py module inside the greet_app package. Shell Using Virtual Environment Shell: If you frequently run commands, you might find it convenient to activate the Poetry-managed virtual environment shell: poetry shell This will activate the virtual environment, and you can run your command without prefixing it with poetry run: greet-app To exit this new shell type exit. To deactivate the virtual environment without leaving the shell use deactivate. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:3","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Init Method Instead of creating a new project, Poetry can be used to ‘initialise’ a pre-populated directory. To interactively create a pyproject.toml file in directory pre-existing-project: poetry init Then, install it by poetry install To see the information about the project poetry env info -p: prints a path of the virtual environment By setting a config, you can generate a virtual environment in your projects: poetry config virtualenvs.in-project true ","date":"2024-07-07","objectID":"/20240707_poetry/:1:4","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Use with Git Poetry automatically creates a .gitignore file for you. It should include entries to ignore the virtual environment directory (.venv if you use Poetry’s built-in virtual environment management). Whenever you add or update dependencies, make sure to commit the pyproject.toml and poetry.lock files: To clone your project: git clone \u003cyour-repository-url\u003e cd my-project poetry install ","date":"2024-07-07","objectID":"/20240707_poetry/:2:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Linux"],"content":"Pacman tutorial","date":"2024-05-01","objectID":"/20240501_pacman/","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Pacman , the package manager for Arch Linux, is known for its simple binary package format and easy-to-use build system . The primary aim of Pacman is to facilitate straightforward management of packages from both the official repositories and user-generated builds. Pacman ensures your system remains updated by synchronizing the package lists with the master server. This client/server model simplifies the process of downloading and installing packages, along with all their dependencies, using basic commands. ","date":"2024-05-01","objectID":"/20240501_pacman/:0:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Installing and Upgrading Packages Install a Package: sudo pacman -S \u003cpackage-name\u003e Full System Upgrade: sudo pacman -Syu -y synchronizes the database, similar to sudo apt-get update. -u upgrades all out-of-date packages, akin to sudo apt-get upgrade. Installing Packages from Git: Clone the package and install: git clone \u003crepository-url\u003e makepkg -si ","date":"2024-05-01","objectID":"/20240501_pacman/:0:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Removing Packages Remove a Specific Package: sudo pacman -R \u003cpackage-name\u003e Using -s removes dependencies not required by other packages, but be cautious as it may affect dependencies needed by other programs. Best Practice for Removing Packages: sudo pacman -Rns \u003cpackage-name\u003e Remove Obsolete Packages: sudo pacman -Sc ","date":"2024-05-01","objectID":"/20240501_pacman/:0:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Package Lists List All Installed Packages: sudo pacman -Q List Manually Installed Packages: sudo pacman -Qe List Installed AUR Packages: sudo pacman -Qm List Unneeded Dependencies: sudo pacman -Qdt ","date":"2024-05-01","objectID":"/20240501_pacman/:0:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Getting Started with Pacman Creating a List of Installed Packages: Generate a list to easily reinstall packages on a new system: pacman -Qqen \u003e pkglist.txt To reinstall packages from the list: pacman -S - \u003c pkglist.txt ","date":"2024-05-01","objectID":"/20240501_pacman/:1:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Rollback to Previous Versions List Cached Packages: ls /var/cache/pacman/pkg/ To downgrade a package: sudo pacman -U \u003cpackage-file\u003e ","date":"2024-05-01","objectID":"/20240501_pacman/:1:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Mirror Lists Edit and Refresh Mirror List: sudo vim /etc/pacman.d/mirrorlist sudo pacman -Syy Use -yy to force a refresh of the package databases, even if they are up to date. ","date":"2024-05-01","objectID":"/20240501_pacman/:1:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Configuration Tips Enable Parallel Downloads: Open your configuration file: sudo vim /etc/pacman.conf Then uncomment or add the following line to enable multiple simultaneous downloads: ParallelDownloads=5 Ignore Specific Packages: Add the following line to /etc/pacman.conf to prevent specific packages from being updated: IgnorePkg = postgresql* ","date":"2024-05-01","objectID":"/20240501_pacman/:1:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Vim"],"content":"The Appeal of Vim in Modern Programming","date":"2024-05-01","objectID":"/20240501_vim/","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Vim"],"content":"While it may look somewhat obsolete in an era dominated by graphically rich IDEs, Vim remains not just a highly relevant and effective tool for today’s programmers but also a badge of coolness in the tech world. Those who master its commands are often seen as coding wizards. With its unique advantages in speed, efficiency, and customizability, Vim is an invaluable asset in software development environments, proving that old-school can still be trendy. Vim is celebrated for its minimalistic approach, using fewer system resources, which facilitates fast and responsive editing. This efficiency is further enhanced by Vim’s command-driven interface, allowing developers to perform complex edits quickly through simple keystrokes. This minimizes the need for mouse use, thereby reducing the risk of repetitive strain injuries on the wrists. Here’s an example of how to yank (copy) and paste in Vim: To yank (copy) a line in Vim, just press yy. To paste the yanked text, simply move the cursor to where you want to insert the copied text and press p to paste after the cursor position or P to paste before it. The universal Ctrl+C to copy and Ctrl+V to paste are straightforward and familiar to most users. However, this often requires alternating between the keyboard and mouse, which can slow down the editing process and increase physical strain on the wrists. Vim’s approach is designed to keep your fingers on the keyboard, reducing the need to switch to a mouse and enhancing focus and productivity. This is especially beneficial in coding and scripting environments where rapid navigation and changes are common. Beyond its capabilities as a programming editor, Vim excels as a general-purpose text editor. It’s an excellent tool for note-taking and managing documentation, thanks to its lightweight nature and fast operation. Moreover, Vim is highly effective for composing complex documents in LaTeX (See my machine learning study notes !), allowing users to edit large amounts of text with ease and precision. The ability to customize Vim with plugins and scripts also extends its functionality into areas like PDF viewing and reference management. Customizability is another cornerstone of Vim’s design. Developers can adjust nearly every aspect of the editor to fit their workflow, from key bindings to complex integrations. Vim’s adaptability extends to broader system configurations, seamlessly integrating with tools like the i3 window manager (i3wm) and file browsers like LF and Ranger. This integration allows for a more unified and efficient desktop environment, where everything from file management to window resizing is streamlined through Vim-like commands. Mastering Vim not only boosts your coding efficiency but also earns you some cool points among tech peers who appreciate the power and elegance of classic tools. I will post more Vim tips soon! ","date":"2024-05-01","objectID":"/20240501_vim/:0:0","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Python"],"content":"Why Use Python's `pdb` Debugger Over an IDE?","date":"2024-04-27","objectID":"/20240426_pdb/","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"When it comes to debugging Python code, most programmers reach for an Integrated Development Environment (IDE) because of its convenience and powerful features. However, there’s a classic, built-in tool that shouldn’t be overlooked: Python’s own debugger, pdb. This tool might seem basic at first glance, but it offers some compelling advantages, especially in scenarios where an IDE might be less effective. Here’s why you might consider using pdb for debugging your Python projects: ","date":"2024-04-27","objectID":"/20240426_pdb/:0:0","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Simplicity pdb comes as part of Python’s standard library, which means it’s ready to use out of the box—no installation or complex setup required. If you’re working on a simple script or need a quick debugging session, pdb is just a few keystrokes away. pdb offers an interactive session that lets you control the flow of your program. You can step through your code line by line, inspect and modify variables, and execute Python commands on the fly. This hands-on control can make finding and fixing bugs much clearer and sometimes even faster. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:1","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Environment Independence One of pdb’s greatest strengths is its versatility. Whether you’re coding on a local machine, a remote server, or even in a container, pdb works just the same. This universal compatibility is a huge plus, particularly when dealing with production environments where installing a full-fledged IDE isn’t feasible. Also, pdb operates entirely in the terminal, it’s perfect for low-resource environments or situations where a graphical interface might slow you down. This makes pdb incredibly efficient and responsive, even over network connections like SSH. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:2","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Flexibility in Use You can start pdb in several ways: directly from the command line, by inserting a breakpoint in your code, or as a module. This flexibility allows you to adapt your debugging approach to the needs of each specific project or problem. For developers who prefer working in text editors like Vim or Emacs, pdb integrates smoothly, enabling powerful debugging without leaving your editor. This integration supports a streamlined workflow, particularly for those who favor a more textual or minimalist development environment. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:3","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Conclusion While modern IDEs are undeniably powerful and user-friendly, pdb holds its own with features that are particularly suited to debugging in a variety of environments and situations. It’s a tool that encourages mastery of debugging by getting you close to the code in a way that GUI tools sometimes can’t match. Whether you’re a beginner looking to understand the inner workings of Python or an experienced developer needing a reliable tool on a remote server, pdb is worth exploring. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:4","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"A tutorial for Pydantic","date":"2024-04-26","objectID":"/20240426_pydantic/","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Python’s dynamic typing system is indeed convenient, allowing you to create variables without explicitly declaring their types. While this flexibility can streamline development, it can also introduce unexpected behavior, particularly when handling data from external sources like APIs or user input. Consider the following scenario: employee = Employee(\"Han\", 30) # Correct employee = Employee(\"Moon\", \"30\") # Correct Here, the second argument is intended to represent an age, typically an integer. However, in the second example, it’s a string, potentially leading to errors or unexpected behavior down the line. To address such issues, Pydantic offers a solution through data validation. Pydantic is a library specifically designed for this purpose, ensuring that the data conforms to pre-defined schemas. The primary method of defining schemas in Pydantic is through models. Models are essentially classes that inherit from pydantic.BaseModel and define fields with annotated attributes. You can think of models as similar to structs in languages like C. While Pydantic models share similarities with Python’s dataclasses, they are preferred when data validation is essential. Pydantic models guarantee that the fields of an instance will adhere to specified types, providing both runtime validation and serving as type hints during development. Let’s illustrate this with a simple example: from pydantic import BaseModel class User(BaseModel): id: int name: str = \"John Doe\" User model has two fields: id integer and name string, which has a default value. You can create an instance, user = User(id=\"123\") You can also define models that include other models, allowing for complex data structures: from typing import List class Item(BaseModel): name: str price: float class Order(BaseModel): items: List[Item] total_price: float order = Order(items=[{\"name\": \"Burger\", \"price\": 5.99}, {\"name\": \"Fries\", \"price\": 2.99}], total_price=8.98) print(order) ","date":"2024-04-26","objectID":"/20240426_pydantic/:0:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Validators Pydantic provides a versatile decorator called validator, which enables you to impose custom validation rules on model fields. These validators extend beyond simple type validation and allow you to enforce additional checks. Here’s how you can define and utilize a custom validator: from pydantic import BaseModel, validator class Person(BaseModel): name: str age: int @validator('age') def check_age(cls, value): if value \u003c 18: raise ValueError('Age must be at least 18') return value # This will raise an error because the age is below 18 try: Person(name=\"Charlie\", age=17) except Exception as e: print(e) In this example, the custom validator ensures that the age provided is at least 18 years. Custom validators can target individual fields, multiple fields, or the entire model, making them invaluable for enforcing complex validation logic or cross-field constraints. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Built-in Validators Pydantic models leverage Python type annotations to enforce data types. Alongside the fundamental types like str, int, float, bool, Pydantic supports complex data types such as List, Dict, Union, and Optional, among others. These annotations are the first level of validation: from pydantic import BaseModel from typing import List, Optional class User(BaseModel): name: str age: int tags: Optional[List[str]] = None In this example, name must be a string, age an integer, and tags is an optional list of strings. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Field Validation For more detailed validation, Pydantic’s Field function can be used to specify additional constraints: from pydantic import BaseModel, Field class User(BaseModel): id: int name: str email: str = Field(..., description=\"The email address of the user\") age: int = Field(..., gt=0, description=\"The age of the user\") # Usage user_data = {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\", \"age\": 30} user = User(**user_data) In this example: id, name, email, and age represents fields in the User model. id and name are required fields because they don’t have a default value. email and age have default values specified using the Field class. For email, ... indicates that it’s required, and a description is provided. For age, ... indicates that it’s required, and it must be greater than zero (gt=0). By using Field, you can define additional constraints such as minimum and maximum values, regular expressions for string fields, custom validation functions, etc., to ensure that your data meets specific criteria. age: int = Field(..., gt=0, description=\"The age of the user\") For instance, you can specifies that the age must be greater than 0. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:2","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Root Validators For validation that involves multiple fields, you can use root validators. These are applied to the whole model instead of individual fields: from pydantic import BaseModel, root_validator class Account(BaseModel): username: str password1: str password2: str @root_validator def passwords_match(cls, values): password1, password2 = values.get('password1'), values.get('password2') if password1 and password2 and password1 != password2: raise ValueError('Passwords do not match') return values Root validators have access to all field values of the model, making them ideal for validations that depend on multiple fields. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:3","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pre-Validators and Post-Validators Pre-validators: A pre-validator in Pydantic is used to preprocess or transform the data before it undergoes the main validation process. This is particularly useful when you need to adjust or prepare the incoming data so it can be successfully validated. For instance, you might want to strip whitespace from a string, convert data types, or decompose compound fields into simpler components before validation. from pydantic import BaseModel, validator class TrimmedStringModel(BaseModel): text: str @validator('text', pre=True) def strip_whitespace(cls, value): return value.strip() Post-validator Post-validators are used to validate or transform data after the main validation process. They are useful when certain validations depend on multiple fields or when you need to enforce complex constraints that are not covered by basic type annotations. Post-validators are also defined using the @validator decorator but without specifying pre=True. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:4","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Json Serialization It is really simple to convert Pydantic models to or from JSON. For example, user_json = user.json() You can convert your model instance to JSON file as above. Or you can make a dictionary by user.dict() Conversely, json_str = '{\"name\": \"Han\", \"account\": 1234}' User.parse_raw(json_str) ","date":"2024-04-26","objectID":"/20240426_pydantic/:2:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic for Config Pydantic can also be used for settings management by loading configuration from environment variables: from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: str database_password: str my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) This feature is particularly useful for 12-factor apps that require configuration through the environment for different deployment environments. Pydantic provides a powerful system for data validation, allowing you to enforce type constraints and custom validation rules on your data models. This capability ensures that the data your application works with is correct and consistent, reducing runtime errors and simplifying data handling. Let’s explore more about validation in Pydantic, including built-in validators and how to write custom validation functions. ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic SecretStr Pydantic’s SecretStr is a special data type designed to handle sensitive information, such as passwords or secret tokens, in a more secure manner. This type is part of Pydantic’s data types that provide tools for sensitive data, ensuring that such information isn’t accidentally printed or logged, which could lead to security vulnerabilities. from pydantic import BaseModel, SecretStr class User(BaseModel): username: str password: SecretStr # Usage user_data = {\"username\": \"john_doe\", \"password\": \"secretpassword\"} user = User(**user_data) print(user) # Output: User username='john_doe' password=SecretStr('********') from pydantic import BaseModel, SecretBytes class EncryptedData(BaseModel): data: SecretBytes # Usage encrypted_data = {\"data\": b\"encrypted binary data\"} data_object = EncryptedData(**encrypted_data) print(data_object) # Output: EncryptedData data=SecretBytes('********') from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: SecretStr database_password: SecretStr my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"A tutorial for Enum","date":"2024-04-26","objectID":"/20240426_enum/","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Enum is a way that Python enumerate variables. The enum module allows for the creation of enumerated constants—unique, immutable data types that are useful for representing a fixed set of values. These values, which are usually related by their context, are known as enumeration members. Enum provides… Uniqueness - Each member of an Enum is unique within its definition, meaning no two members can have the same value. Attempting to define two members with the same value will result in an error unless you explicitly allow aliases. Immutability - Enum members are immutable. Once the Enum class is defined, you cannot change the members or their values. Iterability and Comparability - Enum classes support iteration over their members and can be compared using identity and equality checks. Accessing Members - You can access enumeration members by their names or values: Auto - If you want to automatically assign values to enum members, you can use the auto() function from the same module: from enum import Enum class State(Enum): PLAYING=0 PAUSED=1 GAME_OVER=2 If we just want to make sure them to be unique and automatically assigned, then use auto() from enum import Enum, auto class State(Enum): PLAYING=auto() PAUSED=auto() GAME_OVER=auto() print(State.PLAYING) print(State.PLAYING.value) Or simply, from enum import Enum, auto class State(Enum): PLAYING, PAUSED, GAME_OVER=range(3) print(State.PLAYING) print(State.PLAYING.value) However, this hard codes numbers, which can create an issue in the future. ","date":"2024-04-26","objectID":"/20240426_enum/:0:0","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Iterating over Enum Members You can iterate over the members of an enum: for state in State: print(state) ","date":"2024-04-26","objectID":"/20240426_enum/:0:1","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Comparison of Enum Members Enum members are singleton objects, so comparison is possible by identity: if State.PLAYING is State.PLAYING: print(\"RED is indeed RED\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:2","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Using Enum as a Type Hint Enums can be used as type hints, enhancing code readability and correctness: def paint(color: Color): print(f\"Painting with {color.name}\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:3","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Extending Enums: IntEnum and StrEnum For enums where the members are specifically integers or strings, you can inherit from IntEnum or StrEnum for additional benefits, like being able to compare members to integers or strings directly. from enum import IntEnum class Priority(IntEnum): LOW = 1 MEDIUM = 2 HIGH = 3 # Direct comparison with integers if Priority.LOW \u003c Priority.HIGH: print(\"Low priority is less than high\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:4","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Unique Constraint To ensure that all enum values are unique, you can use the @unique decorator: from enum import Enum, unique @unique class StatusCode(Enum): OK = 200 NOT_FOUND = 404 ERROR = 500 Using @unique will raise a ValueError if any duplicate values are detected. ","date":"2024-04-26","objectID":"/20240426_enum/:0:5","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Conclusion Enums in Python are useful for defining sets of named constants that are related and have a fixed set of members. They improve code readability, prevent errors related to using incorrect literal values, and can simplify type checking and validation in your programs. ","date":"2024-04-26","objectID":"/20240426_enum/:0:6","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"A tutorial for Pytest","date":"2024-04-26","objectID":"/20240426_unit-tests/","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Unit testing involves testing individual components of software in isolation to ensure they function correctly. Automated frameworks facilitate this process, which is integral to ensuring that new changes do not disrupt existing functionality. Unit tests also serve as practical documentation and encourage better software design. This testing method boosts development speed and confidence by confirming component reliability before integration. Early bug detection through unit testing also helps minimize future repair costs and efforts. pytest Pytest is one of the best tools that you can use to boost your testing productivity for Python codes. ","date":"2024-04-26","objectID":"/20240426_unit-tests/:0:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Install pip install pytest pip install pytest-cov pytest --cov: this returns a coverage of test functions coverage html: log test results in html format ","date":"2024-04-26","objectID":"/20240426_unit-tests/:1:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Example pytest is a libarary for testing. You can run your unit test code by pytest test_function.py If you wanna create a directory with several testing files, then just create __init__.py and put it inside the test dir. Then, just run pytest test_dir from calc import square import pytest def square(n): return n*n # This is a convention test_{func} def test_negative(): assert square(-2)==4 assert square(-3)==9 def test_positive(): assert square(2)==4 assert square(3)==9 def test_zero(): assert square(0)==0 def test_str(): # Write down what I expect to get # If it successfully raised the error that I expected # Then, it will pass the test with pytest.raises(TypeError): square(\"cat\") def main(): x = int(input(\"What's x? \")) print(\"x squared is\", square(x)) if __name__==\"__main__\": main() If you want to intentially raise an error, then you can do it by pytest.raises(\"SomeErrorType\") Note that you can write a warning message like assert x == \u003ccond\u003e, \u003cMSG\u003e ","date":"2024-04-26","objectID":"/20240426_unit-tests/:2:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Programming"],"content":"A gentle introduction to bash scripting","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Let’s create our first simple shell script #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Unix that the file is to be executed by /bin/sh. This is the standard location of the Bourne shell on just about every Unix system. If you’re using GNU/Linux, /bin/sh is normally a symbolic link to bash (or, more recently, dash). The second line begins with a special symbol: #. This marks the line as a comment, and it is ignored completely by the shell. The only exception is when the very first line of the file starts with #! (shebang) - as ours does. This is a special directive which Unix treats specially. It means that even if you are using csh, ksh, or anything else as your interactive shell, that what follows should be interpreted by the Bourne shell. Similarly, a Perl script may start with the line #!/usr/bin/perl to tell your interactive shell that the program which follows should be executed by perl. For Bourne shell programming, we shall stick to #!/bin/sh. The third line runs a command: echo, with two parameters, or arguments - the first is \"Hello\"; the second is \"World\". Note that echo will automatically put a single space between its parameters. To make it executable, run chmod +rx \u003cfilename\u003e ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:0:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables Let’s look back at our first Hello World example. This could be done using variables. Note that there must be no spaces around the “=” sign: VAR=value works; VAR = value doesn’t work. In the first case, the shell sees the “=” symbol and treats the command as a variable assignment. In the second case, the shell assumes that VAR must be the name of a command and tries to execute it. #!/bin/sh MY_MESSAGE=\"Hello World\" echo $MY_MESSAGE This assigns the string “Hello World” to the variable MY_MESSAGE then echoes out the value of the variable. Note that we need the quotes around the string Hello World. Whereas we could get away with echo Hello World because echo will take any number of parameters, a variable can only hold one value, so a string with spaces must be quoted so that the shell knows to treat it all as one. Otherwise, the shell will try to execute the command World after assigning MY_MESSAGE=Hello The shell does not care about types of variables; they may store strings, integers, real numbers - anything you like. We can interactively set variable names using the read command; the following script asks you for your name then greets you personally #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME - hope you're well.\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:1:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Escape Characters Certain characters are significant to the shell; for example, that the use of double quotes (\") characters affect how spaces and TAB characters are treated, for example: $ echo Hello World Hello World $ echo \"Hello World\" Hello World So how do we display: Hello \"World\" ? $ echo \"Hello \\\"World\\\"\" The first and last \" characters wrap the whole lot into one parameter passed to echo so that the spacing between the two words is kept as is. But the code: $ echo \"Hello \" World \"\" would be interpreted as three parameters: “Hello \" World \"” So the output would be Hello World Note that we lose the quotes entirely. This is because the first and second quotes mark off the Hello and following spaces; the second argument is an unquoted “World” and the third argument is the empty string; “”. ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:2:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Loop ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"For Loop #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i\" done #!/bin/sh for i in hello 1 * 2 goodbye do echo \"Looping ... i is set to $i\" done The output of the above code is Looping ... i is set to hello Looping ... i is set to 1 Looping ... i is set to (name of first file in current directory) ... etc ... Looping ... i is set to (name of last file in current directory) Looping ... i is set to 2 Looping ... i is set to goodbye This is well worth trying. Make sure that you understand what is happening here. Try it without the * and grasp the idea, then re-read the Wildcards section and try it again with the * in place. Try it also in different directories, and with the * surrounded by double quotes, and try it preceded by a backslash (\\*) ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:1","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"While Loop #!/bin/sh INPUT_STRING=hello while [ \"$INPUT_STRING\" != \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done #!/bin/sh while : do echo \"Please type something in (^C to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done The colon (:) always evaluates to true; whilst using this can be necessary sometimes, it is often preferable to use a real exit condition. Compare quitting the above loop with the one below; see which is the more elegant. Also think of some situations in which each one would be more useful than the other: #!/bin/sh while read input_text do case $input_text in hello) echo English ;; howdy) echo American ;; gday) echo Australian ;; bonjour) echo French ;; \"guten tag\") echo German ;; *) echo Unknown Language: $input_text ;; esac done \u003c myfile.txt This reads the file “myfile.txt”, one line at a time, into the variable “$input_text”. The case statement then checks the value of $input_text. If the word that was read from myfile.txt was “hello” then it echoes the word “English”. If it was “gday” then it will echo Australian. If the word (or words) read from a line of myfile.txt don’t match any of the provided patterns, then the catch-all “*” default will display the message “Unknown Language: $input_text” - where of course “$input_text” is the value of the line that it read in from myfile.txt. A handy Bash (but not Bourne Shell) tip I learned recently from the Linux From Scratch project is: mkdir rc{0,1,2,3,4,5,6,S}.d instead of the more cumbersome: for runlevel in 0 1 2 3 4 5 6 S do mkdir rc${runlevel}.d done And ls can be done recursively, too: $ cd / $ ls -ld {,usr,usr/local}/{bin,sbin,lib} drwxr-xr-x 2 root root 4096 Oct 26 01:00 /bin drwxr-xr-x 6 root root 4096 Jan 16 17:09 /lib drwxr-xr-x 2 root root 4096 Oct 27 00:02 /sbin drwxr-xr-x 2 root root 40960 Jan 16 19:35 usr/bin drwxr-xr-x 83 root root 49152 Jan 16 17:23 usr/lib drwxr-xr-x 2 root root 4096 Jan 16 22:22 usr/local/bin drwxr-xr-x 3 root root 4096 Jan 16 19:17 usr/local/lib drwxr-xr-x 2 root root 4096 Dec 28 00:44 usr/local/sbin drwxr-xr-x 2 root root 8192 Dec 27 02:10 usr/sbin ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:2","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Test Test is used by virtually every shell script written. It may not seem that way, because test is not often called directly. test is more frequently called as [. [ is a symbolic link to test, just to make shell programs more readable. It is also normally a shell builtin (which means that the shell itself will interpret [ as meaning test, even if your Unix environment is set up differently): $ type [ [ is a shell builtin $ which [ /usr/bin/[ $ ls -l /usr/bin/[ lrwxrwxrwx 1 root root 4 Mar 27 2000 /usr/bin/[ -\u003e test $ ls -l /usr/bin/test -rwxr-xr-x 1 root root 35368 Mar 27 2000 /usr/bin/test This means that ‘[’ is actually a program, just like ls and other programs, so it must be surrounded by spaces: if [$foo = \"bar\" ] will not work; it is interpreted as if test$foo = \"bar\" ], which is a ‘]’ without a beginning ‘[’. Put spaces around all your operators. I’ve highlighted the mandatory spaces with the word ‘SPACE’ . Note: Some shells also accept “==” for string comparison; this is not portable, a single “=” should be used for strings, or “-eq” for integers. Test is a simple but powerful comparison utility. For full details, run man test on your system, but here are some usages and typical examples. Test is most often invoked indirectly via the if and while statements. It is also the reason you will come into difficulties if you create a program called test and try to run it, as this shell builtin will be called instead of your program! The syntax for if...then...else... is: if [ ... ] then # if-code else # else-code fi Also, be aware of the syntax - the “if [ ... ]” and the “then” commands must be on different lines. Alternatively, the semicolon “;” can separate them: if [ ... ]; then # do something fi You can also use the elif, like this: if [ something ]; then echo \"Something\" elif [ something_else ]; then echo \"Something else\" else echo \"None of the above\" fi This will echo \"Something\" if the [ something ] test succeeds, otherwise it will test [ something_else ], and echo \"Something else\" if that succeeds. If all else fails, it will echo \"None of the above\". ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:4:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Case The case statsement saves going through a whole set of if ... then ... else statements. Its syntax is really simple: #!/bin/sh echo \"Please talk to me ...\" while : do read INPUT_STRING case $INPUT_STRING in hello) echo \"Hello yourself!\" ;; bye) echo \"See you again!\" break ;; *) echo \"Sorry I don't understand\" ;; esac done ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:5:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables 2 The first set of variables we will look at are $0 ... $9 and $#. The variable $0 is the basename of the program as it was called. $1...$9 are the first 9 additional parameters the script was called with. The variable $@ is all parameters. The variable $* is similar, but does not preserve any whitespace and quoting, so “File with spaces” becomes “File”, “with”, and “spaces”. $# is the number of parameters the script was called with. #!/bin/sh echo \"I was called with $# parameters\" echo \"My name is $0\" echo \"My first parameter is $1\" The othere two main variables set are $$ and $!. These are both process numbers. The $$ variable is the PID of the currently running shell. This can be useful for creating temporary files, such as /tmp/my-script.$$ which is useful if many instances of the script could be run at the same time, and they all need their own temporary files. The $! variable is the PID of the last run background processd. This is useful to keep track of the process as it gets on with its job. Another interesting vardfiable is IFS. This is the Interfal Field Separator. The default value is SPACE TAB NEWLINE, but if you are changing it, it’s easier to take a copy as shown: #!/bin/sh old_IFS=\"$IFS\" IFS=: # Set IFS as colon echo \"Please input some data separated by colons\" read x y z IFS=$old_IFS echo \"x is $x y is $y z is $z\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:6:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Functions #!/bin/sh add_a_user() { USER=$1 PASSWORD=$2 COMMENTS=$@ echo \"Adding user $USER\" echo useradd -c \"$COMMENTS\" $USER echo passwd $USER $PASSWORD } ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:7:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Reference shellscript ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:8:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Linux"],"content":"You should use Linux!","date":"2024-04-21","objectID":"/20240421_why-linux/","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Linux"],"content":"Linux, A Path to Digital Simplicity In an age dominated by digital clutter and overwhelming software choices, the minimalist philosophy stands out as a beacon for those seeking simplicity and efficiency. This approach not only applies to physical possessions but extends into the digital realm, where Linux has become a preferred tool for minimalists. Linux, an open-source operating system, embodies the principles of minimalism by offering users control over their digital environments. Unlike mainstream operating systems that often come loaded with non-essential features and bloatware, Linux allows users to select only the components they need, creating a lean and efficient system. The minimalist appeal of Linux is evident in its customizable nature. Users can choose from a variety of distributions, each tailored for different needs. For instance, distributions like Arch Linux provide barebone setup that users can expand as needed. This customization extends to the user interface, where options range from feature-rich desktop environments to more austere ones like i3wm or DWM, which use fewer resources and maintain a clean, unobtrusive design. Furthermore, Linux’s robust command-line interface is a minimalist’s dream. It enables users to perform tasks efficiently without the graphical overhead, which is particularly appealing to those who favor functionality and speed over visual elements. Moreover, Linux supports the concept of free software, which resonates with minimalists’ preference for authenticity and freedom from commercial constraints. Users are free to modify, improve, and redistribute their software in ways that proprietary systems do not allow. Linux offers a compelling choice for anyone looking to embrace a minimalist digital lifestyle. It provides the tools to create a personalized and simple digital environment, encourages the efficient use of resources, and upholds values of freedom and simplicity. For those seeking to reduce their digital footprint while maximizing functionality, Linux proves that less can indeed be more. ","date":"2024-04-21","objectID":"/20240421_why-linux/:0:1","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Python"],"content":"Guide to keep sensitive data in Python","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes: Resource handles to the database, Memcached, and other backing services Credentials to external services such as Amazon S3 or Twitter Per-deploy values such as the canonical hostname for the deploy Apps sometimes store config as constants in the code. This is a violation of twelve-factor, which requires strict separation of config from code. Config varies substantially across deploys, code does not. A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. In a twelve-factor app, env vars are granular controls, each fully orthogonal to other env vars. They are never grouped together as “environments”, but instead are independently managed for each deploy. This is a model that scales up smoothly as the app naturally expands into more deploys over its lifetime. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:0:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Environment Variable For example, you shouldn’t put information directly in your code. db_user = 'my_db_user' db_password = 'my_db_pass_123!' Let’s keep the sensitive information in .bash_profile as follows: export DB_USER=\"my_db_user\" export DB_PASS='my_db_pass_123!' Then, we can call them by import os db_user = os.environ.get('DB_USER') db_password = os.environ.get('DB_PASS') ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:1:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"dotenv ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Introduction Python-dotenv reads key-value pairs from a .env file and can set them as environment variables. It helps in the development of applications following the 12-factor principles. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:1","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Basics Installation: pip install python-dotenv Create .env file in your project directory. Put the data (or variables) in the .env file. e.g, API_KEY=\"dafjei304aldkjf20akj\" To load your key, First, use load_dotenv() with os.getenv(\"[Your variable]\") e.g., API_KEY=os.getenv(\"API_KEY\") Make sure to update .gitignore to exclude the .env file. from dotenv import load_dotenv load_dotenv() API_KEY = os.getenv(\"API_KEY\") or \"\" ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:2","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Reference The Twelve Factor App ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:3:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Guide to understand type hint in Python.","date":"2024-04-20","objectID":"/20240421_type-hint/","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Type hinting is not mandatory, but it can make your code easier to understand and debug by Improved readability Better IDE support: IDEs and linters can use type hints to check your code for potential errors before runtime. While type hints can be simple classes like float or str , they can also be more complex. The typing module provides a vocabulary of more advanced type hints. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Basics # This is how you declare the type of a variable age: int = 1 # You don't need to initialize a variable to annotate it a: int # Ok (no value at runtime until assigned) # Doing so can be useful in conditional branches child: bool if age \u003c 18: child = True else: child = False x: int = 1 x: float = 1.0 x: bool = True x: str = \"test\" x: bytes = b\"test\" # For collections on Python 3.9+, the type of the collection item is in brackets x: list[int] = [1] x: set[int] = {6, 7} # For mappings, we need the types of both keys and values x: dict[str, float] = {\"field\": 2.0} # Python 3.9+ # For tuples of fixed size, we specify the types of all the elements x: tuple[int, str, float] = (3, \"yes\", 7.5) # Python 3.9+ # For tuples of variable size, we use one type and ellipsis x: tuple[int, ...] = (1, 2, 3) # Python 3.9+ # On Python 3.8 and earlier, the name of the collection type is # capitalized, and the type is imported from the 'typing' module from typing import List, Set, Dict, Tuple x: List[int] = [1] x: Set[int] = {6, 7} x: Dict[str, float] = {\"field\": 2.0} x: Tuple[int, str, float] = (3, \"yes\", 7.5) x: Tuple[int, ...] = (1, 2, 3) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:1","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Union Union is for multiple types def process_message(msg: Union[str, bytes, None]) -\u003e str: ... # On Python 3.10+, use the | operator when something could be one of a few types x: list[int | str] = [3, 5, \"test\", \"fun\"] # Python 3.10+ # On earlier versions, use Union x: list[Union[int, str]] = [3, 5, \"test\", \"fun\"] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:2","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Optional # food can be either str or None. def eat_food(food: Optional[str]) -\u003e None: ... # Use Optional[X] for a value that could be None # Optional[X] is the same as X | None or Union[X, None] x: Optional[str] = \"something\" if some_condition() else None if x is not None: # Mypy understands x won't be None here because of the if-statement print(x.upper()) # If you know a value can never be None due to some logic that mypy doesn't # understand, use an assert assert x is not None print(x.upper()) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:3","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Any Any is a special type hint in Python that indicates that a variable can be of any type. It essentially disables static type checking for that variable. It’s typically used when you want to explicitly indicate that a certain variable can have any type, or when dealing with dynamically typed code where the type of the variable cannot be easily inferred. While Any provides flexibility, it also sacrifices the benefits of static type checking, as type errors related to variables annotated as Any won’t be caught by type checkers. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:4","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Functions: Callable Types Callable type hint can define types for callable functions. from typing import Callable Callable[[Parameter types, ...], return_types] Callable objects are functions, classes, and so on. Type [input types] and return types def on_some_event_happened(callback: Callable[[int, str, str], int]) -\u003e None: ... def do_this(a: int, b: str, c:str) -\u003e int: ... on_some_event_happened(do_this) # This is how you annotate a callable (function) value x: Callable[[int, float], float] = f def register(callback: Callable[[str], int]) -\u003e None: ... # A generator function that yields ints is secretly just a function that # returns an iterator of ints, so that's how we annotate it def gen(n: int) -\u003e Iterator[int]: i = 0 while i \u003c n: yield i i += 1 # You can of course split a function annotation over multiple lines def send_email(address: Union[str, list[str]], sender: str, cc: Optional[list[str]], bcc: Optional[list[str]], subject: str = '', body: Optional[list[str]] = None ) -\u003e bool: ... ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:5","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Classes class BankAccount: # The \"__init__\" method doesn't return anything, so it gets return # type \"None\" just like any other method that doesn't return anything def __init__(self, account_name: str, initial_balance: int = 0) -\u003e None: # mypy will infer the correct types for these instance variables # based on the types of the parameters. self.account_name = account_name self.balance = initial_balance # For instance methods, omit type for \"self\" def deposit(self, amount: int) -\u003e None: self.balance += amount def withdraw(self, amount: int) -\u003e None: self.balance -= amount # User-defined classes are valid as types in annotations account: BankAccount = BankAccount(\"Alice\", 400) def transfer(src: BankAccount, dst: BankAccount, amount: int) -\u003e None: src.withdraw(amount) dst.deposit(amount) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:6","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Annotated Annotated in python allows developers to declare type of a reference and and also to provide additional information related to it. name = Annotated[str, \"first letter is capital\"] This tells that name is of type str and that name[0] is a capital letter. On its own Annotated does not do anything other than assigning extra information (metadata) to a reference. It is up to another code, which can be a library, framework or your own code, to interpret the metadata and make use of it. For example FastAPI uses Annotated for data validation: def read_items(q: Annotated[str, Query(max_length=50)]) Here the parameter q is of type str with a maximum length of 50. This information was communicated to FastAPI (or any other underlying library) using the Annotated keyword. Annotated[\u003ctype\u003e, \u003cmetadata\u003e] Here is an example of how you might use Annotated to add metadata to type annotations if you were doing range analysis: @dataclass class ValueRange: lo: int hi: int T1 = Annotated[int, ValueRange(-10, 5)] T2 = Annotated[T1, ValueRange(-20, 3)] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:7","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"TypeVar This is a special type for generic types. from typing import Sequence, TypeVar, Iterable T = TypeVar(\"T\") # `T` is typically used to represent a generic type variable def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Since the generic type is used, batch_iter function can take any type of Sequence type data. For instance, Sequence[int], Sequence[str], Sequence[Person] If we use bound, then we can restrict the generic type. For example, from typing import Sequence, TypeVar, Iterable, Union T = TypeVar(\"T\", bound=Union[int, str, bytes]) def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Thus, the following code will show an error as it takes a list of float numbers: batch_iter([1.1, 1.3, 2.5, 4.2, 5.5], 2) Note that in Python 3.12, generic type hint has been changed ","date":"2024-04-20","objectID":"/20240421_type-hint/:1:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Reference ArjanCodes Type hint cheat sheet ","date":"2024-04-20","objectID":"/20240421_type-hint/:2:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":null,"content":"About Han","date":"2024-04-21","objectID":"/about/","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Han Cheol Moon Welcome to Han's XYZ, where you'll explore a world of ideas across every dimension of thought. I am a curiosity-driven Machine Learning Scientist, passionate about exploring the depths of data and algorithms. As an enthusiastic learner and code lover, I am always eager to expand my knowledge and embrace innovation, with a commitment to sharing my insights and discoveries with you. ","date":"2024-04-21","objectID":"/about/:0:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Research Interests Natural language processing (NLP) Robustness of NLP systems ","date":"2024-04-21","objectID":"/about/:1:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Professional Employments Samsung Electronics Staff ML/DL Engineer, Sept 2023 - Present ","date":"2024-04-21","objectID":"/about/:2:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Education Nanyang Technological University, 2019-2023 Ph.D. in Computer Science Yonsei University, 2016-2018 M.S. in Electrical \u0026 Electronic Engineering Chung-Ang University, 2009-2016 B.S. in Electrical \u0026 Electronic Engineering (Military Service, 2009-2012) ","date":"2024-04-21","objectID":"/about/:3:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Skills Programming Languages Python, C/C++, MATLAB, and Bash/Shell ML/DL Research PyTorch, Tensorflow, Scipy, Numpy, Scikit-learn, Pandas, and HuggingFace ML/DL Ops Git, Docker, Kubernetes, Jenkins, DVC, FastAPI, and LangChain Database SQLite and PostgreSQL with SQLAlchemy and Alembic WebDev HTML/CSS, Django, and Hugo WorkFlow I’ve used Microsoft and GNU/Linux systems (both Debian and Arch-based varieties) with a Vim-based setup for writing scripts and managing files in a tiling window manager (i3). I use Git for version control with Docker and schedule multiple tasks with TaskSpooler. I compile documents using $\\LaTeX$. Soft Skills Time Management, Teamwork, Problem-solving, Documentation, Engaging Presentation. ","date":"2024-04-21","objectID":"/about/:4:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching ","date":"2024-04-21","objectID":"/about/:5:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching Assistant DeepNLP, 2019-2020 Nanyang Technological University, Singapore Digital Logic Circuits, Spring 2018 Yonsei University, Korea Multimedia Signal Processing, Fall 2017 Yonsei University, Korea Signals and Systems, Fall 2016 Yonsei University, Korea ","date":"2024-04-21","objectID":"/about/:5:1","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Honors and Award Singapore International Graduate Award (SINGA), 2019-2023 Top of Class Scholarship, Chung-Ang University, 2015 Dean’s List, Chung-Ang University, 2013-2014 ","date":"2024-04-21","objectID":"/about/:6:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Service and leadership Military Service, 2009-2012 Republic of Korea Army, Capital Defense Command ‘SHIELD’, 1st Security Group Guarding presidential residence Sergeant ","date":"2024-04-21","objectID":"/about/:7:0","tags":null,"title":"About Han","uri":"/about/"}]