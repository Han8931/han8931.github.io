[{"categories":["linux","pass"],"content":"Password management","date":"2025-05-24","objectID":"/drafts/20250524_pass/","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"A Minimalist‚Äôs Guide to pass‚Äî the Unix Password Manager Safely wrangle your secrets from the command-line using GPG encryption and a few intuitive commands. ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:0:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"1. Why pass? Single-purpose \u0026 transparent ‚Äì every secret is just a GPG-encrypted file in ~/.password-store/. Leverages tools you already trust ‚Äì GnuPG for encryption and standard Unix commands for everything else (grep, git, find, etc.). Portable \u0026 scriptable ‚Äì works the same on any POSIX shell and is easy to automate. Prerequisites GnuPG ‚â• 2.2 pass package (available in most distro repos: pacman -S pass, apt install pass, etc.) A clipboard utility (xclip, xsel, wl-clipboard, or pbcopy on macOS) if you want the copy-to-clipboard feature. ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:1:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"2. Generate your first GPG key gpg --full-generate-key Key Type ‚Äì pick the default RSA \u0026 RSA (or ECC if you prefer). Key Size ‚Äì 3072 or 4096 bits (stronger ‚áí larger). Expiration ‚Äì choose a sensible period (e.g., 2 years) so compromised keys self-retire. Identity ‚Äì enter the name + e-mail that will label this key. Passphrase ‚Äì a strong one! You‚Äôll type this each time GPG needs your key (or unlock once per session via a GPG agent). ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:2:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"3. Find your key ID gpg --list-secret-key --keyid-format LONG Look for the line that starts with sec: sec rsa3072/AB12CD34EF56GH78 2025-05-17 [SC] AB12CD34EF56GH78 (16 hexadecimal characters after the slash) is your key ID ‚Äì copy it; we‚Äôll use it to initialise pass. ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:3:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"4. Initialise pass pass init AB12CD34EF56GH78 What happens? pass creates ~/.password-store/ Every file placed there will be encrypted for the listed key(s). A .gpg-id file records which keys to use so you can share the store with additional people later. (If you ever rotate keys, run pass init --path . newKEYID to re-encrypt subsets of the store.) ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:4:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"5. Add your first secret pass insert twitter.com pass opens your $EDITOR (= vi, nano, etc.). Type your password on the first line; anything after that is free-form notes (e.g., username, 2FA scratch codes). Save \u0026 quit ‚Äì you‚Äôll be prompted for your GPG passphrase and the file twitter.com.gpg is created inside the store. Directory layout after one entry: ~/.password-store/ ‚îú‚îÄ‚îÄ .gpg-id ‚îî‚îÄ‚îÄ twitter.com.gpg (Feel free to nest categories like Business/github.com, they become sub-directories.) ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:5:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"6. Display or decrypt secrets Plain display (prints to STDOUT): pass twitter.com # same as `pass twitter` One-off manual decryption (rarely needed, but shows nothing up pass‚Äôs sleeve): gpg -d ~/.password-store/facebook.com.gpg ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:6:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"7. Copy to clipboard (auto-clear!) pass -c git_token The password is pushed to your clipboard. After 45 seconds (configurable via PASSWORD_STORE_CLIP_TIME), pass industriously scrubs it. ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:7:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"8. Remove an entry pass rm Business/cheese-whiz-factory Flags worth knowing: -r ‚Üí recursive (delete directories). -f ‚Üí force (skip confirmation). Deleted files go to your desktop trash only if your shell supports it; otherwise they‚Äôre gone forever (but still recoverable via git, see below). ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:8:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"9. Pro tips \u0026 hygiene Task Command / Tip Version control your store cd ~/.password-store \u0026\u0026 git init \u0026\u0026 git add . \u0026\u0026 git commit -m \"First secret\" With Git you get effortless history and the ability to sync between machines over SSH. Use multiple recipients (e.g., team store) pass init KEYID1 KEYID2 ... ‚Äì future inserts are encrypted for all recipients. Rename or move a secret pass mv oldname newname ‚Äì keeps history intact. Bulk import existing passwords pass import pass-dump.txt or script with pass insert -m \u003cname\u003e. Search pass grep \u003cpattern\u003e ‚Äì greps filenames and decrypted contents. Shell tab-completion Enable the bundled pass.bash-completion or pass.fish-completion for lightning-fast navigation. GUI helpers qtpass, browserpass, passff let your browser/mobile talk to the same store. ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:9:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"10. Backing up \u0026 restoring Because the store is plain GPG files: tar czf pass-backup-$(date +%F).tar.gz ~/.password-store To restore: tar xzf pass-backup-2025-05-17.tar.gz -C ~/ pass git checkout . (If you kept the Git repo you can just git pull from your remote.) ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:10:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["linux","pass"],"content":"11. Revoking / rotating your key Generate \u0026 publish a revocation certificate right after key creation: gpg --output ~/revocation.asc --gen-revoke AB12CD34EF56GH78 If the key is ever compromised, import that file (gpg --import revocation.asc) and re-encrypt the store with a new key: gpg --full-generate-key # new key pass init NEWKEYID # re-encrypt everything ","date":"2025-05-24","objectID":"/drafts/20250524_pass/:11:0","tags":["linux","pass","password manager"],"title":"Managing Password using Pass","uri":"/drafts/20250524_pass/"},{"categories":["python","programming"],"content":"Logging tutorial","date":"2025-05-17","objectID":"/20250517_python_logging/","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"A gentle, practical introduction to logging in Python ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:0","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Why bother with a dedicated logging library? Prints don‚Äôt scale. print() is fine during quick experiments, but real programs need a record that can be filtered, rotated, or shipped elsewhere. Separation of concerns. You decide what to log in your code; logging decides where and how to write it (console, file, etc.). Built-in, no extra dependency. The standard library‚Äôs logging module is powerful enough for most applications. ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:1","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Core concepts Concept Role in the ecosystem Typical examples Logger The entry point your code calls (logger.info(...)). You can have many, one per module. \"__main__\", \"my_package.worker\" Handler Decides where the record goes. StreamHandler (stdout), FileHandler, TimedRotatingFileHandler, SMTPHandler Formatter Decides how the record looks. '%(asctime)s - %(levelname)s - %(name)s - %(message)s' ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:2","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"A minimal logger import logging logging.basicConfig( level=logging.INFO, format=\"%(levelname)s | %(message)s\" ) logging.info(\"Hello, world!\") basicConfig is a one-liner good for small scripts. In bigger projects, mixing multiple modules / log files, you‚Äôll want finer control. ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:3","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Rotating files at midnight Rotating a log file means creating a new log file after a certain time or size limit is reached. In this case, a new file is created every night at midnight. Only the most recent two log files are kept‚Äîyesterday‚Äôs and today‚Äôs‚Äîwhile older ones are deleted automatically. Keeps log files from growing forever. Eases log shipping/archiving. A single command wipes logs older than n days. The TimedRotatingFileHandler in the helper below: Parameter Value when=\"midnight\" Rotate every day at 00:00 server local time. interval=1 Every 1 unit of when (here: days). backupCount=1 Keep one old file (log_file.log.2025-05-17). Older ones are deleted. encoding=\"utf-8\" Avoids surprises with non-ASCII characters. If you set backupCount=30, it will keep: Today‚Äôs log file (log_file.log), and The 30 most recent rotated log files (log_file.log.2025-05-17, log_file.log.2025-05-16, ‚Ä¶) ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:4","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Drop-in helper: get_logger # logger.py import logging from pathlib import Path from logging.handlers import TimedRotatingFileHandler LOG_FILE = Path(\"./logs/log_file.log\") LOG_FILE.parent.mkdir(parents=True, exist_ok=True) # ensure ./logs/ def get_logger(name: str) -\u003e logging.Logger: \"\"\"Return a module-specific logger configured for daily rotation.\"\"\" logger = logging.getLogger(name) logger.setLevel(logging.INFO) # Prevent adding duplicate handlers if the module is imported repeatedly if not logger.handlers: handler = TimedRotatingFileHandler( filename=LOG_FILE, when=\"midnight\", interval=1, backupCount=1, encoding=\"utf-8\", ) formatter = logging.Formatter( \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", ) handler.setFormatter(formatter) logger.addHandler(handler) # Block propagation so the same record is not also printed logger.propagate = False return logger if not logger.handlers:: Guarantees that multiple imports don‚Äôt attach multiple handlers, which would duplicate every line in the output. logger.propagate = False: Stops messages from bubbling up to the root logger, so you don‚Äôt accidentally get console spam when your app also configures a root handler. In other words, ‚ÄúDon‚Äôt pass my log message to parent loggers. I‚Äôll handle it here.‚Äù ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:5","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Using the helper in your scripts # worker.py from logger import get_logger logger = get_logger(__name__) def create_job(job_id: str): logger.info(f\"Create: JobID: {job_id}\") Handling exceptions: try: result = do_something() except ProcessException as e: # Log message plus full traceback logger.error(\"%s: %s\", e.code, e.message, exc_info=True) When you‚Äôre inside an except block and you want to record not just the error message, but also where exactly the error happened. This is a case where exc_info comes in: try: 1 / 0 except ZeroDivisionError as e: logger.error(\"An error occurred!\", exc_info=True) This will produce a log like: 2025-05-17 10:23:00 - ERROR - __main__ - An error occurred! Traceback (most recent call last): File \"main.py\", line 2, in \u003cmodule\u003e 1 / 0 ZeroDivisionError: division by zero Without it, you would only see 2025-05-17 10:23:00 - ERROR - __main__ - An error occurred! ","date":"2025-05-17","objectID":"/20250517_python_logging/:1:6","tags":["python","logging","logger"],"title":"Introduction to logging in Python","uri":"/20250517_python_logging/"},{"categories":["python","programming"],"content":"Pathlib tutorial","date":"2025-05-17","objectID":"/20250517_pathlib/","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"From Type Hint to Power Tool: Python‚Äôs Pathlib For a long time, I used Path from Python‚Äôs pathlib module purely as a type hint - a way to make function signatures look more modern and semantically clear. Like this: from pathlib import Path def process_file(file_path: Path): ... It changed when I started building an application that handled user-uploaded documents. I had to create temporary folders, write intermediate files, manage output paths, and ensure directories existed before saving results. That‚Äôs when Path went from just a type hint to a core part of my file management logic. ","date":"2025-05-17","objectID":"/20250517_pathlib/:1:0","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"Why pathlib is Worth More than a Hint Here are the use cases where it transformed my workflow: ","date":"2025-05-17","objectID":"/20250517_pathlib/:2:0","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"Ensuring Directories Exist Before: import os if not os.path.exists(\"output\"): os.makedirs(\"output\") Now: from pathlib import Path Path(\"output\").mkdir(parents=True, exist_ok=True) parents=True: Creates any missing parent directories. exist_ok=True: Prevents an error if the directory already exists. This is equivalent to os.makedirs(), but more cleaner and readable! ","date":"2025-05-17","objectID":"/20250517_pathlib/:2:1","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"Managing Directories While handling temporary files for a file-processing API, I needed to: Create a temp folder Ensure it exists Write intermediate files Clean up afterward With Path, this became natural and structured: from pathlib import Path temp_dir = Path(\"/tmp/myapp\") / \"job123\" temp_dir.mkdir(parents=True, exist_ok=True) output_file = temp_dir / \"translated.txt\" output_file.write_text(\"Translated content\") content = output_file.read_text() write_text(data: str): Writes a string to a text file (overwrites if it exists). read_text(): Reads file content and returns a string. To cleanup the (temporary) directories: # Cleanup later if needed output_file.unlink() temp_dir.rmdir() unlink: Deletes a file rmdir: Removes an empty directory You can use unlink with a parameter missing_ok: Without missing_ok=True: raises FileNotFoundError if the file doesn‚Äôt exist. With missing_ok=True: silently does nothing if the file is already gone. You can also check path status to avoid runtime errors: p = Path(\"result.txt\") p.exists() # True if file or directory exists p.is_file() # True if it's a regular file p.is_dir() # True if it's a directory To list directory contents: p = Path(\"my_folder\") for item in p.iterdir(): print(item) ","date":"2025-05-17","objectID":"/20250517_pathlib/:2:2","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"Path Arithmetic Joining paths becomes expressive with /: base = Path(\"/data\") file = base / \"user\" / \"output.txt\" Compared to: file = os.path.join(\"/data\", \"user\", \"output.txt\") You can also extract name, stem, suffix (file extension) and so on: p = Path(\"/home/user/project/file.txt\") p.name # 'file.txt' p.stem # 'file' p.suffix # '.txt' p.parent # PosixPath('/home/user/project') p.parts # ('/', 'home', 'user', 'project', 'file.txt') ","date":"2025-05-17","objectID":"/20250517_pathlib/:2:3","tags":["python","pathlib","path"],"title":"Rediscovering Python's Pathlib","uri":"/20250517_pathlib/"},{"categories":["python","programming"],"content":"Tutorial for UV python package manager","date":"2025-04-13","objectID":"/20250413_uv_package_manager/","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Meet uv¬†‚Äì¬†A Blazingly¬†Fast, All‚Äëin‚ÄëOne Python Package¬†Manager In my last post I dove into Poetry , one of the best‚Äëloved modern packaging tools. However, Poetry is just one piece of an toolkit: we still reach for pip to install packages, virtualenv to isolate them, pyenv to juggle Python versions, and maybe Pipenv or pip‚Äëtools for lock‚Äëfiles. Each solves its own niche, yet hopping between them adds friction. uv removes that friction. This single, project manager‚Äîwritten in Rust and typically 10-1000x faster-replaces the whole stack: installing Python itself, creating virtual environments, resolving and locking dependencies, and even publishing to PyPI, all behind one concise CLI. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:1:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Why Python Packaging Needed a Fresh Start Pain Point Traditional Landscape How uv Fixes It Tool sprawl pip / virtualenv / pyenv / Poetry / pipx Covers the full workflow Performance Pure‚ÄëPython dependency resolution and single‚Äëthreaded downloads Rust core, SAT‚Äëbased solver (PubGrub) Reproducibility requirements.txt is order‚Äësensitive \u0026 lacks metadata Deterministic uv.lock capturing hashes \u0026 markers Since its public launch, uv already powers \u003e10‚ÄØ% of all downloads on PyPI‚Äîevidence that developers crave a faster, simpler workflow. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:2:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"The Lock‚ÄëFile Landscape at a Glance Before we dive into uv, it helps to remember what ‚Äúlocking‚Äù looks like in the existing ecosystem: Pipenv ‚Äì¬†Pipfile + Pipfile.lock Poetry ‚Äì¬†pyproject.toml + poetry.lock pip ‚Äì¬†requirements.txt created via pip freeze Each of these pins versions, but they store different metadata and often disagree on the resolver algorithm. uv adopts pyproject.toml syntax and adds its own uv.lock that captures exact versions and SHA‚Äë256 hashes for deterministic installs. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:3:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"From init to publish ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"1. Spin‚Äëup uv init myapp # ‚ûä scaffold project \u0026 git repo uv chooses a Python interpreter (downloading if necessary), creates .venv, writes .python-version, and commits a minimal pyproject.toml. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:1","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"2. Add Dependencies uv add ruff pytest # ‚ûã add linting \u0026 tests Environment bootstrap ‚Äì if .venv doesn‚Äôt exist, it‚Äôs created. Dependency resolution ‚Äì the PubGrub SAT solver computes a compatible graph (an NP‚Äëhard problem) in milliseconds. Lockfile update ‚Äì uv.lock is regenerated atomically so that users can reproduce the exact set. Installation If you Need to remove something, uv remove pytest cleans both the environment and the lockfile. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:2","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"3. Iterate Quickly uv run hello.py # ‚ûå execute code inside .venv uv run uvicorn main:app --reload # run uvicorn uvx ruff check # ‚ûç run a CLI tool in a throw‚Äëaway env uv includes a dedicated interface for interacting with tools. Tools can be invoked without installation using uv tool run, in which case their dependencies are installed in a temporary virtual environment isolated from the current project. Because it is very common to run tools without installing them, a uvx alias is provided for uv tool run ‚Äî the two commands are exactly equivalent. For brevity, the documentation will mostly refer to uvx instead of uv tool run. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:3","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"4. Manage Python Versions Mid‚ÄëStream uv python install 3.12.0 # ‚ûé fetch new interpreter # switch by editing .python-version, then: uv sync # rebuild .venv deterministically No sudo, no global state‚Äîinterpreters live under ~/.local/share/uv/python. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:4","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"5. Publish uv publish # ‚ûè upload to PyPI Zero extra config; metadata comes from pyproject.toml. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:4:5","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Workspaces: Multiple Apps, One Cohesive Repo uv can treat a directory tree as a workspace. Sub‚Äëprojects share the same interpreter and .venv, which is perfect for monorepos housing a library, CLI, and docs site side‚Äëby‚Äëside. uv init api # creates api/ inside current repo uv init web --no-workspace # standalone env if you prefer isolation Switching between shared and isolated workflows is a flag away. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:5:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Dependency Groups for Dev \u0026 Prod Sometimes you want linting and testing tools only in development. uv follows Poetry‚Äôs group syntax: uv add --dev pytest ruff # added under [tool.poetry.group.dev] uv sync --no-group dev # skip dev deps ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:6:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Testing with Pytest (and a VS¬†Code Tip) When you start a project, it‚Äôs worth deciding up front where your test files will live. I like to keep them in a dedicated tests/ directory that sits alongside the src/ directory rather than inside it. That keeps production code and test code clearly separated and makes the project tree easy to scan: project_root/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ main.py ‚îÇ ‚îî‚îÄ‚îÄ utils.py ‚îî‚îÄ‚îÄ tests/ ‚îú‚îÄ‚îÄ test_main.py ‚îî‚îÄ‚îÄ test_utils.py With this layout, tools such as pytest find your tests automatically, and IDEs can apply different run configurations or linters to src/ and tests/ without extra setup. Enable pytest discovery in VS¬†Code by adding: // .vscode/settings.json { \"python.testing.pytestEnabled\": true, \"python.testing.cwd\": \"${workspaceFolder}/tests\" } Install and run tests: uv add --dev pytest uv run pytest -q ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:7:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Migrating an Existing venv + pip Project Freeze current deps: pip freeze \u003e requirements.txt Initialize uv in place: uv init . Import: uv pip install -r requirements.txt uv lock Delete the old .venv and enjoy the speed‚Äëup. ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:8:0","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["python","programming"],"content":"Cheat Sheet pip / virtualenv uv python -m venv .venv uv venv pip install pkg uv add pkg pip install -r requirements.txt uv pip install -r requirements.txt pip uninstall pkg uv remove pkg pip freeze uv pip freeze pip list uv pip list I hope you enjoyed my post! ","date":"2025-04-13","objectID":"/20250413_uv_package_manager/:8:1","tags":["python","uv","package manager","Virtual Environment"],"title":"All‚Äëin‚ÄëOne Python Package¬†Manager: UV!","uri":"/20250413_uv_package_manager/"},{"categories":["git","programming","vim"],"content":"Git with Vim Fugitive","date":"2025-04-13","objectID":"/20250413_vim_fugitive/","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"If you‚Äôre working with Git and Vim, vim-fugitive is an essential plugin that transforms your editor into a full-fledged Git interface. Here‚Äôs how I use Fugitive to review, stage, and commit changes‚Äîwithout ever leaving Vim. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:0:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Browsing Git History and Logs First Before jumping into edits, it‚Äôs often useful to understand the file‚Äôs history or recent project changes. :Git log ‚Äî shows the project‚Äôs commit history in reverse chronological order :0Gllog ‚Äî shows the history of the current file To explore who changed what in a file: :Git blame Press \u003cEnter\u003e on a line to inspect its commit, or press g? to see other commands. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:1:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Viewing and Comparing Versions of a File You might want to compare your current changes with previous versions: :Gedit HEAD~2:% ‚Äî opens the file as it was 2 commits ago :Gdiffsplit ‚Äî shows the current file against the index (staged version) Use :Gvdiffsplit for vertical splits or :Ghdiffsplit for horizontal In diff mode, use: do ‚Äî to obtain changes from the other pane dp ‚Äî to put your changes into the other pane ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:2:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Making and Reviewing Changes After edits, you may want to review your local changes: :Gdiffsplit This shows differences between the working copy and the staged version. Use this view to double-check before staging. If you decide to undo your changes: :Gread This reverts the buffer to the version from the index or last commit. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:3:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Staging and Preparing Commits Now you‚Äôre ready to prepare your changes for commit: :Git This opens a status window with: Untracked files Modified (unstaged) files Staged files You can: Press - to toggle staged/unstaged Press X to discard changes Press dv, dh, or dd to open diff views for review When you‚Äôre satisfied, press: cc to commit staged changes ca to amend the previous commit You‚Äôll get a buffer to write your commit message. Save and close it to finish. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:4:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Managing Files and Advanced Commands Here are more powerful tools from Fugitive: :Gwrite ‚Äî stage the file (like git add) :GMove / :GRename ‚Äî move or rename a file with Git :GDelete / :GRemove ‚Äî remove a file and buffer :Ggrep, :Glgrep ‚Äî grep across the repository :GBrowse ‚Äî open the current file on GitHub or other providers You can extend :GBrowse with plugins for: GitHub: vim-rhubarb GitLab: fugitive-gitlab.vim Others like Bitbucket, Azure DevOps, and Sourcehut ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:5:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Fugitive Cheat Sheet Here are some quick facts and commands that make vim-fugitive incredibly powerful: :Git with no arguments opens a summary/status window for the current repo :Gdiffsplit or :Gvdiffsplit opens staged vs working tree versions for side-by-side diff :Gread reverts local changes (undo-able with u) :Gwrite stages the file (or updates it from history, depending on context) :Git blame opens interactive blame mode ‚Äî press \u003cEnter\u003e on a line to view its commit, or g? to see available options :Gedit HEAD~3:% opens the current file as it existed 3 commits ago :Ggrep and :Glgrep perform Git-powered searches within the repository ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:6:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"File Management :GMove ‚Äî Perform a git mv on the file and rename the buffer :GRename ‚Äî Like :GMove, but destination is relative to current file :GDelete ‚Äî git rm + close buffer :GRemove ‚Äî git rm + keep buffer open ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:6:1","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"Web Integration Use :GBrowse to open the current file on your Git hosting provider. It even supports line ranges and works well in visual mode. Plugins exist for: GitHub: vim-rhubarb GitLab: fugitive-gitlab.vim Bitbucket, Gitee, Azure DevOps, and others also supported. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:6:2","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["git","programming","vim"],"content":"üèÅ Final Thoughts Vim Fugitive brings Git right into your fingertips, allowing you to manage version control without ever leaving your editor. Whether you‚Äôre staging, reviewing diffs, or digging into commit history, Fugitive streamlines your workflow and keeps your focus in code. ","date":"2025-04-13","objectID":"/20250413_vim_fugitive/:7:0","tags":["git","vim","fugitive"],"title":"Git with Vim Fugitive: A Streamlined Workflow","uri":"/20250413_vim_fugitive/"},{"categories":["NLP","LLM","Deep Learning"],"content":"A Gentle Guide to DeepSeek","date":"2025-02-14","objectID":"/20250214_deepseek_inside/","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"DeepSeek ‚Äôs latest moves have sent ripples through the AI community. Not only has it marked the beginning of a new era in artificial intelligence, but it has also made significant contributions to the open-source AI landscape. Their engineering techniques behind DeepSeek are truly impressive, and their reports are quite enjoyable. However, understanding their core ideas can be challenging and demands a substantial amount of effort. At the forefront of this innovation is DeepSeek-R1, a model that built upon the foundation established by preceding projects such as DeepSeek Coder, Math, MoE, and notably, the DeepSeek-V3 model. While DeepSeek-R1 is the center of the DeepSeek‚Äôs frenzy, its success is rooted on these past works. To help general readers navigate DeepSeek‚Äôs innovations more easily, I decided to write this post as a gentle introduction to their key components. I will begin by exploring the key ideas of V3 model, which serves as a cornerstone for DeepSeek-R1. I hope that this post will provide a clear and accessible explanation of their major contributions. Also, I strongly encourage you to read their reports :) ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:0:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Contents Multi-Head Latent Attention Quick Review of Multi-Head Attention Low-Rank Joint Compression Efficient Computation Without Explicit Key \u0026 Value Computation Decoupled RoPE Mixture-of-Experts in DeepSeek The Role of Shared Experts Group Relative Policy Optimization Proximal Policy Optimization GRPO: PPO for DeepSeek DeepSeek R1-Zero Conclusion Multi-Head Latent Attention ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:1:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Quick Review of Multi-Head Attention The query, key, and value in a standard multi-head attention (MHA) mechanism can be expressed as follows: \\begin{align*} \\mathbf{q}_t \u0026= W^{Q}\\mathbf{h}_t\\\\ \\mathbf{k}_t \u0026= W^{K}\\mathbf{h}_t\\\\ \\mathbf{v}_t \u0026= W^{V}\\mathbf{h}_t \\end{align*} $\\mathbf{q}_t,\\mathbf{k}_t,\\mathbf{v}_t\\in \\mathbb{R}^{d_hn_h}$ $\\mathbf{h}_t\\in \\mathbb{R}^{d}$: Attention input of the $t$-th token at an layer. $d_h$: the attention head‚Äôs dimension $n_h$: the number of attention heads During inference, all keys and values need to be cached to speed up computation. A cache requirement of MHA is roughly $2n_hd_hl$ elements per token (i.e., key, value for across all layers and heads). This heavy KV cache creates a major bottleneck, limiting the maximum batch size and sequence length during deployment. ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:1:1","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Low-Rank Joint Compression DeepSeek addresses this memory-intensive KV caching problem by introducing an alternative attention mechanism called Multi-Head Latent Attention (MLA). The core idea behind MLA is to compress keys and values into a low-dimensional latent space. Let‚Äôs break it down step by step: \\begin{align*} \\mathbf{c}_t^{KV} \u0026= W^{DKV}\\mathbf{h}_t\\\\ [\\mathbf{k}_{t,1}^C; \\mathbf{k}_{t,2}^C; \\dots ;\\mathbf{k}_{t,n_h}^C] = \\mathbf{k}_t^{C} \u0026= W^{UK}\\mathbf{c}_t^{KV}\\\\ \\mathbf{k}_t^{R} \u0026= \\text{RoPE}(W^{KR}\\mathbf{h}_t)\\\\ \\mathbf{k}_{t,i} \u0026= [\\mathbf{k}_{t,i}^C;\\mathbf{k}_t^{R}]\\\\ [\\mathbf{v}_{t,1}^C; \\mathbf{v}_{t,2}^C; \\dots ;\\mathbf{v}_{t,n_h}^C] = \\mathbf{v}_t^{C} \u0026= W^{UV}\\mathbf{c}_t^{KV} \\end{align*} The superscripts $D$ and $U$ denote the up- and down- projection, respectively. $\\mathbf{c}_t^{KV}\\in \\mathbb{R}^{d_c}$ is the compressed latent vector for keys and values, where $d_c\\ll d_hn_h$. Note that this is not a query vector. $W^{DKV}\\in \\mathbb{R}^{d_c\\times d}$ is the down-projection matrix that generates the latent vector $\\mathbf{c}_t^{KV}$. $W^{UK},W^{UV}\\in \\mathbb{R}^{d_hn_h\\times d_c}$ are the up-projection matrices for keys and values, respectively. These operations help reconstruct the compressed information of $\\mathbf{h}_t$. $W^{KR}\\in \\mathbb{R}^{d_h^R\\times d}$ is the matrix responsible for generating the positional embedding vector. I will explain it soon. Unlike standard attention mechanisms like Grouped-Query Attention (GQA) or Multi-Query Attention (MQA), MLA only needs to cache the compressed vector $\\mathbf{c}_t^{KV}$ during inference. MLA does not reduce the number of keys and values, allowing it to maintain the full representational power of self-attention while alleviating memory bottlenecks. The following figure provides an overview of KV-caching approaches in various attention mechanisms. Efficient Computation Without Explicit Key and Value Computation A key advantage of MLA is that it avoids explicit computation of the full-sized key and value matrices. Instead, attention scores can be computed as follows: \\begin{align*} q_t^Tk_t \u0026= (W^{UQ}c_t^Q)^T(W^{UK}c_t^{KV})\\\\ \u0026= (c_t^Q)^T(W^{UQT}W^{UK})c_t^{KV}, \\end{align*} where $W^{UQT}W^{UK}$ is a pre-computed matrix product of the two projection matrices. Similarly, for values: \\begin{align*} o_{t,i} = \\text{AttnScore}\\cdot v_t^C. \\end{align*} The final output is given by \\begin{align*} u_t \u0026= W^O[o_{t,1}, \\dots, o_{t,n_h}]\\\\ \u0026= W^O[\\text{AttnScore}\\cdot (W^{UV}c_t^{KV})]\\\\ \u0026= W^OW^{UV}[\\text{AttnScore}\\cdot (c_t^{KV})], \\end{align*} where $W^O\\in \\mathbb{R}^{d\\times d_hn_h}$ is the output projection matrix. Decoupled RoPE A potential issue with MLA is how to incorporate positional embeddings (PE). While sinusoidal positional embeddings are a straightforward option, research has shown that Rotary Positional Embedding (RoPE) tends to provide better performance. However, applying RoPE in MLA poses a challenge: normally, RoPE modifies keys and values directly, which would require explicit computation of keys (i.e, $\\mathbf{k}_t^{C}=W^{UK} c_t^{KV}$)‚Äîdefeating the MLA‚Äôs efficiency. DeepSeek circumvents this issue by introducing an explicit positional embedding vector $\\mathbf{k}_t^{R}$, called decoupled RoPE. The PE vector is separately broadcasted across the keys. This allows MLA to benefit from RoPE without losing the efficiency gains of its compression scheme. To further reduce activation memory during training, DeepSeek also compresses queries: \\begin{align*} \\mathbf{c}_t^{Q} \u0026= W^{DQ}\\mathbf{h}_t\\\\ [\\mathbf{q}_{t,1}^C; \\mathbf{q}_{t,2}^C; \\dots ;\\mathbf{q}_{t,n_h}^C] = \\mathbf{q}_t^{C} \u0026= W^{UQ}\\mathbf{c}_t^{Q}\\\\ [\\mathbf{q}_{t,1}^R; \\mathbf{q}_{t,2}^R; \\dots ;\\mathbf{q}_{t,n_h}^R] = \\mathbf{q}_t^{R} \u0026= \\text{RoPE}(W^{QR}\\mathbf{c}_t^Q)\\\\ \\mathbf{q}_{t,i} \u0026= [\\mathbf{q}_{t,i}^C;\\mathbf{q}_{t,i}^{R}] \\end{align*} $\\mathbf{c}_t^{Q}\\in \\mathbb{R}^{d_c‚Äô}$ is the compressed latent vector for queries, where $d_c‚Äô\\ll d_hn_h$ $W^{DQ}\\in \\mathbb","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:1:2","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"The Role of Shared Experts Unlike traditional MoE models, where all experts compete to process tokens, DeepSeek‚Äôs shared experts are always active. These experts are responsible for capturing and consolidating common knowledge across different input contexts. By concentrating general knowledge within shared experts, DeepSeek reduces redundancy among routed experts. This separation of responsibilities allows routed experts to focus more effectively on specialized tasks, leading to better model efficiency and performance. Group Relative Policy Optimization DeepSeek introduces a reinforcement learning (RL) algorithm called Group Relative Policy Optimization (GRPO), a simple variant of Proximal Policy Optimization (PPO). If you have a basic understanding of RL and PPO, you‚Äôll find GRPO quite straightforward. Let‚Äôs first review PPO. ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:2:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"Proximal Policy Optimization PPO was proposed to address the instability of the vanilla policy gradient algorithm (i.e., REINFORCE algorithm). The core idea of PPO is to stabilize the policy update process by restricting the amount of update. The objective function of PPO is given by \\begin{align*} \\theta_{k+1} = \\operatorname{argmax}_{\\theta} \\mathbb{E}_{s,a\\sim \\pi_{\\theta_k}} [L(s, a, \\theta_k, \\theta)], \\end{align*} where $\\theta_{k}$ is a parameter of a policy network at $k$-th step, $\\theta$ is the current policy we want to update, and the $A$ is the advantage (\\i.e., reward). Finally, the loss function $L$ is given by \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text {k}}}\\left(a | s\\right)} A^{\\pi_{\\theta_k}}(s,a), \\text{ Clip}\\Bigg(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1-\\varepsilon, 1+\\varepsilon\\Bigg) A^{\\pi_{\\theta_k}}(s,a)\\right). \\end{align*} Roughly, $\\varepsilon$ is a hyperparameter which says how far away the new policy is allowed to go from the old one. A simpler expression of the above expression is \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text {k}}}\\left(a | s\\right)} A^{\\pi_{\\theta_k}}(s,a), g(\\varepsilon, A^{\\pi_{\\theta_k}}(s,a)) \\right), \\end{align*} where \\begin{align*} g(\\varepsilon,A) = \\begin{cases} (1+\\varepsilon)A \u0026 A\\geq 0\\\\ (1-\\varepsilon)A \u0026 A\u003c 0. \\end{cases} \\end{align*} There are two cases: $A\\geq 0$: The advantage for that state-action pair is positive, in which case its contribution to the objective reduces to \\begin{align*} L(s, a, \\theta_k, \\theta) = \\min \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1+\\varepsilon \\right) A^{\\pi_{\\theta_k}}(s,a). \\end{align*} Since the advantage is positive, the objective function increases if the action $a$ becomes likely under the policy (i.e., $\\pi_{\\theta}(a|s)$ increases). However, the $\\min$ operator limits how much the objective can increase. Once \\begin{align*} \\pi_{\\theta}(a|s) \u003e (1+\\epsilon) \\pi_{\\theta_k}(a|s), \\end{align*} the $\\min$ ensures that the term is capped at $(1+\\epsilon) A^{\\pi_{\\theta_k}}(s,a)$. This prevents the new policy from straying too far from the old policy. $A\u003c0$: The advantage for that state-action pair is negative, its contribution is given by \\begin{align*} L(s, a, \\theta_k, \\theta) = \\max \\left(\\frac{\\pi_{\\theta}\\left(a | s\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(a | s\\right)}, 1-\\varepsilon \\right) A^{\\pi_{\\theta_k}}(s,a). \\end{align*} Since the advantage is negative, the objective function increases if the action $a$ becomes less likely (i.e., $\\pi_{\\theta}(a|s)$ decreases). The $\\max$ operator limits this increase. Once \\begin{align*} \\pi_{\\theta}(a|s) \u003c (1-\\varepsilon) \\pi_{\\theta_k}(a|s), \\end{align*} the $\\max$ caps the term at $(1-\\varepsilon) A^{\\pi_{\\theta_k}}(s,a)$, again, ensuring that the new policy does not deviate too far from the old policy. In sum, clipping serves as a regularizer by restricting the rewards to the policy, which change it dramatically with the hyperparameter $\\varepsilon$ corresponds to how far away the new policy can go from the old while still profiting the objective. ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:3:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"GRPO: PPO for DeepSeek GRPO can be expressed as follows: \\begin{align*} \\mathcal{J} = \\frac{1}{G}\\sum_{i=1}^{G} \\min \\left(\\frac{\\pi_{\\theta}\\left(o_i | q\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(o_i | q\\right)} A_i, \\text{ Clip}\\left(\\frac{\\pi_{\\theta}\\left(o_i | q\\right)}{\\pi_{\\theta_{\\text{k}}}\\left(o_i | q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right) -\\beta D_{KL}(\\pi_{\\theta}| \\pi_{\\text{ref}}). \\end{align*} GRPO first samples a group of outputs ${o_1, o_2,\\dots, o_G }$ from the old policy $\\pi_{\\theta_k}$ Then it optimizes the policy model by maximizing the objective The KL-divergence is used to restrict the sudden change of policy. The advantage can be calculated by averaging and normalizing the rewards. Instead of using a value model explicitly, GRPO computes a value of the states (i.e., $o_i$) by averaging them. Note that $A(s,a) = Q(s,a)-V(s)$ $\\pi_{\\text{ref}}$ is the reference model, which is an initial SFT model. DeepSeek-R1 DeepSeek-R1 is essentially a large language model fine-tuned using reinforcement learning. The process begins with training the DeepSeek-V3 model using the GRPO technique described earlier. Before applying RL, the model is pre-tuned with a small, carefully curated set of warm-up data designed to encourage logical outputs in a Chain-of-Thought (CoT) format. This preliminary step significantly improves training stability. Interestingly, DeepSeek‚Äôs researchers first experimented with pure RL training‚Äîwithout any supervised signals‚Äîwhich led to the creation of DeepSeek-R1-Zero. Here are some observations from that experiment and my opinions. ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:4:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["NLP","LLM","Deep Learning"],"content":"DeepSeek R1-Zero To train DeepSeek-R1-Zero, a rule-based reward signal was adopted. Two types of rewards are used: Accuracy rewards: The reward model evaluates whether the response is correct. For example, in math problems with deterministic results, the model is required to provide the final answer in a specified format, enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. Format rewards: In addition to the accuracy reward model, they employed a format reward model that enforces the model to put its thinking process between \u003cthink\u003e and \u003c/think\u003e tags. Notably, DeepSeek-R1 does not rely on a neural reward model‚Äîlikely because neural models may not consistently provide reliable rewards for training. The team also reported an intriguing ‚Äúaha moment‚Äù with DeepSeek-R1-Zero: ‚ÄúDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.‚Äù However, such behavior appears infrequently. More often, DeepSeek-R1-Zero tends to generate gibberish outputs, which may be attributed to the inherently unstable nature of RL training. Conclusion In this post, I‚Äôve introduced some of the core ideas behind the DeepSeek-v3 and R1 models. While their deep learning techniques are undoubtedly interesting, I find their hardware-level engineering particularly more impressive. In my opinion, their success lies in these engineering achievements, and I look forward to exploring this aspect in a forthcoming post. ","date":"2025-02-14","objectID":"/20250214_deepseek_inside/:5:0","tags":["DeepSeek","LLM","Deep Learning"],"title":"Inside DeepSeek-R1","uri":"/20250214_deepseek_inside/"},{"categories":["python","programming"],"content":"Tutorial for Python's abstract classes and protocols","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"Introduction When it comes to writing clean, maintainable, and scalable Python code, design matters. As your projects grow, you‚Äôll often find yourself needing to enforce structure, ensure consistency, and promote reusability. This is where Python‚Äôs Abstract Base Classes (ABCs) and Protocols come into play‚Äîtwo powerful features that help you design better software. Abstract classes act as blueprints for other classes, allowing you to define methods that must be implemented by any subclass. They‚Äôre typically used for creating a shared foundation while enforcing a specific structure. Protocols, on the other hand, take a more flexible approach. Instead of relying on inheritance, they let you define interfaces based on behavior, making them ideal for duck typing (or structural subtyping) and runtime flexibility. However, when should you use abstract classes, and when are protocols the better choice? How do these concepts differ, and what problems do they solve? In this blog post, we‚Äôll explore these questions in detail. Through clear explanations and practical examples, you‚Äôll learn how to leverage abstract classes and protocols to write more elegant and maintainable Python code. Whether you‚Äôre designing a small script or a large-scale application, these tools will help you take your Python skills to the next level. Let‚Äôs dive in! What are Abstract Base Classes An abstract class in Python is a class that cannot be instantiated on its own and is designed to be a blueprint for other classes. It allows you to define methods that must be created within any child classes built from the abstract class. Abstract classes are used primarily in situations where a base class is required to define a common interface for a set of derived classes. Consistency: Ensures that all subclasses implement certain methods, providing a consistent interface. Documentation: Serves as a form of documentation by clearly specifying which methods need to be implemented. Design: Helps in designing a robust architecture by defining a template for subclasses. ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:0:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"Pure ABCs (ABC as Interface) The simplest way to use an ABC is as a pure ABC, for example: from abc import ABC, abstractmethod class Animal(ABC): @abstractmethod def walk(self) -\u003e None: pass @abstractmethod def speak(self) -\u003e None: pass Here we have defined an ABC Animal with two methods: walk and speak. Note that the way to do this is to subclass ABC and to decorate the methods that must be implemented (i.e. part of the ‚Äúinterface‚Äù) with the @abstractmethod decorator. Now we can implement this ‚Äúinterface‚Äù to create a Dog class Dog(Animal): def walk(self) -\u003e None: print(\"This is a dog walking\") def speak(self) -\u003e None: print(\"Woof!\") This works as expected. However, if we forget to implement the speak method, Python will raise an error when we try to instantiate the class: \u003e\u003e\u003e dog = Dog() TypeError: Can't instantiate abstract class Dog with abstract method speak We can see that we get an error because we haven‚Äôt implemented the abstract method speak. This ensures that all subclasses implement the correct ‚Äúinterface‚Äù. ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:1:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"ABCs as a tool for code reuse Another, and probably more common, use case for ABCs is for code reuse. Below is a slightly more realistic example of a base class for a statistical or Machine Learning regression model from abc import ABC, abstractmethod from typing import List, TypeVar import numpy as np T = TypeVar(\"T\", bound=\"Model\") class Model(ABC): def __init__(self): self._is_fitted = False def fit(self: T, data: np.ndarray, target: np.ndarray) -\u003e T: fitted_model = self._fit(data, target) self._is_fitted = True return fitted_model def predict(self, data: np.ndarray) -\u003e List[float]: if not self._is_fitted: raise ValueError(f\"{self.__class__.__name__} must be fit before calling predict\") return self._predict(data) @property def is_fitted(self) -\u003e bool: return self._is_fitted @abstractmethod def _fit(self: T, data: np.ndarray, target: np.ndarray) -\u003e T: pass @abstractmethod def _predict(self, data: np.ndarray) -\u003e List[float]: pass In this example, the Model class provides a reusable structure for fitting and predicting data, while leaving the implementation of _fit and _predict to subclasses. Protocols ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:2:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"Dynamic v.s. Static Typing To better understand protocols, it‚Äôs important to first grasp the concept of typing in Python. Python is a dynamically typed language. What does that mean? In Python, type declarations are not required. For example, you can define a function without specifying the types of its arguments or its return type: def simple_function(a, b): return a + b Types are handled and checked at runtime. You can call simple_function with integers, floats, or a mix of both, and the return type will depend on the input: result = simple_function(2, 8) # type(result) -\u003e int result = simple_function(1.4, 9) # type(result) -\u003e float Compare this to a statically typed language like C, where type declarations are mandatory: int simple_function(int a, int b) { return a + b; } In C, providing any other type would result in a compilation error. For example, the following code would not compile: int result = simple_function(2.2, 9); This highlights a key benefit of statically typed languages: types are checked at compile time, so you‚Äôre less likely to encounter type-related issues at runtime. In Python, however, you might run into type-related errors at runtime, which could have been caught earlier in a statically typed language. On the flip side, dynamically typed languages like Python offer greater flexibility when it comes to the types they accept. They also eliminate the need for explicit type declarations, which can be a boon for productivity. ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:3:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"Duck Typing Dynamic typing is often referred to as duck typing, a concept captured by the saying: If it walks like a duck and it quacks like a duck, then it must be a duck. In programming terms, this means that if an object behaves like a certain type (i.e., it has the required methods and attributes), it can be treated as that type. Protocols embrace this concept, allowing you to define interfaces based on behavior rather than explicit inheritance. from typing import Protocol class Flyer(Protocol): def fly(self) -\u003e None: ... def let_it_fly(entity: Flyer) -\u003e None: entity.fly() class Bird: def fly(self) -\u003e None: print(\"Bird is flying\") class Airplane: def fly(self) -\u003e None: print(\"Airplane is flying\") bird = Bird() airplane = Airplane() # Both Bird and Airplane instances can be passed to let_it_fly, thanks to Duck Typing let_it_fly(bird) # Output: Bird is flying let_it_fly(airplane) # Output: Airplane is flying In this example, both Bird and Airplane implement the flay method, so they can be treated as instances of the Flyer protocol. This flexibility is one of the key strengths of duck typing and protocols in Python. ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:4:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["python","programming"],"content":"So ABCs or Protocols? In summary, here are the best practices for choosing between ABCs and Protocols: When to use abstract classes: When you need to share common implementation code. When you want to enforce a strict class hierarchy. When to use protocols: When you care about behavior, not implementation. When you want to avoid tight coupling between classes. Avoid overusing inheritance; prefer composition where possible. ","date":"2025-01-28","objectID":"/20250128_python_protocol_abstract_classes/:5:0","tags":["python","ABCs","Protocols"],"title":"Abstract Classes or Protocols","uri":"/20250128_python_protocol_abstract_classes/"},{"categories":["programming","linux"],"content":"Setting Up DL/ML Environments","date":"2025-01-11","objectID":"/20250111_pytorch_container/","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Setting Up DL Experiment Environments ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:0:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"A Challenge for Arch Linux Users If you‚Äôve ever tried to set up a new experiment environment for deep learning on Arch Linux, you‚Äôre probably familiar with the challenges involved. Arch Linux, renowned for its rolling-release model and cutting-edge updates, provides unparalleled flexibility and control over your system. However, this same flexibility can often lead to headaches when setting up complex environments for machine learning or deep learning experiments. Dependency conflicts, missing libraries, and version mismatches are all too common. One particular pain point is the setup of Python environments using tools like Conda or virtual environments. While these tools work seamlessly on many systems, Arch Linux users often encounter version conflicts. Installing Conda itself can be tricky and painful. For researchers and developers, this process can feel like a significant barrier to productivity. Instead of focusing on model development or data analysis, hours are spent troubleshooting environment issues. On Arch Linux, where package versions are always at the bleeding edge, finding compatibility between your system and the tools required by frameworks like PyTorch can be very challenging. This is where Docker steps in to save the day. By using Docker containers, you can create isolated, portable environments that encapsulate all the dependencies you need, regardless of the host operating system. For PyTorch users who rely on a CPU-only setup for studying DL/ML and testing PyTorch code, Docker offers a streamlined solution to avoid the usual hassle of configuring local environments on Arch Linux. In this blog post, I will go over the process of setting up a PyTorch container using Docker, exploring how it simplifies the creation of a reproducible environment for your deep learning experiments. ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:1:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Install Docker on Arch sudo pacman -S docker docker-compose docker-buildx Then, sudo systemctl enable --now docker.service ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:2:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"PyTorch Container ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:3:0","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","linux"],"content":"Steps for Using PyTorch with CPU-Only Docker Image Pull the CPU-Only PyTorch Docker Image The latest PyTorch images without CUDA can be pulled using the following command: docker pull pytorch/pytorch:latest If you prefer a specific version, for example, PyTorch 1.13.1, use: docker pull pytorch/pytorch:1.13.1-cpu Run the PyTorch Container Start the container interactively: docker run -it --rm pytorch/pytorch:latest This will launch a Python environment with PyTorch installed. Mount Your Project Files (Optional) If you want to access your local project files inside the container, mount a directory: docker run -it --rm -v $(pwd):/workspace pytorch/pytorch:latest -it -i: Keeps STDIN open, allowing you to interact with the container (important for running interactive shells or REPLs). -t: Allocates a pseudo-TTY, which is useful for interactive sessions. --rm: This flag automatically removes the container when it stops. It‚Äôs useful to avoid cluttering your system with stopped containers. -v $(pwd):/workspace: This is the volume flag for mounting a directory. Here‚Äôs how it works: $(pwd) refers to the current working directory on your host machine (outside the container). /workspace is the directory inside the container where the mounted files will be accessible. The files inside /workspace in the container are directly linked to the files in your host machine‚Äôs current directory. If you modify a script in your host machine, the changes will be visible inside the container immediately. Similarly, if you create or edit a file inside the /workspace directory in the container, the changes will reflect on your host machine. Install Additional Python Libraries (If Needed) Install any extra libraries required for your project: pip install \u003cpackage-name\u003e To save this setup for future use, create a custom Docker image with these dependencies pre-installed. Write and Test PyTorch Code Create a simple PyTorch script (e.g., test.py): import torch # Check if CUDA is available print(\"CUDA Available:\", torch.cuda.is_available()) # Perform a simple tensor operation x = torch.tensor([5.0, 10.0, 15.0]) print(\"Tensor:\", x) Run it inside the container: python test.py The output should confirm that CUDA is not available and display the tensor. Save and Exit To persist changes, save your code in the mounted directory (e.g., /workspace). You can also commit the container if you‚Äôve made extensive modifications: exit docker ps -a # Find the container ID docker commit \u003ccontainer_id\u003e my_pytorch_cpu_image Note that the container would not appear if you run the container with --rm. ","date":"2025-01-11","objectID":"/20250111_pytorch_container/:3:1","tags":["pytorch","arch","container","docker"],"title":"Run Pytorch Container in Arch Linux","uri":"/20250111_pytorch_container/"},{"categories":["programming","python"],"content":"Introduction to asyncio library","date":"2025-01-05","objectID":"/20250105_asyncio/","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"For the past few months, I‚Äôve been working on an exciting internal project at my company: taking users‚Äô documents and running them through LLM APIs to translate and summarize their content, somewhat similar to DeepL . The output is a collection of translated documents, each overlaid with the newly translated text. Our goal is to provide a stable service that can handle large files efficiently for thousands of employees at Samsung‚Äîno small task! To achieve this, we needed a concurrency strategy that supports high throughput while remaining responsive. That‚Äôs where Asyncio comes in. In this post, we‚Äôll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We‚Äôll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle large-scale I/O more effectively. Whether you‚Äôre building an internal service for employees or creating a high-performance web server, Asyncio‚Äôs approach to concurrency might just be the key to unlocking the scalability you need. A Deep Dive into Asynchronous I/O Modern software frequently needs to handle large volumes of input/output (I/O) operations. For instance, you might be retrieving data from web services, communicating with microservices over a network, or running multiple database queries simultaneously. These tasks can often take hundreds of milliseconds‚Äîor even seconds‚Äîif the network is under heavy load or the database is busy. If you approach these operations in a strictly synchronous manner‚Äîdoing one after another‚Äîeach I/O request can block the execution of your entire application. When you have many such requests, total execution time can balloon significantly. Picture having to process 100 requests, each taking 1 second. Doing that sequentially results in a 100-second runtime. However, if you can handle them concurrently, you might complete all in roughly the same amount of time as a single request. In this post, we‚Äôll look at how Python tackles concurrency through Asyncio, a library designed to handle asynchronous I/O. We‚Äôll explore the concepts of concurrency, parallelism, multitasking, the difference between I/O-bound and CPU-bound tasks, and finally see how Asyncio harnesses cooperative multitasking to help your applications handle I/O more efficiently. ","date":"2025-01-05","objectID":"/20250105_asyncio/:0:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Why Concurrency Matters ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Synchronous Bottleneck In a synchronous application, each line of code must complete before moving on to the next. This is usually acceptable for simple tasks but becomes problematic if a single operation is slow or unresponsive. While any operation can block an application, many applications will be stuck waiting for I/O. I/O refers to a computer‚Äôs input and output devices such as a keyboard, hard drive, and, most commonly, a network card. A classic example is a web server that processes each request in series; if one request takes longer than expected, all subsequent requests are delayed. Users of a slow website or client application may experience hang-ups, timeouts, or sluggish responsiveness due to this ‚Äúqueue‚Äù of operations. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Concurrency as a Solution Concurrency allows multiple tasks to be in progress simultaneously. In code terms, this often means starting multiple operations and then efficiently switching between them, so the application doesn‚Äôt grind to a halt waiting on just one task. For I/O-bound tasks, concurrency can provide remarkable speedups because while one operation is waiting on a response, your program can continue working on other tasks. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Concurrency vs. Parallelism It‚Äôs important to distinguish concurrency from parallelism: Concurrency means you can have multiple tasks in progress at once, but they are not necessarily all running at the exact same moment. Parallelism means two or more tasks truly run at the same time, which requires at least as many CPU cores as the number of tasks you want to run in parallel. Even on a single-core machine, you can achieve concurrency by rapidly switching (or time slicing) between tasks. However, true parallelism requires multiple CPU cores, letting tasks run simultaneously without interrupting each other. ","date":"2025-01-05","objectID":"/20250105_asyncio/:1:3","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The I/O-Bound vs. CPU-Bound Distinction When we label a particular operation as I/O-bound or CPU-bound, we‚Äôre describing what fundamentally limits its performance. I/O-Bound: The operation spends most of its time waiting for I/O devices such as hard drives or network interfaces. Examples include fetching a remote web page, reading from a file, or waiting on a database query. These tasks can benefit significantly from concurrency, because while one operation waits, the program can do other work. CPU-Bound: The task is primarily gated by processor speed. Examples include computing the nth Fibonacci number using a recursive function, performing complex data analysis, or running CPU-intensive algorithms. Concurrency alone may not help here, especially in Python, because of the Global Interpreter Lock (GIL). ","date":"2025-01-05","objectID":"/20250105_asyncio/:2:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"A Primer on Processes, Threads, and the GIL ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Processes A process is a running instance of an application with its own memory space. An example of creating a Python process would be running a simple ‚Äúhello world‚Äù application or typing python at the command line to start up the REPL (read eval print loop). Modern operating systems allow multiple processes to run at once. If your CPU has multiple cores, it can execute processes truly in parallel. Otherwise, the OS uses time slicing to rapidly switch among processes. ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Threads A thread is a more lightweight form of concurrency that runs within a single process, sharing the parent process‚Äôs memory space. Threads have no their own memory. A process will always have at least one thread associated with it, usually known as the main thread. A process can also create other threads, which are more commonly known as worker or background threads. These threads can perform other work concurrently alongside the main thread. Threads, much like processes, can run alongside one another on a multi-core CPU, and the operating system can also switch between them via time slicing. When we run a normal Python application, we create a process as well as a main thread that will be responsible for running our Python application. import os import threading print(f'Python process with process id: {os.getpid()}') num_threads = threading.active_count() thread_name = threading.current_thread().name print(f'{num_threads} thread(s) are running') print(f'The current thread is {thread_name}') Multithreading in Python You might assume that starting multiple threads automatically takes advantage of multi-core systems. However, Python has a key constraint called the Global Interpreter Lock (GIL). The GIL ensures that only one thread can run one Python instruction at a time. This means that even on a multi-core machine, your Python code cannot run more than one CPU-bound thread simultaneously within the same process. So, are threads useless in Python? Far from it. Threads do provide genuine concurrency for I/O-bound tasks because Python releases the GIL during I/O operations. This allows you to overlap network calls, file reads, etc., effectively improving throughput. Yet for CPU-bound tasks, you won‚Äôt get true parallelism using just threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:3:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Global Interpreter Lock (GIL) in More Detail The Global Interpreter Lock is often regarded as a tricky limitation in Python. At a high level, the GIL: Prevents multiple native threads from executing Python bytecode simultaneously. Releases the lock when code interacts with the operating system for I/O (e.g., network or disk). Reacquires the lock once I/O completes and Python bytecode needs to be executed again. Why does it exist? The main reason is memory safety in the CPython implementation, which relies heavily on reference counting to manage objects. While convenient, reference counting can become unsafe when multiple threads mutate the same objects without careful synchronization. For I/O-bound code‚Äîlike sending concurrent HTTP requests‚Äîthis arrangement works well. You start multiple threads, each waiting on different I/O operations, and the GIL is periodically released while those operations happen, giving an overall speedup. For CPU-bound tasks‚Äîlike computing Fibonacci numbers with a naive recursion‚Äîthreads won‚Äôt help much because the lock is rarely released. Instead, you might use multiprocessing or specialized libraries that bypass the GIL for compute-intensive work. ","date":"2025-01-05","objectID":"/20250105_asyncio/:4:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Enter Asyncio: Asynchronous I/O in a Single Thread Asyncio is Python‚Äôs built-in library (introduced in Python 3.4 and improved in Python 3.5 with the async and await keywords) that focuses on concurrent I/O without the overhead of managing threads or processes. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Coroutines The foundation of Asyncio is the concept of a coroutine‚Äîa special function that can pause itself (await) while waiting for an I/O operation, and then resume right where it left off once the operation completes. While one coroutine is waiting, other coroutines can continue running, effectively achieving concurrency within a single thread. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Event Loop At the core of every Asyncio program is the event loop. Think of it as a manager that schedules coroutines. The event loop steps through coroutines one by one: A coroutine starts running until it hits an await for an I/O operation. The coroutine ‚Äúpauses,‚Äù returning control to the event loop. The event loop checks if there‚Äôs another coroutine ready to run. If so, it switches to that coroutine immediately. Meanwhile, the operating system handles the actual I/O. Once the I/O is ready (e.g., the network has responded), the event loop ‚Äúwakes up‚Äù the paused coroutine and resumes its execution. Because only one thread is responsible for executing Python code, the GIL is never contended between multiple threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:5:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Where Asyncio Shines‚Äîand Where It Doesn‚Äôt ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"The Sweet Spot: I/O-Bound Work Asyncio is incredibly useful when you‚Äôre dealing with a large number of concurrent I/O operations. Common examples include: Building high-performance web servers that handle thousands of simultaneous connections. Writing web scrapers that fetch and parse dozens or hundreds of pages concurrently. Coordinating multiple microservice requests in a single workflow without blocking. In these scenarios, Asyncio‚Äôs single-threaded event loop can handle many I/O-bound coroutines, each pausing when it must wait for data. This often results in a dramatic improvement in throughput compared to a purely synchronous approach. ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Handling CPU-Bound Work What if your task is mainly compute-heavy? Asyncio won‚Äôt magically run CPU-bound code in parallel because the GIL still applies to Python bytecode, and you‚Äôre still on a single thread. For CPU-bound tasks‚Äîlike image processing, machine learning, or large-scale data transformations‚Äîyou‚Äôd likely want to offload work to another process or leverage special libraries that release the GIL. That said, Asyncio does provide interoperability with threading and multiprocessing; you can combine CPU-intensive tasks with your I/O-bound coroutines. For instance, you can use asyncio.to_thread (in Python 3.9+) to run a CPU-bound function in a separate thread or harness a process pool executor for true parallelism at the CPU level. ","date":"2025-01-05","objectID":"/20250105_asyncio/:6:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Putting It All Together: An Example Below is a simplified comparison of synchronous, multithreaded, and Asyncio-based approaches to fetching two web pages: ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:0","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Synchronous Approach import time import requests def fetch_example(): response = requests.get('https://www.example.com') return response.status_code def sync_fetch(): start = time.time() status_one = fetch_example() status_two = fetch_example() end = time.time() print(f\"Synchronous: {status_one}, {status_two}, time={end - start:.4f}s\") if __name__ == \"__main__\": sync_fetch() Synchronous mode fetches one URL at a time. If each request blocks for one second, the total time is roughly two seconds. Here, Python will release the GIL while waiting for the network, letting both threads run concurrently. The total time is potentially cut almost in half, assuming the responses come back quickly. ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:1","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["programming","python"],"content":"Asyncio Approach import time import asyncio import aiohttp async def async_fetch_example(): async with aiohttp.ClientSession() as session: async with session.get('https://www.example.com') as response: status = response.status print(status) async def main(): start = time.time() await asyncio.gather(async_fetch_example(), async_fetch_example()) end = time.time() print(f\"Asyncio: time={end - start:.4f}s\") if __name__ == \"__main__\": asyncio.run(main()) With Asyncio, both fetch operations are initiated concurrently in the same thread, with the event loop switching between them whenever one is waiting for I/O. Like multithreading, you should see a meaningful speedup compared to the synchronous approach‚Äîbut without the complexities of shared data across threads. ","date":"2025-01-05","objectID":"/20250105_asyncio/:7:2","tags":["python","asyncio"],"title":"Asyncio in Python: A Deep Dive into Asynchronous I/O","uri":"/20250105_asyncio/"},{"categories":["linux"],"content":"How to install Arch Linux","date":"2025-01-05","objectID":"/20250105_arch/","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"I recently bought a mini PC because I wanted a lightweight machine that I can easily carry anywhere. Arch Linux‚Äôs minimalistic, rolling-release approach aligns perfectly with my love for a Vim-based workflow and a highly customizable setup. While the process can seem intimidating at first, it‚Äôs an incredibly rewarding experience that offers complete control over your system. Installing Arch Linux (UEFI or BIOS) Arch Linux is well-known for giving users full control over their system. This guide walks you through a fresh Arch Linux installation. While it is detailed, always refer to the official Arch Wiki for up-to-date information. ","date":"2025-01-05","objectID":"/20250105_arch/:0:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Contents Prerequisites Creating a Bootable USB Initial Setup Check UEFI or BIOS Wi-Fi Connection Check Internet Time \u0026 NTP Partitioning Using fdisk or cfdisk BIOS Partition Scheme UEFI Partition Scheme Formatting and Mounting Creating File Systems Mounting Partitions Installing the Base System Fast Mirror Selection pacstrap / basestrap Generating fstab Chroot Configuration Setting up Network Manager Installing and Configuring GRUB Root Password Locale and Timezone Hostname Final Steps Post-Installation Creating a New User Sudoers Configuration Installing X.org and a Window Manager Fonts Enabling a Display Manager (Optional) Sound Setup Installing Yay (AUR Helper) ","date":"2025-01-05","objectID":"/20250105_arch/:1:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Prerequisites A working internet connection on another device (in case you need help or to follow the Arch Wiki). A USB drive of at least 2‚ÄØGB capacity. Familiarity with the command line. Important: Installing Arch Linux involves formatting drives, which is destructive. Back up all important data before proceeding. ","date":"2025-01-05","objectID":"/20250105_arch/:2:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating a Bootable USB ","date":"2025-01-05","objectID":"/20250105_arch/:3:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"On Linux sudo dd if=\u003cpath-to-arch-iso\u003e of=\u003cpath-to-usb\u003e status=progress if = input file (the ISO file). of = output file (usually something like /dev/sdb). Be sure to confirm the correct USB path using lsblk or fdisk -l before running the command. ","date":"2025-01-05","objectID":"/20250105_arch/:3:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"On Windows Use Rufus . It‚Äôs a straightforward tool that avoids many potential pitfalls. ","date":"2025-01-05","objectID":"/20250105_arch/:3:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Initial Setup Boot from your USB and select the Arch Linux USB in your system‚Äôs boot menu. You should see a command-line shell once Arch boots. ","date":"2025-01-05","objectID":"/20250105_arch/:4:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Check UEFI or BIOS ls /sys/firmware/efi/efivars If you get an error, you‚Äôre in BIOS (Legacy) mode. If you see contents, you‚Äôre in UEFI mode. ","date":"2025-01-05","objectID":"/20250105_arch/:4:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Wi-Fi Connection If you‚Äôre on Wi-Fi, use iwctl: iwctl device list station \u003cwlan\u003e scan station \u003cwlan\u003e get-networks station \u003cwlan\u003e connect \u003cwifi-name\u003e station \u003cwlan\u003e show exit ","date":"2025-01-05","objectID":"/20250105_arch/:4:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Check Internet ping google.com If you have no connection, re-check your Wi-Fi settings or use a wired connection. ","date":"2025-01-05","objectID":"/20250105_arch/:4:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Time \u0026 NTP timedatectl set-ntp true This ensures your system clock stays synchronized. ","date":"2025-01-05","objectID":"/20250105_arch/:4:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Partitioning ","date":"2025-01-05","objectID":"/20250105_arch/:5:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Using fdisk or cfdisk Identify target disk: lsblk Open the disk utility: fdisk /dev/sda or cfdisk /dev/sda ","date":"2025-01-05","objectID":"/20250105_arch/:5:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"BIOS Partition Scheme A common layout might be: Boot: +200M Swap: typically 50% of your RAM size (+8G for 16‚ÄØGB RAM) Root: at least +25G Home: rest of the disk space Press w to write changes and exit. ","date":"2025-01-05","objectID":"/20250105_arch/:5:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"UEFI Partition Scheme EFI: around +550M and formatted as FAT32. Swap: 50% of RAM or as needed. Root: Minimum +25G or more. Home: Rest of the disk (if desired on a separate partition). ","date":"2025-01-05","objectID":"/20250105_arch/:5:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Formatting and Mounting ","date":"2025-01-05","objectID":"/20250105_arch/:6:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating File Systems Example commands (adjust partitions to suit your layout): mkfs.ext4 /dev/sda1 # For /boot or /root or /home mkfs.fat -F32 /dev/sda1 # For UEFI partition if using UEFI mkswap /dev/sda2 # Swap partition ","date":"2025-01-05","objectID":"/20250105_arch/:6:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Mounting Partitions Swap: swapon /dev/sda2 Root: mount /dev/sda3 /mnt Boot (UEFI): mkdir /mnt/boot mount /dev/sda1 /mnt/boot Home (if separate): mkdir /mnt/home mount /dev/sda4 /mnt/home ","date":"2025-01-05","objectID":"/20250105_arch/:6:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing the Base System ","date":"2025-01-05","objectID":"/20250105_arch/:7:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Fast Mirror Selection Choose the fastest mirrors by editing /etc/pacman.d/mirrorlist. Move the closest/fastest mirrors to the top. This significantly speeds up package downloads. ","date":"2025-01-05","objectID":"/20250105_arch/:7:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"pacstrap / basestrap For Arch Linux: pacstrap /mnt base base-devel linux linux-firmware vim git For Artix Linux (example): basestrap -i /mnt base base-devel runit elogind-runit linux linux-firmware \\ grub networkmanager networkmanager-runit cryptsetup lvm2 lvm2-runit neovim vim ","date":"2025-01-05","objectID":"/20250105_arch/:7:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Generating fstab genfstab -U /mnt \u003e\u003e /mnt/etc/fstab Check the file to ensure correct entries: vim /mnt/etc/fstab ","date":"2025-01-05","objectID":"/20250105_arch/:7:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Chroot Now ‚Äúenter‚Äù the new system: Arch: arch-chroot /mnt Artix: artix-chroot /mnt bash ","date":"2025-01-05","objectID":"/20250105_arch/:7:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Configuration ","date":"2025-01-05","objectID":"/20250105_arch/:8:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Setting up Network Manager pacman -S networkmanager systemctl enable NetworkManager (In Artix, you would enable the corresponding runit service instead.) ","date":"2025-01-05","objectID":"/20250105_arch/:8:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing and Configuring GRUB For BIOS pacman -S grub grub-install --target=i386-pc /dev/sda grub-mkconfig -o /boot/grub/grub.cfg For UEFI pacman -S grub efibootmgr grub-install --target=x86_64-efi --efi-directory=/boot grub-mkconfig -o /boot/grub/grub.cfg ","date":"2025-01-05","objectID":"/20250105_arch/:8:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Root Password passwd Set a strong password for the root user. ","date":"2025-01-05","objectID":"/20250105_arch/:8:3","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Locale and Timezone Edit /etc/locale.gen and uncomment your locale lines (e.g., en_US.UTF-8). Generate them: locale-gen Create /etc/locale.conf: echo \"LANG=en_US.UTF-8\" \u003e /etc/locale.conf Set your timezone: ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # or use tzselect ","date":"2025-01-05","objectID":"/20250105_arch/:8:4","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Hostname echo \"myhostname\" \u003e /etc/hostname ","date":"2025-01-05","objectID":"/20250105_arch/:8:5","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Final Steps Exit the chroot environment: exit umount -R /mnt reboot Remove your USB before booting, and the system should start from the newly installed Arch Linux. ","date":"2025-01-05","objectID":"/20250105_arch/:8:6","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Post-Installation ","date":"2025-01-05","objectID":"/20250105_arch/:9:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Creating a New User Log in as root. Create a user: useradd -m -g wheel \u003cusername\u003e passwd han Add additional groups if needed: usermod -aG audio,video,storage han ","date":"2025-01-05","objectID":"/20250105_arch/:9:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Sudoers Configuration Edit /etc/sudoers: visudo Add or uncomment a line to allow members of the wheel group to use sudo: %wheel ALL=(ALL:ALL) ALL ","date":"2025-01-05","objectID":"/20250105_arch/:9:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing X.org and a Window Manager Install Xorg: pacman -S xorg-server xorg-xinit Minimal Window Manager: pacman -S i3 dmenu rxvt-unicode Start X (for testing): startx For an automated start, add exec i3 in your ~/.xinitrc. ","date":"2025-01-05","objectID":"/20250105_arch/:10:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Fonts sudo pacman -S noto-fonts noto-fonts-cjk noto-fonts-emoji noto-fonts-extra Or any other font packages that suit your language preferences. For powerline or devicons, install Nerd Fonts: yay -S nerd-fonts-hack ","date":"2025-01-05","objectID":"/20250105_arch/:11:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Enabling a Display Manager (Optional) If you prefer a graphical login screen: sudo pacman -S lightdm lightdm-gtk-greeter sudo systemctl enable lightdm.service (Again, adapt for runit or other init systems.) ","date":"2025-01-05","objectID":"/20250105_arch/:12:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Sound Setup ","date":"2025-01-05","objectID":"/20250105_arch/:13:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Alsa sudo pacman -S alsa-utils alsa-plugins amixer Use M to unmute any channels. ","date":"2025-01-05","objectID":"/20250105_arch/:13:1","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"PulseAudio sudo pacman -S pulseaudio pulsemixer pulseaudio --start ","date":"2025-01-05","objectID":"/20250105_arch/:13:2","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Installing Yay (AUR Helper) Clone the Yay repository: git clone https://aur.archlinux.org/yay.git Build and install: cd yay makepkg -si Yay lets you install packages from both the official repositories and the AUR. ","date":"2025-01-05","objectID":"/20250105_arch/:14:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["linux"],"content":"Conclusion Congratulations! Your Arch Linux system is now up and running. From here, you can customize it with whatever software and configurations you like. Remember, the Arch Wiki is your best friend for finding detailed guides and troubleshooting tips. ","date":"2025-01-05","objectID":"/20250105_arch/:15:0","tags":["linux","arch"],"title":"Install Arch Linux","uri":"/20250105_arch/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels","date":"2024-09-01","objectID":"/20240902_svm3/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Introduction to Asymmetric Kernels Recall that the dual form of LS-SVM is given by \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way. Asymmetric kernels are particularly useful in capturing directional relationships in data that symmetric kernels cannot. For instance, in scenarios involving directed graphs or conditional probabilities, the relationship from $x$ to $y$ is inherently different from the relationship from $y$ to $x$. ","date":"2024-09-01","objectID":"/20240902_svm3/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"AsK-LS Primal Problem Formulation We first define a generalized kernel trick for an inner product of two mappings $\\phi_s$ and $\\phi_t$. \\begin{align*} K(\\mathbf{u}, \\mathbf{v}) = \\langle \\phi_s(\\mathbf{u}), \\phi_t(\\mathbf{v})\\rangle, \\forall \\mathbf{u} \\in \\mathbb{R}^{d_s}, \\mathbf{v} \\in \\mathbb{R}^{d_t}, \\end{align*} where $\\phi_s: \\mathbb{R}^{d_s}\\to \\mathbb{R}^{p}$, $\\phi_t: \\mathbb{R}^{d_t}\\to \\mathbb{R}^{p}$, and $\\mathbb{R}^p$ is a high-dimensional or even an infinite-dimensional space. Note that $d_s$ and $d_t$ can be different. This formulation is closely related to the traditional LS-SVM but extends it by simultaneously considering both source and target feature spaces. The optimization goal is to find the weight vectors $ \\omega $ and $ \\nu $, and bias terms $ b_1 $ and $ b_2 $, that minimize the following objective function: \\begin{align*} \\min_{\\omega, \\nu, b_1, b_2, e, h} \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2, \\end{align*} subject to the constraints: \\begin{align*} \u0026 y_i (\\omega^T \\phi_s(x_i) + b_1) = 1 - e_i\\ \u0026 y_i (\\nu^T \\phi_t(x_i) + b_2) = 1 - h_i \\end{align*} Here: $ \\omega $ and $ \\nu $ are weight vectors for the source and target features. $ \\phi_s(x) $ and $ \\phi_t(x) $ are the source and target feature mappings. $ e_i $ and $ h_i $ are error terms for the source and target constraints. $ \\gamma $ is a regularization parameter. Note that this formulation is almost the same as the LS-SVM except that this considers both the source and target feature spaces simultaneously. ","date":"2024-09-01","objectID":"/20240902_svm3/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Dual Form Let‚Äôs transform it into a dual form. The dual problem involves solving a system of linear equations derived from the primal problem‚Äôs Lagrangian. The Lagrangian function for the primal problem is: \\begin{align*} \\mathcal{L}( \\omega, \\nu, b_1, b_2, e, h, \\alpha, \\beta) \u0026= \\frac{1}{2} \\omega^T \\nu + \\frac{\\gamma}{2} \\sum_{i=1}^m e_i^2 + \\frac{\\gamma}{2} \\sum_{i=1}^m h_i^2\\\\ + \\sum_{i=1}^m \\alpha_i (1 - e_i \u0026- y_i (\\omega^T \\phi_s(x_i) + b_1)) + \\sum_{i=1}^m \\beta_i (1 - h_i - y_i (\\nu^T \\phi_t(x_i) + b_2)) \\end{align*} The KKT conditions are derived by setting the partial derivatives of the Lagrangian with respect to $ \\omega, \\nu, b_1, b_2, e, $ and $ h $ to zero. The dual problem leads to the following linear system: \\begin{align*} \\begin{bmatrix} 0 \u0026 0 \u0026 Y^T \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 Y^T \\\\ Y \u0026 0 \u0026 \\frac{I}{\\gamma} \u0026 H \\\\ 0 \u0026 Y \u0026 H^T \u0026 \\frac{I}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\alpha \\\\ \\beta \\end{bmatrix} =\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} where: $ Y $ is a vector of class labels. $ H $ is the kernel matrix with elements $ H_{ij} = y_i K(x_i, x_j) y_j $, where $ K(x_i, x_j) = \\langle \\phi_s(x_i), \\phi_t(x_j) \\rangle $ is the asymmetric kernel function. For an asymmetric kernel $ K $, the kernel function $ K(x_i, x_j) \\neq K(x_j, x_i) $. This asymmetry is directly incorporated into the matrix $ H $, where: \\begin{align*} H_{ij} \u0026= y_i K(x_i, x_j) y_j \\\\ H_{ji} \u0026= y_j K(x_j, x_i) y_i \\end{align*} AsK-LS uses two different feature mappings $ \\phi_s $ and $ \\phi_t $ for the source and target features. This approach allows capturing more information compared to symmetric kernels. The dual solution provides weight vectors $ \\omega $ and $ \\nu $, which span the target and source feature spaces, respectively. The decision functions for classification from the source and target perspectives are given by \\begin{align*} f_t(x) \u0026= \\sum_{i=1}^m \\alpha_i y_i K(x_i, x) + b_1\\\\ f_s(x) \u0026= \\sum_{i=1}^m \\beta_i y_i K(x, x_i) + b_2 \\end{align*} These decision functions leverage the learned asymmetric relationships in the data, providing a more nuanced classification model. ","date":"2024-09-01","objectID":"/20240902_svm3/:2:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","Asymmetric Kernels"],"title":"Introduction to SVM Part 3. Asymmetric Kernels","uri":"/20240902_svm3/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 2.","date":"2024-08-31","objectID":"/20240825_svm2/","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction to Least-Square SVM ","date":"2024-08-31","objectID":"/20240825_svm2/:0:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a least squares cost function. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve. ","date":"2024-08-31","objectID":"/20240825_svm2/:1:0","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Optimization Problem (Primal Problem) \\begin{align*} \u0026\\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2,\\\\ \u0026\\text{subject to } y_i (w^T \\phi(x_i) + b) = 1 - e_i, \\ \\forall i \\end{align*} where: $w$ is the weight vector. $b$ is the bias term. $e_i$ are the error variables. $\\gamma$ is a regularization parameter. $\\phi(x_i)$ is the feature mapping function. Note that $y_i^{-1} = y_i$, since $y_i = \\pm 1$. ","date":"2024-08-31","objectID":"/20240825_svm2/:1:1","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Lagrangian Function To solve the constraint optimization problem, we define the Lagrangian function: \\begin{align*} L(w, b, e, \\alpha) = \\min_{w, b, e} \\frac{1}{2} \\lVert w\\rVert^2 + \\frac{\\gamma}{2} \\sum_{i=1}^N e_i^2 - \\sum_{i=1}^n \\alpha_i \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right], \\end{align*} where $\\alpha_i$ are Lagrange multipliers. Then, by setting the partial derivatives of the Lagrangian with respect to $w$, $b$, $e$, and $\\alpha$ to zero, we get the KKT conditions. $w$: \\begin{align*} \\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) = 0 \\implies w = \\sum_{i=1}^n \\alpha_i y_i \\phi(x_i) \\end{align*} $b$: \\begin{align*} \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^n \\alpha_i y_i = 0 \\end{align*} $e_i$: \\begin{align*} \\frac{\\partial L}{\\partial e_i} = \\gamma e_i - \\alpha_i = 0 \\implies \\alpha_i = \\gamma e_i \\end{align*} Thus, $e_i = \\frac{\\alpha_i}{\\gamma}$ $\\alpha_i$: \\begin{align*} \\frac{\\partial L}{\\partial \\alpha_i} = - \\left[ y_i (w^T \\phi(x_i) + b) - 1 + e_i \\right] = 0 \\implies y_i (w^T \\phi(x_i) + b) = 1 - e_i, i=1,\\dots, N. \\end{align*} Let‚Äôs substitute $w$ and $e$: $K$: kernel matrix $\\alpha = [\\alpha_1, \\alpha_2, \\ldots, \\alpha_n]^T$: $N\\times 1$ $y = [y_1, y_2, \\ldots, y_n]^T$. $\\Omega = YKY^T$, where $\\Omega_{kl}= y_ky_l\\phi(x_k)^T\\phi(x_l)$ $Y = \\text{diag}(y)$. $b$: $1\\times 1$ Then, we can express it compactly \\begin{align*} \u0026 Y(KY^T\\alpha+b\\mathbf{1})-\\mathbf{1}+\\frac{\\alpha}{2\\gamma} = 0\\\\ \u0026 \\mathbf{1}^TY\\alpha = 0. \\end{align*} \\begin{align*} \\end{align*} By using the expression of $\\alpha$ and $b$, we get \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1_n \\end{bmatrix} \\end{align*} Note that the dimension of the matrix on the left-hand side is $(N+1)\\times (N+1)$. Once we have $b$ and $\\alpha$ by solving the linear system, the decision function for a new input $x$ can be obtained by: \\begin{align*} f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b. \\end{align*} Example Suppose we have three training examples with feature vectors $x_1, x_2$, and $x_3$, and corresponding labels $y_1, y_2$, and $y_3$. The kernel matrix $\\Omega$ is defined as: \\begin{align*} \\Omega_{ij} = y_i y_j K(x_i, x_j) \\end{align*} The dual form is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y^T \\\\ y \u0026 \\Omega + \\frac{1}{\\gamma} I \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ e \\end{bmatrix} \\end{align*} $y = \\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\end{bmatrix}$ $\\alpha = \\begin{bmatrix} \\alpha_1\\ \\alpha_2 \\ \\alpha_3 \\end{bmatrix} $ $e = \\begin{bmatrix} 1 \\ 1 \\ 1 \\end{bmatrix}$ $I$ is a $3 \\times 3$ identity matrix Then, the $\\Omega$ is given by \\begin{align*} \\Omega = \\begin{bmatrix} y_1 y_1 K(x_1, x_1) \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 y_1 K(x_2, x_1) \u0026 y_2 y_2 K(x_2, x_2) \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3 y_3 K(x_3, x_3) \\end{bmatrix} \\end{align*} Now, the complete matrix equation is: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 \\Omega_{11} + \\frac{1}{\\gamma} \u0026 \\Omega_{12} \u0026 \\Omega_{13} \\\\ y_2 \u0026 \\Omega_{21} \u0026 \\Omega_{22} + \\frac{1}{\\gamma} \u0026 \\Omega_{23} \\\\ y_3 \u0026 \\Omega_{31} \u0026 \\Omega_{32} \u0026 \\Omega_{33} + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} This can be written explicitly as: \\begin{align*} \\begin{bmatrix} 0 \u0026 y_1 \u0026 y_2 \u0026 y_3 \\\\ y_1 \u0026 y_1^2 K(x_1, x_1) + \\frac{1}{\\gamma} \u0026 y_1 y_2 K(x_1, x_2) \u0026 y_1 y_3 K(x_1, x_3) \\\\ y_2 \u0026 y_2 y_1 K(x_2, x_1) \u0026 y_2^2 K(x_2, x_2) + \\frac{1}{\\gamma} \u0026 y_2 y_3 K(x_2, x_3) \\\\ y_3 \u0026 y_3 y_1 K(x_3, x_1) \u0026 y_3 y_2 K(x_3, x_2) \u0026 y_3^2 K(x_3, x_3) + \\frac{1}{\\gamma} \\end{bmatrix} \\begin{bmatrix} b \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\end{align*} The solution to this matrix equation provides the values of $b","date":"2024-08-31","objectID":"/20240825_svm2/:1:2","tags":["machine learning","svm","Support vector machines","Least-Square SVM","LS-SVM"],"title":"Introduction to SVM Part 2. LS-SVM","uri":"/20240825_svm2/"},{"categories":["machine learning"],"content":"Introduction to Support Vector Machines Part 1.","date":"2024-08-25","objectID":"/20240825_svm1/","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Support Vector Machine ","date":"2024-08-25","objectID":"/20240825_svm1/:0:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Introduction Support Vector Machines (SVMs) are among the most effective and versatile tools in machine learning, widely used for various tasks. SVMs work by finding the optimal boundary, or hyperplane, that separates different classes of data with the maximum margin, making them highly reliable for classification, especially with complex datasets. What truly sets SVMs apart is their ability to handle both linear and non-linear data through the kernel trick, allowing them to adapt to a wide range of problems with impressive accuracy. In this blog post, we‚Äôll delve into how SVMs work and gently explore the mathematical foundations behind their powerful performance. ","date":"2024-08-25","objectID":"/20240825_svm1/:1:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Orthogonal Projection When working with vectors $x$ and $y$, finding the orthogonal projection of $x$ onto $y$ is a common task in linear algebra. The projection is a way to express how much of $x$ lies in the direction of $y$. By definition, the magnitude of the projection $z$ of $x$ onto $y$ is given by:: $$\\lVert z\\rVert = \\lVert x\\rVert cos(\\theta).$$ Here, $\\theta$ is the angle between $x$ and $y$. To connect this with the dot product, recall that: $$x\\cdot y = \\lVert x\\rVert \\ \\lVert y\\rVert cos(\\theta).$$ This formula allows us to replace the cosine term: $$\\lVert z\\rVert = \\lVert x\\rVert \\frac{x\\cdot y}{\\lVert x\\rVert \\cdot\\lVert y\\rVert }.$$ Simplifying further, we express the magnitude of $z$ as: $$\\lVert z\\rVert = u\\cdot x,$$ where $u$ is an unit vector of $y$. Since $z$ is in the direction of $y$, we can write: $$z = \\lVert z\\rVert \\cdot u,$$ Then, \\begin{align*} z \u0026= (u\\cdot x)\\cdot u. \\end{align*} This gives us the final expression for the orthogonal projection of $x$ onto $y$: \\begin{align*} \\textrm{Proj}_yx \u0026= (u\\cdot x)\\cdot u\\\\ \u0026= \\Bigg(\\frac{y\\cdot x}{\\lVert y\\rVert ^2}\\Bigg)y\\\\ \u0026= \\Bigg(\\frac{y\\cdot x}{\\lVert y\\rVert }\\Bigg)\\frac{y}{\\lVert y\\rVert } \\end{align*} In this formula, the projection $\\textrm{Proj}_yx$ represents the component of $x$ that lies along the direction of $y$. ","date":"2024-08-25","objectID":"/20240825_svm1/:1:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Decision Boundary with Margin A hyperplane(or decision surface) is used to separate data points belonging to different classes. The goal of SVM is to find the optimal separating hyperplane. However, what is the optimal separating hyperplanes? The optimal hyperplane is the one which maximizes the distance from the hyperplane to the nearest data point of any class. Support vectors are the data points that lie closest to the hyperplane. The distance is referred to as the margin. SVMs maximize the margin around the separating hyperplane. The equation of a hyperplane in $\\mathbb{R}^p$ can be expressed as: $$\\mathbf{w}\\cdot \\mathbf{x}+b=0.$$ Here, $\\mathbf{w}$ is the normal vector to the hyperplane. It is clear by expressing it $$\\mathbf{w}(\\mathbf{x}-\\mathbf{x}_0)=0,$$ where $b = \\mathbf{w}\\cdot\\mathbf{x}_0$. % The support vectors are directly related to the optimal hyperplane. The decision function is fully specified by a subset of training samples, the support vectors. Let‚Äôs consider a simple scenario, where training data is linearly separable: $$\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) | \\mathbf{x}_i \\in \\mathbb{R}^p,\\ y_i \\in {-1,1}\\}_{i=1}^N.$$ Then, we can build two hyperplanes separating the data with no points between them: $H_1:\\mathbf{w}\\cdot \\mathbf{x}+b=1$ $H_2:\\mathbf{w}\\cdot \\mathbf{x}+b=-1$ All samples have to satisfy one of two constraints: $\\mathbf{w}\\cdot \\mathbf{x}+b\\geq1$ $\\mathbf{w}\\cdot \\mathbf{x}+b\\leq-1$ These constraints can be combined into a single expression: $$y(\\mathbf{w}\\cdot \\mathbf{x}+b)\\geq 1.$$ To maximize the margin, we can consider a unit vector $\\mathbf{u} = \\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}$, which is perpendicular to the hyperplanes and a point $x_0$ on the hyperplane $H_2$. If we scale $u$ from $x_0$, we get $z = x_0+ru$. If we assume $z$ is on $H_1$, then $\\mathbf{w}\\cdot z +b=1$. This is equivalent to \\begin{align*} \\mathbf{w}\\cdot (x_0+ru)+b=1\\\\ \\mathbf{w}x_0+\\mathbf{w}r\\frac{\\mathbf{w}}{\\lVert\\mathbf{w}\\rVert}+b=1\\\\ \\mathbf{w}x_0+r\\lVert \\mathbf{w}\\rVert +b=1\\\\ \\mathbf{w}x_0+b=1-r\\lVert \\mathbf{w}\\rVert \\end{align*} As $x_0$ is on $H_2$, we get $\\mathbf{w}x_0+b=-1$. Finally, we obtain \\begin{align*} -1=1-r\\lVert \\mathbf{w}\\rVert \\\\ r=\\frac{2}{\\lVert \\mathbf{w}\\rVert }. \\end{align*} Note that the scaled unit vector $ru$‚Äôs magnitude is $r$. Thus, the maximization of margin is equivalent to maximize $r$. To maximize $r$, we have to minimize $\\lVert \\mathbf{w} \\rVert$. Thus, finding the optimal hyperplane reduces to solving the following optimization problem: \\begin{align*} \u0026\\min \\lVert \\mathbf{w}\\rVert ,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Equivalently, \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert w\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} Now, we have convex quadratic optimization problem. The solution of this problem gives us the optimal hyperplane that maximizes the margin (Details are in the following section). However, in practice, the data may not be perfectly separable. To account for this, we introduce a soft margin that allows for some misclassification. This is done by admitting small errors in classification and potentially using a more complex, nonlinear decision boundary, improving the generalization of the model. \\section{Error Handling in SVM} In practice, it‚Äôs unrealistic to expect a perfect separation of data, especially when the data is noisy or not linearly separable. To address this, we can allow for some prediction errors while still striving to find an optimal decision boundary. One approach is to minimize the norm of the weight vector, while penalizing the number of errors $N_e$. The optimization problem can be formulated as follows: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 +C\\cdot N_{e},\\quad \\text{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad \\forall i. \\end{align*} Here, $C$ is a regularization para","date":"2024-08-25","objectID":"/20240825_svm1/:1:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"SVM Optimization: Lagrange Multipliers ","date":"2024-08-25","objectID":"/20240825_svm1/:2:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Lagrange Multipliers Consider the optimization problem: \\begin{align*} \u0026\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\ \u0026\\text{subject to}\\quad g(\\mathbf{x})=0. \\end{align*} To find the minimum of $f$ under the constraint $g(\\mathbf{x})$, we use the method of Lagrange multipliers. The key idea is that at the optimal point, the gradient of $f(\\mathbf{x})$ must be parallel to the gradient of $g(\\mathbf{x})$. Mathematically, this condition is expressed as: $$\\nabla f(\\mathbf{x}) = \\lambda\\nabla g(\\mathbf{x}).$$ Example: Consider a simple 2D example where you want to minimize the function $f(x,y)=x^2+y^2$, which represents a circle centered at the origin. This function increases as you move away from the origin, so the minimum is at the origin. Now, consider the constraint: $g(x,y)=x+y-1=0$. This constraint is a line that runs through the $xy$-plane. Our goal is to find the point on this line that minimizes $f(x,y)$. A Lagrange multiplier is like a balancing factor that adjusts the direction and magnitude of your search along the constraint. As you move along the constraint line $g(x,y)$, $\\lambda$ ensures that the solution also respects the shape of the function $f(x,y)$ that you are trying to minimize. To solve the constraint optimization problem, we define the Lagrangian function: $$\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x}).$$ To find the minimum, we take the partial derivatives of $\\mathcal{L}(\\mathbf{x}, \\lambda)$ with respect to both $\\mathbf{x}$ and $\\lambda$, and set them equal to zero. \\subsection{SVM Optimization} Recall that we want to solve the following convex quadratic optimization problem: \\begin{align*} \u0026\\min \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2,\\quad \\textrm{subject to } \\\\ \u0026y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)\\geq 1 \\quad\\forall i. \\end{align*} The objective is to find the optimal hyperplane that maximizes the margin between two classes of data points. We can reformulate this optimization problem using the method of Lagrange multipliers, which introduces a set of multipliers $\\alpha_i$ (one for each constraint). The Lagrangian function for this problem is given by: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert ^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} ","date":"2024-08-25","objectID":"/20240825_svm1/:2:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Duality and the Lagrangian Problem While we could attempt to solve the primal optimization problem directly, it is often more practical, especially for large datasets, to reformulate the problem using the duality principle. The dual form is advantageous because it depends only on the inner products of the data points, which allows the use of kernel methods for non-linear classification. To find the solution to the primal problem, we solve the following problem: \\begin{align*} \u0026\\max_{\\mathbf{w}, b}\\min_\\alpha \\mathcal{L}(\\mathbf{w}, b, \\alpha)\\\\ \u0026\\textrm{subject to}\\quad \\alpha_i\\geq 0, \\forall i. \\end{align*} Here, we maximize the Lagrangian with respect to the multipliers $\\alpha_i$, while minimizing with respect to the primal variables $\\mathbf{w}$ and $b$. ","date":"2024-08-25","objectID":"/20240825_svm1/:2:2","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Handling Inequality Constraints with KKT Conditions You may observe that the method of Lagrange multipliers is used for equality constraints. However, it can be extended to handle inequality constraints through the use of additional conditions known as the Karush-Kuhn-Tucker (KKT) conditions. These conditions ensure that the solution satisfies the necessary optimality criteria for problems with inequality constraints. ","date":"2024-08-25","objectID":"/20240825_svm1/:2:3","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"The Wolfe Dual Problem The Lagrangian problem for SVM optimization involves $N$ inequality constraints, where $N$ is the number of training examples. This problem is typically tacked using its dual form. The duality principle provides a powerful framework, stating that an optimization problem can be approached from two perspectives: The primal problem, which in our context is a minimization problem. The dual problem, which is a maximization problem. An important aspect of duality is that the maximum value of the dual problem is always less than or equal to the minimum value of the primal problem. This relationship means that the dual problem provides a lower bound to the solution of the primal problem. In the context of SVM optimization, we are dealing with a convex optimization problem. According to Slater‚Äôs condition, which applies to problems with affine constraints, strong duality holds. Strong duality implies that the optimal values of the primal and dual problems are equal, meaning the maximum value of the dual problem equals the minimum value of the primal problem. This equality allows us to solve the dual problem instead of the primal problem, often leading to computational advantages. Recall that we aim to solve the following optimization problem: \\begin{align*} \\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 - \\sum_{i=1}^N \\alpha_i \\left[y_i(\\mathbf{w}\\cdot \\mathbf{x}_i+b)-1\\right] \\end{align*} The minimization problem involves solving the partial derivatives of $\\mathcal{L}$ with respect to $\\rvw$ and $b$ and set them equal to zero: \\begin{align*} \u0026\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}, b, \\alpha) = \\mathbf{w} - \\sum_i \\alpha_i y_i \\mathbf{x}_i\\\\ \u0026 \\nabla_b\\mathcal{L}(\\mathbf{w}, b, \\alpha) = -\\sum_i \\alpha_i y_i \\end{align*} Form the first equation, we obtain: \\begin{align*} \u0026\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i\\ \\end{align*} Next, we substitute the objective function with $\\rvw$: \\begin{align*} \\mathbf{W}(\\alpha, b) \u0026= \\frac{1}{2}\\left(\\sum_i \\alpha_i y_i \\mathbf{x}_i\\right)\\cdot \\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\left[y_i\\left(\\left(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\right)\\cdot \\mathbf{x}_i+b\\right)-1\\right]\\\\ \u0026= \\frac{1}{2}\\Big(\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j\\Big)\\\\ \u0026\\quad - \\sum_i \\alpha_i \\Bigg[y_i\\Bigg(\\Big(\\sum_j \\alpha_j y_j \\mathbf{x}_j\\Big)\\cdot \\mathbf{x}_i+b\\Bigg)\\Bigg]+\\sum_i \\alpha_i \\\\ \u0026= \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j - \\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i \\cdot \\mathbf{x}_j\\\\ \u0026\\quad -\\sum_i \\alpha_i y_i b+\\sum_i \\alpha_i \\\\ \u0026= \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j \\mathbf{x}_i\\cdot \\mathbf{x}_j-\\sum_i \\alpha_i y_i b \\end{align*} Note that we use two indices, $i$ and $j$ when substituting $\\mathbf{W}$. This is obvious if we consider a simple example. Imagine you have two data points: \\begin{align*} \\mathbf{x}_1,y1\u0026=(1,2),1\\\\ \\mathbf{x}_2,y2\u0026=(2,1),‚àí1 \\end{align*} Then, \\begin{align*} \\lVert \\mathbf{w}\\rVert^2=\\mathbf{w}\\cdot \\mathbf{w}=\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_i}\\cdot\\underbrace{(\\alpha_1y_1\\mathbf{x}_1+\\alpha_2y_2\\mathbf{x}_2)}_{\\sum_j}. \\end{align*} This simplification shows that the optimization problem can be reformulated purely in terms of the Lagrange multipliers $\\alpha_i$. Note that the term involving $b$ can be removed by setting $b=0$, simplifying our equation further: \\begin{align*} \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_iy_j (\\mathbf{x}_i\\cdot \\mathbf{x}_j) \\end{align*} This expression is known as the Wolfe dual Lagrangian function. We have transformed the problem into one involving only the multipliers $\\alpha_i$, resulting in a quadratic programming problem, commonly referred to as the Wolfe dual problem: \\begin{align*} \u0026\\max_\\alpha \\mathbf{W}(\\alpha, b) = \\sum_i \\alpha_i -\\frac{1}{2}\\sum_i\\sum_j ","date":"2024-08-25","objectID":"/20240825_svm1/:3:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Karush-Kuhn-Tucker conditions When dealing with optimization problems that involve inequality constraints, such as those encountered in Support Vector Machines (SVMs), an additional requirement must be met: the solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The KKT conditions are a set of first-order necessary conditions that must be satisfied for a solution to be optimal. These conditions extend the method of Lagrange multipliers to handle inequality constraints and are particularly useful in non-linear programming. For the KKT conditions to apply, the problem must also satisfy certain regularity conditions. Fortunately, one of these regularity conditions is Slater‚Äôs condition, which we have already established holds true for SVMs. ","date":"2024-08-25","objectID":"/20240825_svm1/:4:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"KKT Conditions and SVM Optimization In the context of SVMs, the optimization problem is convex, meaning that the KKT conditions are not only necessary but also sufficient for optimality. This implies that if a solution satisfies the KKT conditions, it is guaranteed to be the optimal solution for both the primal and dual problems. Moreover, in this case, there is no duality gap, meaning the optimal values of the primal and dual problems are equal. ","date":"2024-08-25","objectID":"/20240825_svm1/:4:1","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"Prediction Firstly, for a Linear SVM with no kernel, the primal weight vector is given by $$ w=\\sum_{i\\in \\mathcal S}\\alpha_i,y_i,x_i $$ Then, the decision function is $$ f(x)=w^{!\\top}x + b $$ Since the feature map $\\phi(,\\cdot,)$ may live in a huge or even infinite-dimensional space, we never form $w$ explicitly. Instead we keep the kernel trick inside the decision function: $$ f(x)=\\sum_{i\\in\\mathcal S}\\alpha_i,y_i,K(x_i,,x)+b $$ where $K(x_i,x)=\\langle\\phi(x_i),\\phi(x)\\rangle$. Typical kernels are the RBF $K(u,v)=\\exp\\!\\bigl(-\\frac{|u-v|^2}{2\\sigma^2}\\bigr)$ or the polynomial $K(u,v)=(u^{\\!\\top}v+c)^p$. Finally, we need to turn the decision value into a class label. For binary classification the prediction is simply the sign of the decision function: \\begin{align*} \\hat y = \\operatorname{sign}\\bigl(f(x)\\bigr) = \\begin{cases} +1 \u0026\\text{if }f(x)\\gt 0,\\\\ -1 \u0026\\text{if }f(x)\\lt 0. \\end{cases} \\end{align*} In sum, \\begin{align*} \\textbf{Predict}(x): \\quad f(x)=\\sum_{i\\in\\mathcal S}\\alpha_i,y_i,K(x_i,x)+b,; \\hat y=\\text{sign}\\bigl(f(x)\\bigr) \\end{align*} Reference Alexandre Kowalczyk, Support Vector Machines Succinctly, 2017 ","date":"2024-08-25","objectID":"/20240825_svm1/:5:0","tags":["machine learning","svm","Support vector machines"],"title":"Introduction to SVM Part 1. Basics","uri":"/20240825_svm1/"},{"categories":["machine learning"],"content":"On the direction of gradient descent update","date":"2024-08-19","objectID":"/20240819_gradient_descent/","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"On Gradient Descent Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the function‚Äôs minimum value. It is a fundamental concept in machine learning, particularly in training models such as neural networks. The gradient is a vector that represents the direction of the steepest increase of the function at a given point. For example, for a convex function $z = ax^2 + by^2$, the gradient is $[2ax, 2by]$, which points in the direction of the steepest ascent. In gradient descent, the goal is to minimize the function, so the algorithm moves in the opposite direction of the gradient, which is $[-2ax, -2by]$. This opposite direction is chosen because it is the direction of the steepest decrease in the function value. But how do we know that moving in this direction will strictly decrease the function value? ","date":"2024-08-19","objectID":"/20240819_gradient_descent/:0:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"Direction of Gradient Descent Let‚Äôs investigate the direction of gradient descent. The derivative of the objective function $f(\\mathbf{x})$ provides the slope of $f(\\mathbf{x})$ at the point $f(\\mathbf{x})$. It tells us how to change $\\mathbf{x}$ in order to make a small improvement in our goal. A function $f(\\mathbf{x})$ can be approximated by its first-order Taylor expansion at $\\bar{\\mathbf{x}}$: $$f(\\mathbf{x})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T(x-\\bar{\\mathbf{x}})$$ Now let $\\mathbf{d}\\neq0, |\\mathbf{d}|=1$ be a direction, and in consideration of a new point $\\mathbf{x}:=\\bar{\\mathbf{x}}+\\mathbf{d}$, we define: $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})+\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}$$ We would like to choose $\\mathbf{d}$ that minimizes the function $f$. From the Cauchy-Schwarz inequality1 , we know that $$|\\nabla f(\\bar{\\mathbf{x}})^T\\mathbf{d}|\\leq \\lVert\\nabla f(\\bar{\\mathbf{x}})\\rVert \\ \\lVert\\mathbf{d}\\rVert.$$ The equality holds if and only if $\\mathbf{d}=\\lambda \\nabla f(\\bar{\\mathbf{x}})$, where $\\lambda\\in \\mathbb{R}$. Since we want to minimize the function $f$, we negate the steepest direction $\\mathbf{d}^{*}$, then $$f(\\bar{\\mathbf{x}}+\\mathbf{d})\\approx f(\\bar{\\mathbf{x}})-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}}).$$ Since $\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always positive, the term $-\\lambda\\nabla f(\\bar{\\mathbf{x}})^T\\nabla f(\\bar{\\mathbf{x}})$ is always negative. Therefore, by updating $\\mathbf{x}$ $$\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\lambda \\nabla f(\\mathbf{x}^{(k)}),$$ we get $$f(\\mathbf{x}^{(k+1)}) \u003c f(\\mathbf{x}^{(k)}).$$ Cauchy-Schwarz Inequaility: $|\\mathbf{a}\\cdot \\mathbf{b}|\\leq \\lVert\\mathbf{a}\\rVert \\ \\lVert\\mathbf{b}\\rVert$. Equality holds if and only if either $\\mathbf{a}$ or $\\mathbf{b}$ is a multiple of the other.¬†‚Ü©Ô∏é ","date":"2024-08-19","objectID":"/20240819_gradient_descent/:1:0","tags":["machine learning","gradient descent","gradient"],"title":"Direction of Gradient Descent Update","uri":"/20240819_gradient_descent/"},{"categories":["machine learning"],"content":"Latent variable tutorial","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Latent Variable Modeling ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:0:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Motivation of Latent Variable Modeling Let‚Äôs say we want to classify some data. If we had access to a corresponding latent variable for each observation $ \\mathbf{x}_i $, modeling would be more straightforward. To illustrate this, consider the challenge of finding the latent variable (i.e., the true class of $ \\mathbf{x} $). It can be expressed like $ z^* = \\argmax_{z} p(\\mathbf{x} | z) $. It is hard to identify the true clusters without prior knowledge about them. For example, we can cluster like Fig. (b) or (c). Consider modeling the complete data set $ p(\\mathbf{x} | z) $ under the assumption that the observations are independent and identically distributed (i.i.d.). Based on the above Fig. (c), the joint distribution for a single observation $ (\\mathbf{x}_i, \\mathbf{z}_i) $ given the model parameters $ \\boldsymbol{\\theta} $ can be expressed: \\begin{align*} p(\\mathbf{x}_i, \\mathbf{z}_i | \\boldsymbol{\\theta}) = \\begin{cases} p(\\mathcal{C}_1) p(\\mathbf{x}_i | \\mathcal{C}_1) \u0026 \\text{if } z_i = 0 \\\\ p(\\mathcal{C}_2) p(\\mathbf{x}_i | \\mathcal{C}_2) \u0026 \\text{if } z_i = 1 \\\\ p(\\mathcal{C}_3) p(\\mathbf{x}_i | \\mathcal{C}_3) \u0026 \\text{if } z_i = 2 \\\\ \\end{cases} \\end{align*} Given $ N $ observations, the joint distribution for the entire dataset $ { \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N } $ along with their corresponding latent variables $ { \\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_N } $ is: \\begin{align*} p(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N, \\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_N | \\boldsymbol{\\theta}) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\pi_k^{z_{nk}} \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_{nk}} \\end{align*} Here, $ \\pi_k = p(\\mathcal{C}_k) $ represents the prior probability of the $ k $-th component, and $ p(\\mathbf{x}_n | \\mathcal{C}_k) = \\mathcal{N}(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ denotes the Gaussian distribution associated with component $ \\mathcal{C}_k $. Also, $z_{nk}\\in{0,1}$ and $\\sum_k z_{nk}=1$. However, in practice, the latent variables $ \\mathbf{z}_k $ are often not directly observable, which complicates the modeling process. In the following sections, we present various methods for identifying and handling these latent variables to improve the classification and modeling of data. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:1:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"K-Means Clustering Suppose that we have a data set $\\mathbf{X} = {\\mathbf{x}_1,\\dots, \\mathbf{x}_n}$ consisting of $N$ observations of a random $D$-dimensional variable $\\mathbf{x}\\in \\mathbb{R}^{D}$. Our goal is to partition the data into $K$ of clusters. Intuitively, a cluster can be thought as a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. This notion can be formalized by introducing a set of $D$-dimensional vectors $\\boldsymbol{\\mu}_k$, which represents the centers of the clusters. Our goal is to find an assignment of data points to clusters, as well as a set of vectors ${\\boldsymbol{\\mu}_k}$. Objective function of $K$-means clustering (distortion measure) can be defined as follows: $$J = \\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}\\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k\\rVert^2$$ , where $r_{nk}\\in{0,1}$ is a binary indicator variable which represents the membership of data $\\mathbf{x}_n$. It can be expressed as follows: % The $r_{nk}$ can be optimized in a closed-form solution as follows: \\begin{align*} r_{nk}=\\begin{cases} 1 \u0026 \\text{if } k=\\argmin_{j} \\lVert\\boldsymbol{x}_n-\\boldsymbol{\\mu}_j\\rVert^2\\\\ 0 \u0026 \\text{otherwise} \\end{cases} \\end{align*} Our goal is to find values for the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$ that minimize $J$. We can minimize $J$ through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the $\\boldsymbol{\\mu}_k$ and the $r_{nk}$. First we choose some initial values for the $\\boldsymbol{\\mu}_k$. Then, in the first phase, we minimize $J$ with respect to the $r_{nk}$, keeping the $\\boldsymbol{\\mu}_k$ fixed. In the second phase we minimize $J$ with respect to the $\\boldsymbol{\\mu}_k$, keeping $r_{nk}$ fixed. This two-stage optimization is then repeated until convergence. Now consider the optimization of the $\\boldsymbol{\\mu}_k$ with the $r_{nk}$ held fixed. The objective function $J$ is a quadratic function of $\\boldsymbol{\\mu}_k$, and it can be minimized by setting its derivative with respect to $\\boldsymbol{\\mu}_k$ to zero giving \\begin{align*} 2\\sum_{n=1}^{N}r_{nk}(\\boldsymbol{x}_n-\\boldsymbol{\\mu}_k) = 0. \\end{align*} We can arrange as \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{\\sum_n r_{nk}\\boldsymbol{x}_n}{\\sum_n r_{nk}}. \\end{align*} The denominator of $\\boldsymbol{\\mu}_k$ is equal to the number of points assigned to cluster $k$. The mean of cluster $k$ is essentially the same as the mean of data points $\\mathbf{x}_n$ assigned to cluster $k$. For this reason, the procedure is known as the $K$-means clustering algorithm. The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments. These two phases reduce the value of the objective function $J$, so the convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of $J$. We can also sequentially update the $\\mu_k$ as follows: \\begin{align*} \\mu_{k+1} = \\mu_{k} + \\eta(\\mathbf{x}_k-\\mu_{k}) \\end{align*} There are some properties to note: It is a hard clustering algorithm ($\\leftrightarrow$ soft clustering) It is sensitive to the initialization of centroid. The number of clusters is uncertain. Sensitive to distance metrics (e.g., Euclidean?) ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:2:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Gaussian Mixture Models $K$-means clustering is a form of hard clustering, where each data point is assigned to exactly one cluster. However, in some cases, soft clustering‚Äîwhere data points can belong to multiple clusters with varying degrees of membership‚Äîprovides a better model in practice. A Gaussian Mixture Model (GMM) assumes a linear superposition of Gaussian components, offering a richer class of density models than a single Gaussian distribution. In essence, rather than assuming that all data points are generated by a single Gaussian distribution, we assume that the data is generated by a mixture of $ K $ different Gaussian distributions, where each Gaussian represents a different component in the mixture. For a single sample, the Gaussian Mixture Model can be expressed as a weighted sum of these individual Gaussian distributions: \\begin{align*} p(\\mathbf{x}) \u0026= \\sum_\\mathbf{z} p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) \\\\ \u0026= \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\end{align*} Here, $ \\mathbf{x} $ is a data point, $ \\pi_k $ represents the mixing coefficients, $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is a Gaussian distribution with mean $ \\boldsymbol{\\mu}_k $ and covariance $ \\boldsymbol{\\Sigma}_k $, and $ K $ is the number of Gaussian components. A key quantity in GMMs is the conditional probability of $ \\mathbf{z} $ given $ \\mathbf{x} $, denoted as $ p(z_k = 1 | \\mathbf{x}) $ or $ \\gamma(z_k) $. This is also known as the responsibility or assignment probability, which represents the probability that a given data point $ \\mathbf{x} $ belongs to component $ k $ of the mixture. Essentially, this can be thought of as the classification result for $ \\mathbf{x} $. This responsibility is updated using Bayes‚Äô Theorem, and can be expressed as: \\begin{align*} \\gamma(z_k) \\equiv p(z_k=1|\\mathbf{x}) \u0026 \\equiv \\frac{p(z_k=1)p(\\mathbf{x}|z_k=1)}{\\sum_{j=1}^{K}p(z_j=1)p(\\mathbf{x}|z_j=1)} \\\\ \u0026 = \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\end{align*} In this expression, $ \\pi_k $ is the prior probability (or mixing coefficient) for component $ k $, and $ \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) $ is the likelihood of the data point $ \\mathbf{x} $ under the Gaussian distribution corresponding to component $ k $. The denominator is a normalization factor that ensures the responsibilities sum to 1 across all components for a given data point. This framework allows for a soft classification of data points, where each point is associated with a probability of belonging to each cluster, rather than being strictly assigned to a single cluster as in $K$-means. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:0","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Maximum Likelihood Suppose we have a data set of observations $\\mathbf{X}=\\{ \\mathbf{x}_1,\\dots,\\mathbf{x}_n \\}^{T}\\in\\mathbb{R}^{N\\times D}$ and we want to model the data distribution $p(\\mathbf{X})$ using GMM. Assuming the data is independent and identically distributed (i.i.d.), the likelihood of the entire dataset can be expressed as: \\begin{align*} p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\prod_{n=1}^{N}\\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg). \\end{align*} To simplify the optimization process, we consider the log-likelihood function, which is given by: \\begin{align*} \\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \u0026= \\sum_{n=1}^{N}\\ln \\Bigg(\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\Bigg) \\end{align*} To solve the Maximum Likelihood Estimation (MLE) for Gaussian Mixture Models (GMMs), we typically consider the iterative Expectation-Maximization (EM) algorithm due to the non-convex nature of the problem. Before discussing how to maximize the likelihood, it is important to emphasize two significant issues that arise in GMMs: singularities and identifiability. Singularity A major challenge in applying the maximum likelihood framework to Gaussian Mixture Models is the presence of singularities. This problem arises because the likelihood function can become unbounded under certain conditions, leading to an ill-posed optimization problem. For simplicity, consider a Gaussian mixture model where each component has a covariance matrix of the form $ \\Sigma_k = \\sigma^2_k I $, where $ I $ is the identity matrix. Suppose one of the mixture components, say the $ j $-th component, has its mean $ \\boldsymbol{\\mu}_j $ exactly equal to one of the data points $ \\mathbf{x}_n $, so that $ \\boldsymbol{\\mu}_j = \\mathbf{x}_n $ for some value of $ n $. The contribution of this data point to the likelihood function would then be: \\begin{align*} \\mathcal{N}(\\mathbf{x}_n | \\mathbf{x}_n, \\sigma^2_j I) = \\frac{1}{\\sqrt{2\\pi} \\sigma_j} \\cdot \\exp^0 \\end{align*} As $ \\sigma_j $ approaches 0, this term goes to infinity, causing the log-likelihood function to also diverge to infinity. Therefore, maximizing the log-likelihood function becomes an ill-posed problem because such singularities can always be present. These singularities occur whenever one of the Gaussian components collapses onto a specific data point, leading to a covariance matrix with a determinant approaching zero. This issue did not arise with a single Gaussian distribution because the variance cannot be zero by definition. Identifiability Another issue in finding MLE solutions for GMMs is related to identifiability. For any given maximum likelihood solution, a GMM with $ K $ components has a total of $ K! $ equivalent solutions. This arises from the fact that the $ K! $ different ways of permuting the $ K $ sets of parameters (means, covariances, and mixing coefficients) yield the same likelihood. In other words, for any point in the parameter space that represents a maximum likelihood solution, there are $ K! - 1 $ additional points that produce exactly the same probability distribution. This lack of identifiability means that the solution is not unique, complicating both the interpretation of the model and the optimization process. ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:1","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning"],"content":"Expectation Maximization for GMM The goal of the Expectation-Maximization (EM) algorithm is to find maximum likelihood solutions for models that involve latent variables. Suppose that directly optimizing the likelihood $p(\\mathbf{X}|\\boldsymbol{\\theta})$ is difficult. However, it is easier to optimize the complete-data likelihood function $p(\\mathbf{X}, \\mathbf{Z}|\\boldsymbol{\\theta})$ as as discussed in the previous sections. In such cases, we can use the EM algorithm. EM algorithm is a general technique for finding maximum likelihood solutions in latent variable models. Let‚Äôs begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. By setting the derivatives of $\\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with respect to the means $\\boldsymbol{\\mu}_k$ of the Gaussian components to zero, we obtain \\begin{align*} 0 = -\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}\\boldsymbol{\\Sigma}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k) \\end{align*} Multiplying by $\\boldsymbol{\\Sigma}_k^{-1}$ (which we assume to be non-singular) and rearranging we obtain \\begin{align*} \\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\gamma(z_{nk})\\mathbf{x}_n, \\end{align*} where we have defined \\begin{align*} N_k = \\sum_{n=1}^{N}\\gamma(z_{nk}). \\end{align*} We can interpret $N_k$ as the effective number of points assigned to cluster $k$. We can obtain the MLE solutions for other variables similarly. References: Christoper M. Bishop, Pattern Recognition and Machine Learning, 2006 ","date":"2024-08-18","objectID":"/20240818_latent_variable_part1/:3:2","tags":["machine learning","latent variable","k-means clustering","clustering","gmm","Gaussian mixture modeling"],"title":"Introduction to Latent Variable Modeling (Part 1)","uri":"/20240818_latent_variable_part1/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular value decomposition tutorial","date":"2024-08-15","objectID":"/20240815_svd/","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular Value Decomposition In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square matrix by extending the concept to asymmetric or rectangular matrices, which cannot be diagonalized directly using eigendecomposition. The SVD aims to find the following decomposition of a real-valued matrix $A$: $$A = U\\Sigma V^T,$$ where $U$ and $V$ are orthogonal (orthonormal) matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are called the left singular vectors of $A$, the columns of $V$ are called the right singular vectors, and the diagonal elements of $\\Sigma$ are called the singular values. Despite its widespread applications in areas such as data compression, noise reduction, and machine learning, SVD is often perceived as challenging to grasp. Many people find the mathematical intricacies daunting, even though there are numerous tutorials and explanations available. I remember struggling to fully understand SVD during my undergraduate studies, despite spending significant time on it. The complexity often arises from the abstract nature of the concepts involved, such as the interplay between eigenvectors, eigenvalues, and matrix decompositions. However, understanding SVD is crucial for many advanced techniques in data science and engineering. For instance, if the data matrix $A$ is close to being of low rank, it is often desirable to find a low-rank matrix that approximates the original data matrix well. As we will see, the singular value decomposition of $A$ provides the best low-rank approximation of $A$. The SVD process involves finding the eigenvalues and eigenvectors of the matrices $AA^T$ and $A^TA$. Since $A$ is generally not symmetric, it does not have orthogonal eigenvectors or guaranteed real eigenvalues, which complicates the process of SVD. However, the matrix $A^TA$ is guaranteed to be symmetric, as $$(A^TA)^T=A^TA,$$ and positive semi-definite, meaning all its eigenvalues are non-negative. Symmetric matrices have real eigenvalues and orthogonal eigenvectors, simplifying the decomposition process. These properties ensure that $A^TA$ can be diagonalized using an orthogonal matrix, which is crucial for deriving the SVD. The eigenvectors of $A^TA$ form the columns of $V$. We can diagonalize $A^TA$ as follows: $$A^TA = V\\Lambda V^T = \\sum_{i=1}^{n}\\lambda_i v_iv_i^T = \\sum_{i=1}^{n}\\sigma_i^2v_iv_i^T,$$ where the singular values of $A$ are defined as $\\sigma_i = \\sqrt{\\lambda_i}$. Since $A^TA$ is a symmetric matrix, its eigenvalues are non-negative. The matrix $\\Sigma$ in the SVD is a diagonal matrix whose diagonal entries are the singular values $\\sigma_1, \\dots, \\sigma_r$, where $r$ is the rank of $A$. Note that $rank(A) = rank(A^TA)$, and these singular values appear in the first $r$ positions on the diagonal of $\\Sigma$. For the $i$-th eigenvector-eigenvalue pair, we have $$A^TAv_i = \\sigma_i^2v_i.$$ Now, let‚Äôs derive the eigenvectors of $U$: \\begin{align*} A A^T (Av_i) \u0026= A (\\lambda_i v_i)\\\\ \u0026= \\lambda_i (A v_i). \\end{align*} Thus, $Av_i$ is an eigenvector of $AA^T$. However, to ensure that the matrix $U$ is orthonormal, we need to normalize these vectors as follows: \\begin{align*} u_i \u0026= \\frac{Av_i}{\\lVert Av_i\\rVert} \\\\ \u0026= \\frac{Av_i}{\\sqrt{(Av_i)^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^TA^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^T\\lambda_i v_i}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i \\underbrace{\\lVert v_i\\rVert}_{=1}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i}. \\end{align*} We can express $U$ as follows: $$U= \\left[\\frac{Av_1}{\\sigma_1}, \\dots, \\frac{Av_r}{\\sigma_r}, \\dots, \\frac{Av_n}{\\sigma_n}\\right].$$ Then, we have $$U\\Sigma = AV.$$ By rearranging, we get $$A = U\\Sigma V^{-1}.$$ Since the inverse of an orthogonal matrix $V$ is its transpose, $V^T$, the final form of the SVD is: $$A = U\\Sigma V^T.$$ ","date":"2024-08-15","objectID":"/20240815_svd/:0:0","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning"],"content":"Introduction to Regression: Recursive Least Squares (Part 3)","date":"2024-08-12","objectID":"/20240812_recursive_least_square/","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Deep Dive into Regression: Recursive Least Squares Explained (Part 3) ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:0:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Recursive Least Squares Ordinary least squares assumes that all data is available at once, but in practice, this isn‚Äôt always the case. Often, measurements are obtained sequentially, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where Recursive Least Squares (RLS) comes into play. RLS allows us to update our estimates efficiently as new measurements are obtained, without having to recompute everything from scratch. Suppose we have an estimate $\\boldsymbol{\\theta}_{k-1}$ after $k-1$ measurements and we receive a new measurement $\\mathbf{y}_k$. We want to update our estimate $\\boldsymbol{\\theta}_k$ using the following linear recursive model: \\begin{align*} \\mathbf{y}_k\u0026=\\mathbf{X}_k\\boldsymbol{\\theta} + \\boldsymbol{\\eta}_k\\\\ \\boldsymbol{\\theta}_k\u0026=\\boldsymbol{\\theta}_{k-1} + K_k (\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1}) \\end{align*} where $\\mathbf{X}_k$ is the observation matrix, $K_k$ is the gain matrix, and $\\boldsymbol{\\eta}_k$ represents the measurement error. The term $(\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is the correction term that adjusts our previous estimate using the new data. Also, $\\boldsymbol{\\eta}_k$ is the measurement error. The new estimate is modified from the previous estimate $\\boldsymbol{\\theta}_{k-1}$ with a correction via the gain matrix. To update the estimate optimally, we need to calculate the gain matrix $K_k$. This involves minimizing the variance of the estimation errors at time $k$. The error at step $k$ can be expressed as: \\begin{align*} \\boldsymbol{\\epsilon}_k \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k \\\\ \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1} - K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k (\\mathbf{X}_k\\boldsymbol{\\theta}+\\boldsymbol{\\eta}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k \\mathbf{X}_k(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1})-K_k\\boldsymbol{\\eta}_k\\\\ \u0026= (I-K_k \\mathbf{X}_k)\\boldsymbol{\\epsilon}_{k-1}-K_k\\boldsymbol{\\eta}_k, \\end{align*} where $I$ is the $d\\times d$ identity matrix. The mean of this error is then \\begin{align*} \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}] = (I-K_k \\mathbf{X}_k)\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]-K_k\\mathbb{E}[\\boldsymbol{\\eta}_{k}] \\end{align*} If $\\mathbb{E}[\\boldsymbol{\\eta}_{k}]=0$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]=0$, then $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\\boldsymbol{\\theta}$ is set equal to its expected value, then $\\boldsymbol{\\theta}_k=\\boldsymbol{\\theta}_k, \\forall k$. This property tells us that the estimator $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is unbiased. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\\boldsymbol{\\theta}$ on average. The key task is to find the optimal $K_k$ by minimizing the trace of the estimation-error covariance matrix $P_k = \\mathbb{E}[\\boldsymbol{\\epsilon}_k \\boldsymbol{\\epsilon}_k^T]$. This optimization leads to the following expression for ( K_k ): \\begin{align*} J_k \u0026= \\mathbb{E}[\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k\\rVert^2]\\\\ \u0026= \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}^T\\boldsymbol{\\epsilon}_{k}]\\\\ \u0026= \\mathbb{E}[tr(\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T)]\\\\ \u0026= tr(P_k), \\end{align*} where $P_k=\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T]$ is the estimation-error covariance (i.e., covariance matrix). Note that the third line holds by the trace of a product (i.e., cyclic property) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternative Formulations Sometimes alternate forms of the equations for $P_k$ and $K_k$ are useful for computational purposes. Let‚Äôs first set $\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k = S_k$, then we get $$K_k = P_{k-1}\\mathbf{X}_k^TS_k^{-1}.$$ By putting this into $P_k$, we can obtain \\begin{align*} P_k \u0026= (I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)P_{k-1}(I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)^T+P_{k-1}\\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\\mathbf{X}_kP_{k-1}\\\\ \u0026\\quad \\vdots\\\\ \u0026= P_{k-1}-P_{k-1}\\mathbf{X}_k^TS_k^{-1}\\mathbf{X}_k^TP_{k-1}\\\\ \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}. \\end{align*} Note that $P_k$ is symmetric (c.f., $P_k=\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$. Then, we take the inverse of both sides of $$P_{k-1}^{-1} = \\bigg(\\underbrace{P_{k-1}}_{A}-\\underbrace{P_{k-1}\\mathbf{X}_k^T}_{B}\\big(\\underbrace{\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T}_{D}\\big)^{-1}\\underbrace{\\mathbf{X}_kP_{k-1}}_{C}\\bigg)^{-1}.$$ Next, we apply the matrix inversion lemma which is known as Sherman-Morrison-Woodbury matrix identity (or matrix inversion lemma) identity: $$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$ Then, rewrite $P_k^{-1}$ as follows: \\begin{align*} P_k^{-1} \u0026= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\\mathbf{X}_k^T\\big((\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)-\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\\mathbf{X}_k^T)\\big)^{-1}\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\\\ \u0026= P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k \\end{align*} This yields an alternative expression for the covariance matrix: \\begin{align*} P_k = \\big(P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\big)^{-1} \\end{align*} We can also obtain \\begin{align*} K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1} \\end{align*} By \\begin{align*} P_k \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}\\\\ P_kP_{k-1}^{-1} \u0026= (I-K_k\\mathbf{X}_k)\\\\ P_kP_k^{-1} \u0026= P_kP_{k-1}^{-1}+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k=I\\\\ I \u0026= (I-K_k\\mathbf{X}_k)+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\\\ K_k \u0026= P_{k}\\mathbf{X}_k^TR_{k}^{-1}. \\end{align*} ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:1","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Summary of RLS To summarize, the RLS algorithm can be updated as follows: Gain Matrix Update: $K_k = P_{k-1}\\mathbf{X}_k^T(\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)^{-1}$ or $K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1}$ Estimate Update: $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ Covariance Matrix Update: $P_k = (I-K_k\\mathbf{X}_k)P_{k-1}$. $P_k = (I-K_k \\mathbf{X}_k)P_{k-1}(I-K_k \\mathbf{X}_k)^T+K_kR_kK_k^T,$ ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:2","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternate Derivation of RLS This chapter will be posted soon. Stay tuned for updates! References: Simon Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:2:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression Part 3. RLS","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 2)","date":"2024-08-11","objectID":"/20240811_regression2/","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 2) ","date":"2024-08-11","objectID":"/20240811_regression2/:0:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model. Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data. To address these issues, regularization techniques are often used. Regularization involves adding a penalty term to the model‚Äôs objective function, which helps control the complexity of the model and prevents it from overfitting. ","date":"2024-08-11","objectID":"/20240811_regression2/:1:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined and Underdetermined Problems Recall that linear regression is fundamentally an optimization problem where we aim to find the optimal parameter vector $\\boldsymbol{\\theta}_{opt}$ that minimizes the residual sum of squares between the observed data and the model‚Äôs predictions. Mathematically, this is expressed as: $$\\boldsymbol{\\theta}_{opt} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d}\\lVert y-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2.$$ ","date":"2024-08-11","objectID":"/20240811_regression2/:2:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined Systems An optimization problem is termed overdetermined when the design matrix (or data matrix) $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}$ has more rows than columns, i.e., $m\u003ed$. This configuration means that there are more equations than unknowns, typically leading to a unique solution. In other words, there is more information available than the number of unknowns. The unique solution can be found using the formula: $$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$ The solution exists if and only if $\\mathbf{X}^T\\mathbf{X}$ is invertible, which is true when the columns of $\\mathbf{X}$ are linearly independent, meaning $\\mathbf{X}$ is full rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Underdetermined Systems In contrast, when $\\mathbf{X}$ is fat and short (i.e., $m\u003cd$), the problem is called underdetermined. In this scenario, there are more unknowns than equations, leading to infinitely many solutions. This occurs because the system has less information than the number of unknowns. Among these, the solution that minimizes the squared norm of the parameters is preferred. This solution is known as the minimum-norm least-squares solution. For an underdetermined linear regression problem, the objective can be written as: \\begin{align*} \\boldsymbol{\\theta} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d} \\lVert \\boldsymbol{\\theta}\\rVert^2, \\quad \\textrm{subject to}\\ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} Here, $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}, \\boldsymbol{\\theta}\\in \\mathbb{R}^d,$ and $\\mathbf{y}\\in \\mathbb{R}^m$. If the matrix has rank$(\\mathbf{X})=m$, then the linear regression problem will have a unique global minimum \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} This solution is called the minimum-norm least-squares solution. The proof of this solution is given by: \\begin{align*} \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = \\lVert\\boldsymbol{\\theta}\\rVert^2 + \\boldsymbol{\\lambda}^T(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}), \\end{align*} where $\\boldsymbol{\\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives with respec to $\\boldsymbol{\\lambda}$ and $\\boldsymbol{\\theta}$ and setting them to zero: \\begin{align*} \\nabla_{\\boldsymbol{\\theta}} \u0026= 2 \\boldsymbol{\\theta} + \\mathbf{X}^T\\boldsymbol{\\lambda} = 0\\\\ \\nabla_{\\boldsymbol{\\lambda}} \u0026= \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y} = 0 \\end{align*} The first equation gives us $\\boldsymbol{\\theta} = -\\mathbf{X}^T\\boldsymbol{\\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\\mathbf{X})=N$ so that $\\mathbf{X}^T\\mathbf{X}$ is invertible, we have $\\boldsymbol{\\lambda} = -2 (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}.$ Thus, we have \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} Note that $\\mathbf{X}\\mathbf{X}^T$ is often called a Gram matrix, $\\mathbf{G}$. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Regularization and Ridge Regression Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term that encourages the parameters to behave better, effectively controlling their magnitude. Ridge regression is a widely-used regularization technique that adds a penalty proportional to the square of the magnitude of the model parameters. The objective function for ridge regression is formulated as: \\begin{align*} J(\\boldsymbol{\\theta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2 \\end{align*} This can be expanded as: \\begin{align*} J(\\boldsymbol{\\theta}) = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) + \\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\end{align*} Breaking it down further: \\begin{align*} J(\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\lambda\\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta} \\end{align*} To minimize the objective function $J(\\boldsymbol{\\theta})$, we take the derivative with respect to $\\boldsymbol{\\theta}$ and set it to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + 2\\lambda\\mathbf{I}\\boldsymbol{\\theta} = 0 \\end{align*} Solving for $\\boldsymbol{\\theta}$, we obtain the ridge regression solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{align*} ","date":"2024-08-11","objectID":"/20240811_regression2/:3:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding the Role of $\\lambda$ When $\\lambda$ approaches 0, the regularization term $\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ becomes negligible, making ridge regression equivalent to ordinary least squares (OLS), which can lead to overfitting if the model is too complex. If $\\lambda\\to 0$, then $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\underbrace{\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2}_{=0}$ As $\\lambda$ increases towards infinity, the regularization term dominates, forcing $\\boldsymbol{\\theta}$ towards zero. In this case, the solution becomes overly simplistic, effectively shrinking the model parameters to zero, which may result in underfitting. $\\lambda\\to \\infty$, then $\\underbrace{\\frac{1}{\\lambda}\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2}_{=0} + \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ Since what we want to do is to minimize the objective function, we can divide it by $\\lambda$. Therefore, the solution will be $\\boldsymbol{\\theta}=0$, because it is the smallest value the squared function can achieve. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Spectral Decomposition and Invertibility It‚Äôs important to note that $\\mathbf{X}^T\\mathbf{X}$ is always symmetric. According to the Spectral theorem, this matrix can be decomposed as $\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$, where $\\mathbf{Q}$ is the eigenvector matrix, and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues. This allows us to express the inverse operation in ridge regression as: \\begin{align*} \\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{I}\\\\ \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{Q}\\mathbf{Q}^T\\\\ \u0026= \\mathbf{Q}(\\mathbf{\\Lambda}+\\lambda\\mathbf{I})\\mathbf{Q}^T. \\end{align*} Even if $\\mathbf{X}^T\\mathbf{X}$ is not invertible (or is close to being non-invertible), the regularization constant $\\lambda$ ensures invertibility by making the matrix full-rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Dual Form of Ridge Regression Ridge regression can also be expressed in its dual form, which is particularly useful for solving underdetermined problems: \\begin{align*} (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= \\mathbf{X}^T\\mathbf{y}\\\\ \\boldsymbol{\\theta} \u0026= \\lambda^{-1}\\mathbf{I}(\\mathbf{X}^T\\mathbf{y}-\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\lambda^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\alpha\\\\ \\lambda\\alpha \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\mathbf{X}^T\\alpha)\\\\ \\mathbf{y} \u0026= (\\mathbf{X}\\mathbf{X}^T\\alpha+\\lambda\\alpha)\\\\ \\alpha \u0026= (\\mathbf{X}\\mathbf{X}^T+\\lambda)^{-1}\\mathbf{y}\\\\ \\alpha \u0026= (\\mathbf{G}+\\lambda)^{-1}\\mathbf{y}. \\end{align*} Here, $\\alpha$ represents the dual coefficients, and $\\mathbf{G}$ is the Gram matrix. This formulation is especially powerful when dealing with high-dimensional data, where the number of features exceeds the number of samples. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:3","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Considering Varying Confidence in Measurements: Weighted Regression Up to this point, we have assumed that all measurements are equally reliable. However, in practice, the confidence in different measurements may vary. To account for this, we can consider the situation where the noise associated with each measurement has a zero mean and is independent across measurements. Under these conditions, the covariance matrix for the measurement noise can be expressed as follows: \\begin{align*} R \u0026= \\mathbb{E}(\\eta\\eta^T)\\\\ \u0026= \\begin{bmatrix} \\sigma_1^2 \u0026 \\dots \u0026 0\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ 0 \u0026 \\dots \u0026 \\sigma_l^2 \\end{bmatrix} \\end{align*} By denoting the error vector $\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}$ as $\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements: \\begin{align*} J(\\tilde{\\mathbf{x}}) \u0026= \\boldsymbol{\\epsilon}^TR^{-1}\\boldsymbol{\\epsilon}=\\frac{\\boldsymbol{\\epsilon}_1^2}{\\sigma_1^2}+\\dots+\\frac{\\boldsymbol{\\epsilon}_l^2}{\\sigma_l^2}\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^TR^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) \\end{align*} Note that by dividing each residual by its variance, we effectively equalize the influence of each data point on the overall fitting process. Subsequently, by taking the partial derivative of $J$ with respect to $\\boldsymbol{\\theta}$, we get the best estimate of the parameter, which is given by $$\\boldsymbol{\\theta} = (\\mathbf{X}^TR^{-1}\\mathbf{X})^{-1}\\mathbf{X}^TR^{-1}\\mathbf{y}.$$ Note that the measurement noise matrix $R$ must be non-singular for a solution to exist. To learn more, please take a look at this note ! This article continues in Part 3. References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-11","objectID":"/20240811_regression2/:4:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression Part 2. Ridge Regression","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 1)","date":"2024-08-10","objectID":"/20240810_regression1/","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 1) Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That‚Äôs why I‚Äôve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction. ","date":"2024-08-10","objectID":"/20240810_regression1/:0:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Linear Regression Regression is a method used to identify the relationship between input and output variables. In a regression problem, we are given a set of noisy measurements (or output data) $\\mathbf{y} = [y_1, \\dots, y_d]^T$, which are affected by measurement noise $\\boldsymbol{\\eta} = [\\eta_1, \\dots, \\eta_d]^T$. The corresponding input data is denoted by $\\mathbf{x} = [x_1, \\dots, x_d]$. We refer to the collection of these input-output pairs as the training data, $\\mathcal{D} = {(\\mathbf{x}_1, \\mathbf{y}_1), \\dots, (\\mathbf{x}_m, \\mathbf{y}_m)}$. The true relationship between the input and output data is unknown and is represented by a function $f(\\cdot)$ that maps $\\mathbf{x}_n$ to $y_n$, i.e., $$ \\mathbf{y} = f(\\mathbf{x}). $$ Determining the exact function $f(\\cdot)$ from a finite set of data points $\\mathcal{D}$ is not feasible because there are infinitely many possible mappings for each $\\mathbf{x}_i$. The idea of regression is to introduce structure to the problem. Instead of trying to find the true $f(\\cdot)$, we seek an approximate model $g_\\theta(\\cdot)$, which is parameterized by $\\boldsymbol{\\theta} = [\\theta_1,\\dots,\\theta_d]^T$. For example, we might assume a linear relationship between $(\\mathbf{x}_n, \\mathbf{y}_n)$: \\begin{align*} g_{\\boldsymbol{\\theta}}(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}, \\end{align*} where $\\mathbf{X}$ is an $m \\times d$ input matrix derived from our observations. Since the true relationship is unknown, any chosen model is essentially a hypothesis. However, we can quantify the error in our model. Given a parameter $\\boldsymbol{\\theta}$, the error between the noisy measurements and the estimated values is: \\begin{align*} \\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} The goal of regression is to find the best $\\boldsymbol{\\theta}$ that minimizes this error. This leads us to the following objective function: \\begin{align*} J(\\boldsymbol{\\theta}) = \\boldsymbol{\\epsilon}^T \\boldsymbol{\\epsilon}. \\end{align*} This objective function is equivalent to minimizing the mean squared error (MSE): \\begin{align*} MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{x}_i \\boldsymbol{\\theta})^2. \\end{align*} We can optimize this function in closed form as follows: \\begin{align*} J(\\boldsymbol{\\theta}) \u0026= \\lVert\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}\\rVert^2_2 \\\\ \u0026= (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) \\\\ \u0026= \\mathbf{y}^T \\mathbf{y} - \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}. \\end{align*} To find the $\\boldsymbol{\\theta}$ that minimizes the objective function, we compute the derivative of the function and set it equal to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} = 0, \\end{align*} which simplifies to: \\begin{align*} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0, \\end{align*} leading to the solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}. \\end{align*} The equation $\\mathbf{X}^T(\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0$ is known as the normal equation. ","date":"2024-08-10","objectID":"/20240810_regression1/:1:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Python Code Let‚Äôs implement a simple regression in Python: import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.randn(N) w_1 = 3.4 # True Parameter w_0 = 0.9 # True Parameter y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) W = np.array([w_1, w_0]) # From Scratch XtX = np.dot(X.T, X) XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m W_best = np.dot(XtXinvX, y.T) print(f\"W_best: {W_best}\") # Pythonic Approach theta = np.linalg.lstsq(X, y, rcond=None)[0] print(f\"Theta: {theta}\") t = np.linspace(0, 1, 200) y_pred = W_best[0]*t+W_best[1] yhat = theta[0]*t+theta[1] plt.plot(x, y, 'o') plt.plot(t, y_pred, 'r', linewidth=4) plt.show() To learn more, please take a look at this note ! This article continues in Part 2 . References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 ","date":"2024-08-10","objectID":"/20240810_regression1/:1:1","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression Part 1. Basics","uri":"/20240810_regression1/"},{"categories":["gpg"],"content":"Guide to encrypt/decrypt data using GPG","date":"2024-08-04","objectID":"/20240804_gpg_encryption/","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Securing Your Privacy The importance of securing your data has become critical in the modern digital era. This post explores a versatile tool called GnuPG, or GNU Privacy Guard, which allows you to encrypt your data and communications, ensuring that only the intended recipients can access them. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:0:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Asymmetric Encryption Before looking at GPG, let‚Äôs first review some encryption approaches. A very naive approach to sharing encrypted files is to use the same secret key between a sender and a receiver. This approach is known as symmetric encryption. However, the symmetric approach has a limitation in that it requires a secure method for key exchange. To address this issue, asymmetric encryption is preferred. Asymmetric encryption, also known as public-key cryptography, is a method of encryption that uses a pair of keys: a public key and a private key, to encrypt and decrypt data. The public key is used for encryption. It is openly shared and can be distributed to anyone, allowing anyone to encrypt a message intended for the key owner. The private key is used for decryption. It is kept secret and known only to the owner, allowing the key owner to decrypt messages that were encrypted with the corresponding public key. This is how asymmetric encryption works: Key Pair Generation: A user generates a pair of keys: one public and one private. The public key is shared widely, while the private key is kept secure. Encryption: When someone wants to send a secure message, they use the recipient‚Äôs public key to encrypt the message. This ensures that only the recipient, who has the corresponding private key, can decrypt and read the message. Decryption: The recipient uses their private key to decrypt the received message. The private key is the only key that can decrypt the message encrypted with the corresponding public key. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:1:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"How to Use GPG? Installation: sudo apt-get install gnupg # Ubuntu sudo pacman -S gnupg # Arch Generate a GPG Key: gpg --full-gen-key Export Public Key: gpg --export --armor your-email@example.com \u003e publickey.asc Import Public Key: gpg --import publickey.asc Encrypt a File: gpg --output encryptedfile.gpg --encrypt --recipient recipient@example.com file.txt Decrypt a File: gpg --output file.txt --decrypt encryptedfile.gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:2:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Singinig/Verifying Files A detached signature is a separate file that contains the signature of the original file. gpg --output file.sig --detach-sign file.txt file.txt is the file you want to sign. file.sig is the signature file generated. To verify the detached signature: gpg --verify file.sig file.txt To sign text files, gpg --clearsign \u003cfile name\u003e This will create a file called file.txt.asc which contains the original text and the signature. To verify gpg --verify \u003cfile.asc\u003e ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:3:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"A Simple GitHub Verification using ssh with gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Generate SSH key ssh-keygen -t rsa or simply ssh-keygen id_rsa : private key id_rsa.pub : public key Go to SSH and GPG keys in the setting menu of GitHub Just paste your public key ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:1","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["Tools"],"content":"Guide to manage your tasks using TaskSpooler","date":"2024-07-13","objectID":"/20240713_taskspooler/","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"What is TaskSpooler ? TaskSpooler (ts) is a lightweight job scheduler that allows you to queue up your tasks and execute them in order. It‚Äôs particularly useful for environments where tasks need to be managed sequentially or with a controlled degree of parallelism. Unlike more complex systems like SLURM, TaskSpooler is designed for simplicity and ease of use, making it accessible for individual researchers and small teams. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:0:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Efficient Job Scheduling for ML/DL Researchers with Taskspooler In the dynamic field of Machine Learning (ML) and Deep Learning (DL), managing and optimizing computational resources is crucial. For researchers frequently running numerous experiments, an efficient job scheduler can be a game-changer. Enter Taskspooler, a powerful yet user-friendly job scheduler for Linux, designed to help you manage and schedule your jobs in a queue. Taskspooler is a simpler alternative to SLURM, providing many benefits for ML/DL researchers, especially when it comes to utilizing GPUs efficiently. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"TL;DR Job Queuing: Easily queue up multiple jobs, specifying the order of execution. Resource Management: Find and allocate empty GPUs to your tasks, maximizing resource utilization. Monitoring: Track the status of your jobs in real-time. Simplicity: A straightforward command-line interface that requires minimal setup and configuration. Parallel Execution: Control the number of jobs running simultaneously, which is essential for managing GPU workloads effectively. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Dive into TaskSpooler ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Installation First, clone the repository: git clone https://github.com/justanhduc/task-spooler Install GPU Version To set up Task Spooler with GPU support, run the provided script: ./install_make Alternatively, to use CMake: ./install_cmake If Task Spooler is already installed, and you want to reinstall or upgrade, run: ./reinstall Install CPU Version If you would like to install only the CPU version, use the following commands (recommended): make cpu sudo make install or via CMake: mkdir build \u0026\u0026 cd build cmake .. -DTASK_SPOOLER_COMPILE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release make sudo make install ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Basic Usage Let‚Äôs first put your task into a queue: ts python main.py This command queues up your python main.py. Keeping track of running and pending jobs is crucial for optimizing your workflow. Taskspooler provides real-time updates on job status, allowing you to make informed decisions and adjustments on the fly. To check the overall queue status, simply type: ts This returns your jobs with the job ID, state, time, and the command you typed. To track the status of your jobs: ts -c \u003cjob-id\u003e You can delete finished jobs in the job list by: ts -C To set the size of your job queue (i.e., to limit/expand the number of parallel running processes): ts -S \u003cqueue-size\u003e ","date":"2024-07-13","objectID":"/20240713_taskspooler/:3:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"GPU Utilization For ML/DL researchers, GPUs are invaluable but often limited resources. Taskspooler helps you find available GPUs and assign tasks to them efficiently. This ensures that your experiments run smoothly without unnecessary delays due to resource contention. To specify GPU indices for a job without checking whether they are free, use: ts -g [id,...] python main.py For instance: ts -g 1 python main.py This allows you to run your model on GPU 1. To get the GPU usage: ts -g ","date":"2024-07-13","objectID":"/20240713_taskspooler/:4:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Conclusion Taskspooler offers a simple yet powerful solution for job scheduling and resource management, making it an excellent tool for ML/DL researchers. By effectively queuing your tasks and optimizing GPU usage, you can streamline your workflow and focus more on the research itself rather than managing computational resources. Whether you‚Äôre working on a single machine or a small cluster, Taskspooler can significantly enhance your productivity and efficiency. If you want to know more visit its official github repo ","date":"2024-07-13","objectID":"/20240713_taskspooler/:5:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Python"],"content":"Guide to keep manage dependencies in Python","date":"2024-07-07","objectID":"/20240707_poetry/","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Introduction Poetry is a dependency management and packaging tool in Python, aiming to improve how you define, install, and manage project dependencies. Installation: You can install Poetry through its custom installer script or using package managers. The recommended way is to use their installer script to ensure you get the latest version. Creating a New Project: Use poetry new \u003cproject-name\u003e to create a new project with a standard layout. Adding Dependencies: Add new dependencies directly to your project using poetry add \u003cpackage\u003e. Poetry will resolve the dependencies and update the pyproject.toml and poetry.lock files. Installing Dependencies: Running poetry install in your project directory will install all dependencies defined in your pyproject.toml file. ","date":"2024-07-07","objectID":"/20240707_poetry/:0:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Poetry Example ","date":"2024-07-07","objectID":"/20240707_poetry/:1:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Setting Up a New Project To create a new project named example_project with Poetry and manage its dependencies: poetry new example_project This command creates a new directory named example_project with some initial files, including a pyproject.toml file for configuration. The pyproject.toml file is what is the most important here. This will orchestrate your project and its dependencies. For now, it looks like this: [tool.poetry] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [\"author \u003cauthor@xxxxx.xxx\u003e\"] readme = \"README.md\" packages = [{include = \"poetry_demo\"}] [tool.poetry.dependencies] python = \"^3.7\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" we are allowing any version of Python 3 that is greater than 3.7.0. If you want to use Poetry only for dependency management but not for packaging, you can use the non-package modei: [tool.poetry] package-mode = false ","date":"2024-07-07","objectID":"/20240707_poetry/:1:1","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Add and Install Dependencies Suppose your project depends on requests for making HTTP requests and pytest for testing. To add these: poetry add requests poetry add pytest --dev The --dev flag indicates that pytest is a development dependency, not required for production. To remove a package, poetry remove \u003cpackage\u003e Running the command below will install all dependencies listed in your pyproject.toml: poetry install This also creates a virtual environment for your project if it doesn‚Äôt already exist. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:2","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Run Your Project Run directly To run a script or start your project within the Poetry-managed virtual environment: poetry run python my_script.py This command ensures that python and any other commands are run within the context of your project‚Äôs virtual environment, using the correct versions of Python and all dependencies. Entry Point In Poetry, an entry point is a way to specify which Python script should be executed when your package is run. This is particularly useful for creating command-line applications or defining executable scripts that can be run directly from the command line after your package is installed. To define an entry point in a Poetry project, you use the [tool.poetry.scripts] section in your pyproject.toml file. This section allows you to map a command name to a Python function, which will be executed when the command is run. Edit your pyproject.toml file to include the [tool.poetry.scripts] section. This is where you define your entry point. Add the following lines to the pyproject.toml file: [tool.poetry.scripts] greet-app = \"greet_app.cli:main\" Here, greet-app is the command name, and greet_app.cli:main specifies the main function in the cli.py module inside the greet_app package. Shell Using Virtual Environment Shell: If you frequently run commands, you might find it convenient to activate the Poetry-managed virtual environment shell: poetry shell This will activate the virtual environment, and you can run your command without prefixing it with poetry run: greet-app To exit this new shell type exit. To deactivate the virtual environment without leaving the shell use deactivate. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:3","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Init Method Instead of creating a new project, Poetry can be used to ‚Äòinitialise‚Äô a pre-populated directory. To interactively create a pyproject.toml file in directory pre-existing-project: poetry init Then, install it by poetry install To see the information about the project poetry env info -p: prints a path of the virtual environment By setting a config, you can generate a virtual environment in your projects: poetry config virtualenvs.in-project true ","date":"2024-07-07","objectID":"/20240707_poetry/:1:4","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Use with Git Poetry automatically creates a .gitignore file for you. It should include entries to ignore the virtual environment directory (.venv if you use Poetry‚Äôs built-in virtual environment management). Whenever you add or update dependencies, make sure to commit the pyproject.toml and poetry.lock files: To clone your project: git clone \u003cyour-repository-url\u003e cd my-project poetry install ","date":"2024-07-07","objectID":"/20240707_poetry/:2:0","tags":["Python","Poetry","Virtual Environment","package manager"],"title":"Dependency Management in Python: Poetry","uri":"/20240707_poetry/"},{"categories":["Linux"],"content":"Pacman tutorial","date":"2024-05-01","objectID":"/20240501_pacman/","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Pacman , the package manager for Arch Linux, is known for its simple binary package format and easy-to-use build system . The primary aim of Pacman is to facilitate straightforward management of packages from both the official repositories and user-generated builds. Pacman ensures your system remains updated by synchronizing the package lists with the master server. This client/server model simplifies the process of downloading and installing packages, along with all their dependencies, using basic commands. ","date":"2024-05-01","objectID":"/20240501_pacman/:0:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Installing and Upgrading Packages Install a Package: sudo pacman -S \u003cpackage-name\u003e Full System Upgrade: sudo pacman -Syu -y synchronizes the database, similar to sudo apt-get update. -u upgrades all out-of-date packages, akin to sudo apt-get upgrade. Installing Packages from Git: Clone the package and install: git clone \u003crepository-url\u003e makepkg -si ","date":"2024-05-01","objectID":"/20240501_pacman/:0:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Removing Packages Remove a Specific Package: sudo pacman -R \u003cpackage-name\u003e Using -s removes dependencies not required by other packages, but be cautious as it may affect dependencies needed by other programs. Best Practice for Removing Packages: sudo pacman -Rns \u003cpackage-name\u003e Remove Obsolete Packages: sudo pacman -Sc ","date":"2024-05-01","objectID":"/20240501_pacman/:0:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Package Lists List All Installed Packages: sudo pacman -Q List Manually Installed Packages: sudo pacman -Qe List Installed AUR Packages: sudo pacman -Qm List Unneeded Dependencies: sudo pacman -Qdt ","date":"2024-05-01","objectID":"/20240501_pacman/:0:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Getting Started with Pacman Creating a List of Installed Packages: Generate a list to easily reinstall packages on a new system: pacman -Qqen \u003e pkglist.txt To reinstall packages from the list: pacman -S - \u003c pkglist.txt ","date":"2024-05-01","objectID":"/20240501_pacman/:1:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Rollback to Previous Versions List Cached Packages: ls /var/cache/pacman/pkg/ To downgrade a package: sudo pacman -U \u003cpackage-file\u003e ","date":"2024-05-01","objectID":"/20240501_pacman/:1:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Mirror Lists Edit and Refresh Mirror List: sudo vim /etc/pacman.d/mirrorlist sudo pacman -Syy Use -yy to force a refresh of the package databases, even if they are up to date. ","date":"2024-05-01","objectID":"/20240501_pacman/:1:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Configuration Tips Enable Parallel Downloads: Open your configuration file: sudo vim /etc/pacman.conf Then uncomment or add the following line to enable multiple simultaneous downloads: ParallelDownloads=5 Ignore Specific Packages: Add the following line to /etc/pacman.conf to prevent specific packages from being updated: IgnorePkg = postgresql* ","date":"2024-05-01","objectID":"/20240501_pacman/:1:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Vim"],"content":"The Appeal of Vim in Modern Programming","date":"2024-05-01","objectID":"/20240501_vim/","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Vim"],"content":"While it may look somewhat obsolete in an era dominated by graphically rich IDEs, Vim remains not just a highly relevant and effective tool for today‚Äôs programmers but also a badge of coolness in the tech world. Those who master its commands are often seen as coding wizards. With its unique advantages in speed, efficiency, and customizability, Vim is an invaluable asset in software development environments, proving that old-school can still be trendy. Vim is celebrated for its minimalistic approach, using fewer system resources, which facilitates fast and responsive editing. This efficiency is further enhanced by Vim‚Äôs command-driven interface, allowing developers to perform complex edits quickly through simple keystrokes. This minimizes the need for mouse use, thereby reducing the risk of repetitive strain injuries on the wrists. Here‚Äôs an example of how to yank (copy) and paste in Vim: To yank (copy) a line in Vim, just press yy. To paste the yanked text, simply move the cursor to where you want to insert the copied text and press p to paste after the cursor position or P to paste before it. The universal Ctrl+C to copy and Ctrl+V to paste are straightforward and familiar to most users. However, this often requires alternating between the keyboard and mouse, which can slow down the editing process and increase physical strain on the wrists. Vim‚Äôs approach is designed to keep your fingers on the keyboard, reducing the need to switch to a mouse and enhancing focus and productivity. This is especially beneficial in coding and scripting environments where rapid navigation and changes are common. Beyond its capabilities as a programming editor, Vim excels as a general-purpose text editor. It‚Äôs an excellent tool for note-taking and managing documentation, thanks to its lightweight nature and fast operation. Moreover, Vim is highly effective for composing complex documents in LaTeX (See my machine learning study notes !), allowing users to edit large amounts of text with ease and precision. The ability to customize Vim with plugins and scripts also extends its functionality into areas like PDF viewing and reference management. Customizability is another cornerstone of Vim‚Äôs design. Developers can adjust nearly every aspect of the editor to fit their workflow, from key bindings to complex integrations. Vim‚Äôs adaptability extends to broader system configurations, seamlessly integrating with tools like the i3 window manager (i3wm) and file browsers like LF and Ranger. This integration allows for a more unified and efficient desktop environment, where everything from file management to window resizing is streamlined through Vim-like commands. Mastering Vim not only boosts your coding efficiency but also earns you some cool points among tech peers who appreciate the power and elegance of classic tools. I will post more Vim tips soon! ","date":"2024-05-01","objectID":"/20240501_vim/:0:0","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Python"],"content":"Why Use Python's `pdb` Debugger Over an IDE?","date":"2024-04-27","objectID":"/20240426_pdb/","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"When it comes to debugging Python code, most programmers reach for an Integrated Development Environment (IDE) because of its convenience and powerful features. However, there‚Äôs a classic, built-in tool that shouldn‚Äôt be overlooked: Python‚Äôs own debugger, pdb. This tool might seem basic at first glance, but it offers some compelling advantages, especially in scenarios where an IDE might be less effective. Here‚Äôs why you might consider using pdb for debugging your Python projects: ","date":"2024-04-27","objectID":"/20240426_pdb/:0:0","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Simplicity pdb comes as part of Python‚Äôs standard library, which means it‚Äôs ready to use out of the box‚Äîno installation or complex setup required. If you‚Äôre working on a simple script or need a quick debugging session, pdb is just a few keystrokes away. pdb offers an interactive session that lets you control the flow of your program. You can step through your code line by line, inspect and modify variables, and execute Python commands on the fly. This hands-on control can make finding and fixing bugs much clearer and sometimes even faster. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:1","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Environment Independence One of pdb‚Äôs greatest strengths is its versatility. Whether you‚Äôre coding on a local machine, a remote server, or even in a container, pdb works just the same. This universal compatibility is a huge plus, particularly when dealing with production environments where installing a full-fledged IDE isn‚Äôt feasible. Also, pdb operates entirely in the terminal, it‚Äôs perfect for low-resource environments or situations where a graphical interface might slow you down. This makes pdb incredibly efficient and responsive, even over network connections like SSH. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:2","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Flexibility in Use You can start pdb in several ways: directly from the command line, by inserting a breakpoint in your code, or as a module. This flexibility allows you to adapt your debugging approach to the needs of each specific project or problem. For developers who prefer working in text editors like Vim or Emacs, pdb integrates smoothly, enabling powerful debugging without leaving your editor. This integration supports a streamlined workflow, particularly for those who favor a more textual or minimalist development environment. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:3","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Conclusion While modern IDEs are undeniably powerful and user-friendly, pdb holds its own with features that are particularly suited to debugging in a variety of environments and situations. It‚Äôs a tool that encourages mastery of debugging by getting you close to the code in a way that GUI tools sometimes can‚Äôt match. Whether you‚Äôre a beginner looking to understand the inner workings of Python or an experienced developer needing a reliable tool on a remote server, pdb is worth exploring. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:4","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"A tutorial for Pydantic","date":"2024-04-26","objectID":"/20240426_pydantic/","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Python‚Äôs dynamic typing system is indeed convenient, allowing you to create variables without explicitly declaring their types. While this flexibility can streamline development, it can also introduce unexpected behavior, particularly when handling data from external sources like APIs or user input. Consider the following scenario: employee = Employee(\"Han\", 30) # Correct employee = Employee(\"Moon\", \"30\") # Correct Here, the second argument is intended to represent an age, typically an integer. However, in the second example, it‚Äôs a string, potentially leading to errors or unexpected behavior down the line. To address such issues, Pydantic offers a solution through data validation. Pydantic is a library specifically designed for this purpose, ensuring that the data conforms to pre-defined schemas. The primary method of defining schemas in Pydantic is through models. Models are essentially classes that inherit from pydantic.BaseModel and define fields with annotated attributes. You can think of models as similar to structs in languages like C. While Pydantic models share similarities with Python‚Äôs dataclasses, they are preferred when data validation is essential. Pydantic models guarantee that the fields of an instance will adhere to specified types, providing both runtime validation and serving as type hints during development. Let‚Äôs illustrate this with a simple example: from pydantic import BaseModel class User(BaseModel): id: int name: str = \"John Doe\" User model has two fields: id integer and name string, which has a default value. You can create an instance, user = User(id=\"123\") You can also define models that include other models, allowing for complex data structures: from typing import List class Item(BaseModel): name: str price: float class Order(BaseModel): items: List[Item] total_price: float order = Order(items=[{\"name\": \"Burger\", \"price\": 5.99}, {\"name\": \"Fries\", \"price\": 2.99}], total_price=8.98) print(order) ","date":"2024-04-26","objectID":"/20240426_pydantic/:0:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Validators Pydantic provides a versatile decorator called validator, which enables you to impose custom validation rules on model fields. These validators extend beyond simple type validation and allow you to enforce additional checks. Here‚Äôs how you can define and utilize a custom validator: from pydantic import BaseModel, validator class Person(BaseModel): name: str age: int @validator('age') def check_age(cls, value): if value \u003c 18: raise ValueError('Age must be at least 18') return value # This will raise an error because the age is below 18 try: Person(name=\"Charlie\", age=17) except Exception as e: print(e) In this example, the custom validator ensures that the age provided is at least 18 years. Custom validators can target individual fields, multiple fields, or the entire model, making them invaluable for enforcing complex validation logic or cross-field constraints. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Built-in Validators Pydantic models leverage Python type annotations to enforce data types. Alongside the fundamental types like str, int, float, bool, Pydantic supports complex data types such as List, Dict, Union, and Optional, among others. These annotations are the first level of validation: from pydantic import BaseModel from typing import List, Optional class User(BaseModel): name: str age: int tags: Optional[List[str]] = None In this example, name must be a string, age an integer, and tags is an optional list of strings. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Field Validation For more detailed validation, Pydantic‚Äôs Field function can be used to specify additional constraints: from pydantic import BaseModel, Field class User(BaseModel): id: int name: str email: str = Field(..., description=\"The email address of the user\") age: int = Field(..., gt=0, description=\"The age of the user\") # Usage user_data = {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\", \"age\": 30} user = User(**user_data) In this example: id, name, email, and age represents fields in the User model. id and name are required fields because they don‚Äôt have a default value. email and age have default values specified using the Field class. For email, ... indicates that it‚Äôs required, and a description is provided. For age, ... indicates that it‚Äôs required, and it must be greater than zero (gt=0). By using Field, you can define additional constraints such as minimum and maximum values, regular expressions for string fields, custom validation functions, etc., to ensure that your data meets specific criteria. age: int = Field(..., gt=0, description=\"The age of the user\") For instance, you can specifies that the age must be greater than 0. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:2","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Root Validators For validation that involves multiple fields, you can use root validators. These are applied to the whole model instead of individual fields: from pydantic import BaseModel, root_validator class Account(BaseModel): username: str password1: str password2: str @root_validator def passwords_match(cls, values): password1, password2 = values.get('password1'), values.get('password2') if password1 and password2 and password1 != password2: raise ValueError('Passwords do not match') return values Root validators have access to all field values of the model, making them ideal for validations that depend on multiple fields. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:3","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pre-Validators and Post-Validators Pre-validators: A pre-validator in Pydantic is used to preprocess or transform the data before it undergoes the main validation process. This is particularly useful when you need to adjust or prepare the incoming data so it can be successfully validated. For instance, you might want to strip whitespace from a string, convert data types, or decompose compound fields into simpler components before validation. from pydantic import BaseModel, validator class TrimmedStringModel(BaseModel): text: str @validator('text', pre=True) def strip_whitespace(cls, value): return value.strip() Post-validator Post-validators are used to validate or transform data after the main validation process. They are useful when certain validations depend on multiple fields or when you need to enforce complex constraints that are not covered by basic type annotations. Post-validators are also defined using the @validator decorator but without specifying pre=True. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:4","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Json Serialization It is really simple to convert Pydantic models to or from JSON. For example, user_json = user.json() You can convert your model instance to JSON file as above. Or you can make a dictionary by user.dict() Conversely, json_str = '{\"name\": \"Han\", \"account\": 1234}' User.parse_raw(json_str) ","date":"2024-04-26","objectID":"/20240426_pydantic/:2:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic for Config Pydantic can also be used for settings management by loading configuration from environment variables: from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: str database_password: str my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) This feature is particularly useful for 12-factor apps that require configuration through the environment for different deployment environments. Pydantic provides a powerful system for data validation, allowing you to enforce type constraints and custom validation rules on your data models. This capability ensures that the data your application works with is correct and consistent, reducing runtime errors and simplifying data handling. Let‚Äôs explore more about validation in Pydantic, including built-in validators and how to write custom validation functions. ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic SecretStr Pydantic‚Äôs SecretStr is a special data type designed to handle sensitive information, such as passwords or secret tokens, in a more secure manner. This type is part of Pydantic‚Äôs data types that provide tools for sensitive data, ensuring that such information isn‚Äôt accidentally printed or logged, which could lead to security vulnerabilities. from pydantic import BaseModel, SecretStr class User(BaseModel): username: str password: SecretStr # Usage user_data = {\"username\": \"john_doe\", \"password\": \"secretpassword\"} user = User(**user_data) print(user) # Output: User username='john_doe' password=SecretStr('********') from pydantic import BaseModel, SecretBytes class EncryptedData(BaseModel): data: SecretBytes # Usage encrypted_data = {\"data\": b\"encrypted binary data\"} data_object = EncryptedData(**encrypted_data) print(data_object) # Output: EncryptedData data=SecretBytes('********') from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: SecretStr database_password: SecretStr my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"A tutorial for Enum","date":"2024-04-26","objectID":"/20240426_enum/","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Enum is a way that Python enumerate variables. The enum module allows for the creation of enumerated constants‚Äîunique, immutable data types that are useful for representing a fixed set of values. These values, which are usually related by their context, are known as enumeration members. Enum provides‚Ä¶ Uniqueness - Each member of an Enum is unique within its definition, meaning no two members can have the same value. Attempting to define two members with the same value will result in an error unless you explicitly allow aliases. Immutability - Enum members are immutable. Once the Enum class is defined, you cannot change the members or their values. Iterability and Comparability - Enum classes support iteration over their members and can be compared using identity and equality checks. Accessing Members - You can access enumeration members by their names or values: Auto - If you want to automatically assign values to enum members, you can use the auto() function from the same module: from enum import Enum class State(Enum): PLAYING=0 PAUSED=1 GAME_OVER=2 If we just want to make sure them to be unique and automatically assigned, then use auto() from enum import Enum, auto class State(Enum): PLAYING=auto() PAUSED=auto() GAME_OVER=auto() print(State.PLAYING) print(State.PLAYING.value) Or simply, from enum import Enum, auto class State(Enum): PLAYING, PAUSED, GAME_OVER=range(3) print(State.PLAYING) print(State.PLAYING.value) However, this hard codes numbers, which can create an issue in the future. ","date":"2024-04-26","objectID":"/20240426_enum/:0:0","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Iterating over Enum Members You can iterate over the members of an enum: for state in State: print(state) ","date":"2024-04-26","objectID":"/20240426_enum/:0:1","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Comparison of Enum Members Enum members are singleton objects, so comparison is possible by identity: if State.PLAYING is State.PLAYING: print(\"RED is indeed RED\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:2","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Using Enum as a Type Hint Enums can be used as type hints, enhancing code readability and correctness: def paint(color: Color): print(f\"Painting with {color.name}\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:3","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Extending Enums: IntEnum and StrEnum For enums where the members are specifically integers or strings, you can inherit from IntEnum or StrEnum for additional benefits, like being able to compare members to integers or strings directly. from enum import IntEnum class Priority(IntEnum): LOW = 1 MEDIUM = 2 HIGH = 3 # Direct comparison with integers if Priority.LOW \u003c Priority.HIGH: print(\"Low priority is less than high\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:4","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Unique Constraint To ensure that all enum values are unique, you can use the @unique decorator: from enum import Enum, unique @unique class StatusCode(Enum): OK = 200 NOT_FOUND = 404 ERROR = 500 Using @unique will raise a ValueError if any duplicate values are detected. ","date":"2024-04-26","objectID":"/20240426_enum/:0:5","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Conclusion Enums in Python are useful for defining sets of named constants that are related and have a fixed set of members. They improve code readability, prevent errors related to using incorrect literal values, and can simplify type checking and validation in your programs. ","date":"2024-04-26","objectID":"/20240426_enum/:0:6","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"A tutorial for Pytest","date":"2024-04-26","objectID":"/20240426_unit-tests/","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Unit testing involves testing individual components of software in isolation to ensure they function correctly. Automated frameworks facilitate this process, which is integral to ensuring that new changes do not disrupt existing functionality. Unit tests also serve as practical documentation and encourage better software design. This testing method boosts development speed and confidence by confirming component reliability before integration. Early bug detection through unit testing also helps minimize future repair costs and efforts. pytest Pytest is one of the best tools that you can use to boost your testing productivity for Python codes. ","date":"2024-04-26","objectID":"/20240426_unit-tests/:0:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Install pip install pytest pip install pytest-cov pytest --cov: this returns a coverage of test functions coverage html: log test results in html format ","date":"2024-04-26","objectID":"/20240426_unit-tests/:1:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Example pytest is a libarary for testing. You can run your unit test code by pytest test_function.py If you wanna create a directory with several testing files, then just create __init__.py and put it inside the test dir. Then, just run pytest test_dir from calc import square import pytest def square(n): return n*n # This is a convention test_{func} def test_negative(): assert square(-2)==4 assert square(-3)==9 def test_positive(): assert square(2)==4 assert square(3)==9 def test_zero(): assert square(0)==0 def test_str(): # Write down what I expect to get # If it successfully raised the error that I expected # Then, it will pass the test with pytest.raises(TypeError): square(\"cat\") def main(): x = int(input(\"What's x? \")) print(\"x squared is\", square(x)) if __name__==\"__main__\": main() If you want to intentially raise an error, then you can do it by pytest.raises(\"SomeErrorType\") Note that you can write a warning message like assert x == \u003ccond\u003e, \u003cMSG\u003e ","date":"2024-04-26","objectID":"/20240426_unit-tests/:2:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Programming"],"content":"A gentle introduction to bash scripting","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Let‚Äôs create our first simple shell script #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Unix that the file is to be executed by /bin/sh. This is the standard location of the Bourne shell on just about every Unix system. If you‚Äôre using GNU/Linux, /bin/sh is normally a symbolic link to bash (or, more recently, dash). The second line begins with a special symbol: #. This marks the line as a comment, and it is ignored completely by the shell. The only exception is when the very first line of the file starts with #! (shebang) - as ours does. This is a special directive which Unix treats specially. It means that even if you are using csh, ksh, or anything else as your interactive shell, that what follows should be interpreted by the Bourne shell. Similarly, a Perl script may start with the line #!/usr/bin/perl to tell your interactive shell that the program which follows should be executed by perl. For Bourne shell programming, we shall stick to #!/bin/sh. The third line runs a command: echo, with two parameters, or arguments - the first is \"Hello\"; the second is \"World\". Note that echo will automatically put a single space between its parameters. To make it executable, run chmod +rx \u003cfilename\u003e ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:0:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables Let‚Äôs look back at our first Hello World example. This could be done using variables. Note that there must be no spaces around the ‚Äú=‚Äù sign: VAR=value works; VAR = value doesn‚Äôt work. In the first case, the shell sees the ‚Äú=‚Äù symbol and treats the command as a variable assignment. In the second case, the shell assumes that VAR must be the name of a command and tries to execute it. #!/bin/sh MY_MESSAGE=\"Hello World\" echo $MY_MESSAGE This assigns the string ‚ÄúHello World‚Äù to the variable MY_MESSAGE then echoes out the value of the variable. Note that we need the quotes around the string Hello World. Whereas we could get away with echo Hello World because echo will take any number of parameters, a variable can only hold one value, so a string with spaces must be quoted so that the shell knows to treat it all as one. Otherwise, the shell will try to execute the command World after assigning MY_MESSAGE=Hello The shell does not care about types of variables; they may store strings, integers, real numbers - anything you like. We can interactively set variable names using the read command; the following script asks you for your name then greets you personally #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME - hope you're well.\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:1:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Escape Characters Certain characters are significant to the shell; for example, that the use of double quotes (\") characters affect how spaces and TAB characters are treated, for example: $ echo Hello World Hello World $ echo \"Hello World\" Hello World So how do we display: Hello¬†\"World\" ? $ echo \"Hello \\\"World\\\"\" The first and last \" characters wrap the whole lot into one parameter passed to echo so that the spacing between the two words is kept as is. But the code: $ echo \"Hello \" World \"\" would be interpreted as three parameters: ‚ÄúHello¬†\" World \"‚Äù So the output would be Hello World Note that we lose the quotes entirely. This is because the first and second quotes mark off the Hello and following spaces; the second argument is an unquoted ‚ÄúWorld‚Äù and the third argument is the empty string; ‚Äú‚Äù. ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:2:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Loop ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"For Loop #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i\" done #!/bin/sh for i in hello 1 * 2 goodbye do echo \"Looping ... i is set to $i\" done The output of the above code is Looping ... i is set to hello Looping ... i is set to 1 Looping ... i is set to (name of first file in current directory) ... etc ... Looping ... i is set to (name of last file in current directory) Looping ... i is set to 2 Looping ... i is set to goodbye This is well worth trying. Make sure that you understand what is happening here. Try it without the * and grasp the idea, then re-read the Wildcards section and try it again with the * in place. Try it also in different directories, and with the * surrounded by double quotes, and try it preceded by a backslash (\\*) ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:1","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"While Loop #!/bin/sh INPUT_STRING=hello while [ \"$INPUT_STRING\" != \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done #!/bin/sh while : do echo \"Please type something in (^C to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done The colon (:) always evaluates to true; whilst using this can be necessary sometimes, it is often preferable to use a real exit condition. Compare quitting the above loop with the one below; see which is the more elegant. Also think of some situations in which each one would be more useful than the other: #!/bin/sh while read input_text do case $input_text in hello) echo English ;; howdy) echo American ;; gday) echo Australian ;; bonjour) echo French ;; \"guten tag\") echo German ;; *) echo Unknown Language: $input_text ;; esac done \u003c myfile.txt This reads the file ‚Äúmyfile.txt‚Äù, one line at a time, into the variable ‚Äú$input_text‚Äù. The case statement then checks the value of $input_text. If the word that was read from myfile.txt was ‚Äúhello‚Äù then it echoes the word ‚ÄúEnglish‚Äù. If it was ‚Äúgday‚Äù then it will echo Australian. If the word (or words) read from a line of myfile.txt don‚Äôt match any of the provided patterns, then the catch-all ‚Äú*‚Äù default will display the message ‚ÄúUnknown Language: $input_text‚Äù - where of course ‚Äú$input_text‚Äù is the value of the line that it read in from myfile.txt. A handy Bash (but not Bourne Shell) tip I learned recently from the Linux From Scratch project is: mkdir rc{0,1,2,3,4,5,6,S}.d instead of the more cumbersome: for runlevel in 0 1 2 3 4 5 6 S do mkdir rc${runlevel}.d done And ls can be done recursively, too: $ cd / $ ls -ld {,usr,usr/local}/{bin,sbin,lib} drwxr-xr-x 2 root root 4096 Oct 26 01:00 /bin drwxr-xr-x 6 root root 4096 Jan 16 17:09 /lib drwxr-xr-x 2 root root 4096 Oct 27 00:02 /sbin drwxr-xr-x 2 root root 40960 Jan 16 19:35 usr/bin drwxr-xr-x 83 root root 49152 Jan 16 17:23 usr/lib drwxr-xr-x 2 root root 4096 Jan 16 22:22 usr/local/bin drwxr-xr-x 3 root root 4096 Jan 16 19:17 usr/local/lib drwxr-xr-x 2 root root 4096 Dec 28 00:44 usr/local/sbin drwxr-xr-x 2 root root 8192 Dec 27 02:10 usr/sbin ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:2","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Test Test is used by virtually every shell script written. It may not seem that way, because test is not often called directly. test is more frequently called as [. [ is a symbolic link to test, just to make shell programs more readable. It is also normally a shell builtin (which means that the shell itself will interpret [ as meaning test, even if your Unix environment is set up differently): $ type [ [ is a shell builtin $ which [ /usr/bin/[ $ ls -l /usr/bin/[ lrwxrwxrwx 1 root root 4 Mar 27 2000 /usr/bin/[ -\u003e test $ ls -l /usr/bin/test -rwxr-xr-x 1 root root 35368 Mar 27 2000 /usr/bin/test This means that ‚Äò[‚Äô is actually a program, just like ls and other programs, so it must be surrounded by spaces: if [$foo = \"bar\" ] will not work; it is interpreted as if test$foo = \"bar\" ], which is a ‚Äò]‚Äô without a beginning ‚Äò[‚Äô. Put spaces around all your operators. I‚Äôve highlighted the mandatory spaces with the word ‚ÄòSPACE‚Äô . Note: Some shells also accept ‚Äú==‚Äù for string comparison; this is not portable, a single ‚Äú=‚Äù should be used for strings, or ‚Äú-eq‚Äù for integers. Test is a simple but powerful comparison utility. For full details, run man test on your system, but here are some usages and typical examples. Test is most often invoked indirectly via the if and while statements. It is also the reason you will come into difficulties if you create a program called test and try to run it, as this shell builtin will be called instead of your program! The syntax for if...then...else... is: if [ ... ] then # if-code else # else-code fi Also, be aware of the syntax - the ‚Äúif [ ... ]‚Äù and the ‚Äúthen‚Äù commands must be on different lines. Alternatively, the semicolon ‚Äú;‚Äù can separate them: if [ ... ]; then # do something fi You can also use the elif, like this: if [ something ]; then echo \"Something\" elif [ something_else ]; then echo \"Something else\" else echo \"None of the above\" fi This will echo \"Something\" if the [ something ] test succeeds, otherwise it will test [ something_else ], and echo \"Something else\" if that succeeds. If all else fails, it will echo \"None of the above\". ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:4:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Case The case statsement saves going through a whole set of if ... then ... else statements. Its syntax is really simple: #!/bin/sh echo \"Please talk to me ...\" while : do read INPUT_STRING case $INPUT_STRING in hello) echo \"Hello yourself!\" ;; bye) echo \"See you again!\" break ;; *) echo \"Sorry I don't understand\" ;; esac done ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:5:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables 2 The first set of variables we will look at are $0 ... $9 and $#. The variable $0 is the basename of the program as it was called. $1...$9 are the first 9 additional parameters the script was called with. The variable $@ is all parameters. The variable $* is similar, but does not preserve any whitespace and quoting, so ‚ÄúFile with spaces‚Äù becomes ‚ÄúFile‚Äù, ‚Äúwith‚Äù, and ‚Äúspaces‚Äù. $# is the number of parameters the script was called with. #!/bin/sh echo \"I was called with $# parameters\" echo \"My name is $0\" echo \"My first parameter is $1\" The othere two main variables set are $$ and $!. These are both process numbers. The $$ variable is the PID of the currently running shell. This can be useful for creating temporary files, such as /tmp/my-script.$$ which is useful if many instances of the script could be run at the same time, and they all need their own temporary files. The $! variable is the PID of the last run background processd. This is useful to keep track of the process as it gets on with its job. Another interesting vardfiable is IFS. This is the Interfal Field Separator. The default value is SPACE TAB NEWLINE, but if you are changing it, it‚Äôs easier to take a copy as shown: #!/bin/sh old_IFS=\"$IFS\" IFS=: # Set IFS as colon echo \"Please input some data separated by colons\" read x y z IFS=$old_IFS echo \"x is $x y is $y z is $z\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:6:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Functions #!/bin/sh add_a_user() { USER=$1 PASSWORD=$2 COMMENTS=$@ echo \"Adding user $USER\" echo useradd -c \"$COMMENTS\" $USER echo passwd $USER $PASSWORD } ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:7:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Reference shellscript ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:8:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Linux"],"content":"You should use Linux!","date":"2024-04-21","objectID":"/20240421_why-linux/","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Linux"],"content":"Linux, A Path to Digital Simplicity In an age dominated by digital clutter and overwhelming software choices, the minimalist philosophy stands out as a beacon for those seeking simplicity and efficiency. This approach not only applies to physical possessions but extends into the digital realm, where Linux has become a preferred tool for minimalists. Linux, an open-source operating system, embodies the principles of minimalism by offering users control over their digital environments. Unlike mainstream operating systems that often come loaded with non-essential features and bloatware, Linux allows users to select only the components they need, creating a lean and efficient system. The minimalist appeal of Linux is evident in its customizable nature. Users can choose from a variety of distributions, each tailored for different needs. For instance, distributions like Arch Linux provide barebone setup that users can expand as needed. This customization extends to the user interface, where options range from feature-rich desktop environments to more austere ones like i3wm or DWM, which use fewer resources and maintain a clean, unobtrusive design. Furthermore, Linux‚Äôs robust command-line interface is a minimalist‚Äôs dream. It enables users to perform tasks efficiently without the graphical overhead, which is particularly appealing to those who favor functionality and speed over visual elements. Moreover, Linux supports the concept of free software, which resonates with minimalists‚Äô preference for authenticity and freedom from commercial constraints. Users are free to modify, improve, and redistribute their software in ways that proprietary systems do not allow. Linux offers a compelling choice for anyone looking to embrace a minimalist digital lifestyle. It provides the tools to create a personalized and simple digital environment, encourages the efficient use of resources, and upholds values of freedom and simplicity. For those seeking to reduce their digital footprint while maximizing functionality, Linux proves that less can indeed be more. ","date":"2024-04-21","objectID":"/20240421_why-linux/:0:1","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Python"],"content":"Guide to keep sensitive data in Python","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"An app‚Äôs config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes: Resource handles to the database, Memcached, and other backing services Credentials to external services such as Amazon S3 or Twitter Per-deploy values such as the canonical hostname for the deploy Apps sometimes store config as constants in the code. This is a violation of twelve-factor, which requires strict separation of config from code. Config varies substantially across deploys, code does not. A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. In a twelve-factor app, env vars are granular controls, each fully orthogonal to other env vars. They are never grouped together as ‚Äúenvironments‚Äù, but instead are independently managed for each deploy. This is a model that scales up smoothly as the app naturally expands into more deploys over its lifetime. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:0:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Environment Variable For example, you shouldn‚Äôt put information directly in your code. db_user = 'my_db_user' db_password = 'my_db_pass_123!' Let‚Äôs keep the sensitive information in .bash_profile as follows: export DB_USER=\"my_db_user\" export DB_PASS='my_db_pass_123!' Then, we can call them by import os db_user = os.environ.get('DB_USER') db_password = os.environ.get('DB_PASS') ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:1:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"dotenv ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Introduction Python-dotenv reads key-value pairs from a .env file and can set them as environment variables. It helps in the development of applications following the 12-factor principles. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:1","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Basics Installation: pip install python-dotenv Create .env file in your project directory. Put the data (or variables) in the .env file. e.g, API_KEY=\"dafjei304aldkjf20akj\" To load your key, First, use load_dotenv() with os.getenv(\"[Your variable]\") e.g., API_KEY=os.getenv(\"API_KEY\") Make sure to update .gitignore to exclude the .env file. from dotenv import load_dotenv load_dotenv() API_KEY = os.getenv(\"API_KEY\") or \"\" ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:2","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Reference The Twelve Factor App ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:3:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Guide to understand type hint in Python.","date":"2024-04-20","objectID":"/20240421_type-hint/","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Type hinting is not mandatory, but it can make your code easier to understand and debug by Improved readability Better IDE support: IDEs and linters can use type hints to check your code for potential errors before runtime. While type hints can be simple classes like float or str , they can also be more complex. The typing module provides a vocabulary of more advanced type hints. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Basics # This is how you declare the type of a variable age: int = 1 # You don't need to initialize a variable to annotate it a: int # Ok (no value at runtime until assigned) # Doing so can be useful in conditional branches child: bool if age \u003c 18: child = True else: child = False x: int = 1 x: float = 1.0 x: bool = True x: str = \"test\" x: bytes = b\"test\" # For collections on Python 3.9+, the type of the collection item is in brackets x: list[int] = [1] x: set[int] = {6, 7} # For mappings, we need the types of both keys and values x: dict[str, float] = {\"field\": 2.0} # Python 3.9+ # For tuples of fixed size, we specify the types of all the elements x: tuple[int, str, float] = (3, \"yes\", 7.5) # Python 3.9+ # For tuples of variable size, we use one type and ellipsis x: tuple[int, ...] = (1, 2, 3) # Python 3.9+ # On Python 3.8 and earlier, the name of the collection type is # capitalized, and the type is imported from the 'typing' module from typing import List, Set, Dict, Tuple x: List[int] = [1] x: Set[int] = {6, 7} x: Dict[str, float] = {\"field\": 2.0} x: Tuple[int, str, float] = (3, \"yes\", 7.5) x: Tuple[int, ...] = (1, 2, 3) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:1","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Union Union is for multiple types def process_message(msg: Union[str, bytes, None]) -\u003e str: ... # On Python 3.10+, use the | operator when something could be one of a few types x: list[int | str] = [3, 5, \"test\", \"fun\"] # Python 3.10+ # On earlier versions, use Union x: list[Union[int, str]] = [3, 5, \"test\", \"fun\"] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:2","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Optional # food can be either str or None. def eat_food(food: Optional[str]) -\u003e None: ... # Use Optional[X] for a value that could be None # Optional[X] is the same as X | None or Union[X, None] x: Optional[str] = \"something\" if some_condition() else None if x is not None: # Mypy understands x won't be None here because of the if-statement print(x.upper()) # If you know a value can never be None due to some logic that mypy doesn't # understand, use an assert assert x is not None print(x.upper()) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:3","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Any Any is a special type hint in Python that indicates that a variable can be of any type. It essentially disables static type checking for that variable. It‚Äôs typically used when you want to explicitly indicate that a certain variable can have any type, or when dealing with dynamically typed code where the type of the variable cannot be easily inferred. While Any provides flexibility, it also sacrifices the benefits of static type checking, as type errors related to variables annotated as Any won‚Äôt be caught by type checkers. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:4","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Functions: Callable Types Callable type hint can define types for callable functions. from typing import Callable Callable[[Parameter types, ...], return_types] Callable objects are functions, classes, and so on. Type [input types] and return types def on_some_event_happened(callback: Callable[[int, str, str], int]) -\u003e None: ... def do_this(a: int, b: str, c:str) -\u003e int: ... on_some_event_happened(do_this) # This is how you annotate a callable (function) value x: Callable[[int, float], float] = f def register(callback: Callable[[str], int]) -\u003e None: ... # A generator function that yields ints is secretly just a function that # returns an iterator of ints, so that's how we annotate it def gen(n: int) -\u003e Iterator[int]: i = 0 while i \u003c n: yield i i += 1 # You can of course split a function annotation over multiple lines def send_email(address: Union[str, list[str]], sender: str, cc: Optional[list[str]], bcc: Optional[list[str]], subject: str = '', body: Optional[list[str]] = None ) -\u003e bool: ... ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:5","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Classes class BankAccount: # The \"__init__\" method doesn't return anything, so it gets return # type \"None\" just like any other method that doesn't return anything def __init__(self, account_name: str, initial_balance: int = 0) -\u003e None: # mypy will infer the correct types for these instance variables # based on the types of the parameters. self.account_name = account_name self.balance = initial_balance # For instance methods, omit type for \"self\" def deposit(self, amount: int) -\u003e None: self.balance += amount def withdraw(self, amount: int) -\u003e None: self.balance -= amount # User-defined classes are valid as types in annotations account: BankAccount = BankAccount(\"Alice\", 400) def transfer(src: BankAccount, dst: BankAccount, amount: int) -\u003e None: src.withdraw(amount) dst.deposit(amount) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:6","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Annotated Annotated in python allows developers to declare type of a reference and and also to provide additional information related to it. name = Annotated[str, \"first letter is capital\"] This tells that name is of type str and that name[0] is a capital letter. On its own Annotated does not do anything other than assigning extra information (metadata) to a reference. It is up to another code, which can be a library, framework or your own code, to interpret the metadata and make use of it. For example FastAPI uses Annotated for data validation: def read_items(q: Annotated[str, Query(max_length=50)]) Here the parameter q is of type str with a maximum length of 50. This information was communicated to FastAPI (or any other underlying library) using the Annotated keyword. Annotated[\u003ctype\u003e, \u003cmetadata\u003e] Here is an example of how you might use Annotated to add metadata to type annotations if you were doing range analysis: @dataclass class ValueRange: lo: int hi: int T1 = Annotated[int, ValueRange(-10, 5)] T2 = Annotated[T1, ValueRange(-20, 3)] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:7","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"TypeVar This is a special type for generic types. from typing import Sequence, TypeVar, Iterable T = TypeVar(\"T\") # `T` is typically used to represent a generic type variable def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Since the generic type is used, batch_iter function can take any type of Sequence type data. For instance, Sequence[int], Sequence[str], Sequence[Person] If we use bound, then we can restrict the generic type. For example, from typing import Sequence, TypeVar, Iterable, Union T = TypeVar(\"T\", bound=Union[int, str, bytes]) def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Thus, the following code will show an error as it takes a list of float numbers: batch_iter([1.1, 1.3, 2.5, 4.2, 5.5], 2) Note that in Python 3.12, generic type hint has been changed ","date":"2024-04-20","objectID":"/20240421_type-hint/:1:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Reference ArjanCodes Type hint cheat sheet ","date":"2024-04-20","objectID":"/20240421_type-hint/:2:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":null,"content":"About Han","date":"2024-04-21","objectID":"/about/","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Han Cheol Moon Welcome to Han's XYZ, where you'll explore a world of ideas across every dimension of thought. I am a curiosity-driven Machine Learning Scientist, passionate about exploring the depths of data and algorithms. As an enthusiastic learner and code lover, I am always eager to expand my knowledge and embrace innovation, with a commitment to sharing my insights and discoveries with you. ","date":"2024-04-21","objectID":"/about/:0:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Research Interests Natural language processing (NLP) Robustness of NLP systems ","date":"2024-04-21","objectID":"/about/:1:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Professional Employments Samsung Electronics Staff ML/DL Engineer, Sept 2023 - Present ","date":"2024-04-21","objectID":"/about/:2:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Education Nanyang Technological University, 2019-2023 Ph.D. in Computer Science Yonsei University, 2016-2018 M.S. in Electrical \u0026 Electronic Engineering Chung-Ang University, 2009-2016 B.S. in Electrical \u0026 Electronic Engineering (Military Service, 2009-2012) ","date":"2024-04-21","objectID":"/about/:3:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Skills Programming Languages Python, C/C++, MATLAB, and Bash/Shell ML/DL Research PyTorch, Tensorflow, Scipy, Numpy, Scikit-learn, Pandas, and HuggingFace ML/DL Ops Git, Docker, Kubernetes, Jenkins, DVC, FastAPI, and LangChain Database SQLite and PostgreSQL with SQLAlchemy and Alembic WebDev HTML/CSS, Django, and Hugo WorkFlow I‚Äôve used Microsoft and GNU/Linux systems (both Debian and Arch-based varieties) with a Vim-based setup for writing scripts and managing files in a tiling window manager (i3). I use Git for version control with Docker and schedule multiple tasks with TaskSpooler. I compile documents using $\\LaTeX$. Soft Skills Time Management, Teamwork, Problem-solving, Documentation, Engaging Presentation. ","date":"2024-04-21","objectID":"/about/:4:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching ","date":"2024-04-21","objectID":"/about/:5:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching Assistant DeepNLP, 2019-2020 Nanyang Technological University, Singapore Digital Logic Circuits, Spring 2018 Yonsei University, Korea Multimedia Signal Processing, Fall 2017 Yonsei University, Korea Signals and Systems, Fall 2016 Yonsei University, Korea ","date":"2024-04-21","objectID":"/about/:5:1","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Honors and Award Singapore International Graduate Award (SINGA), 2019-2023 Top of Class Scholarship, Chung-Ang University, 2015 Dean‚Äôs List, Chung-Ang University, 2013-2014 ","date":"2024-04-21","objectID":"/about/:6:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Service and leadership Military Service, 2009-2012 Republic of Korea Army, Capital Defense Command ‚ÄòSHIELD‚Äô, 1st Security Group Guarding presidential residence Sergeant ","date":"2024-04-21","objectID":"/about/:7:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":" --\u003e ","date":"0001-01-01","objectID":"/drafts/how_to_hugo/:0:0","tags":null,"title":"","uri":"/drafts/how_to_hugo/"},{"categories":null,"content":"Writing clean, efficient, and well-structured functions in Python is a skill that takes time to master. I‚Äôve learned so much about this craft, thanks to ArjanCodes , a YouTube channel that has been like a Python guru to me. ","date":"0001-01-01","objectID":"/drafts/how_to_write_functions/:0:0","tags":null,"title":"","uri":"/drafts/how_to_write_functions/"},{"categories":null,"content":"Python Naming Conventions: A Guide to Writing Clean and Readable Code When writing Python code, adhering to consistent naming conventions is crucial for readability and maintainability. Python‚Äôs official style guide, PEP 8 , provides clear guidelines on how to name variables, functions, classes, and other elements in your code. In this blog post, we‚Äôll explore these conventions and provide examples to help you write clean, professional Python code. Note: PEP 8 is a document that provides various guidelines to write the readable in Python. PEP 8 describes how the developer can write beautiful code. ","date":"0001-01-01","objectID":"/drafts/pep8/:0:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"Why Naming Conventions Matter Naming conventions are more than just a formality‚Äîthey make your code easier to read, understand, and collaborate on. Consistent naming helps you and others quickly identify the purpose of a variable, function, or class. It also reduces the likelihood of errors and makes your codebase more maintainable. ","date":"0001-01-01","objectID":"/drafts/pep8/:1:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"PEP 8 Naming Conventions ","date":"0001-01-01","objectID":"/drafts/pep8/:2:0","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"1. Variables and Functions Use snake_case for variable and function names. Names should be lowercase, with words separated by underscores. Choose descriptive and concise names that reflect the purpose of the variable or function. # Good user_name = \"JohnDoe\" def calculate_total_price(items): pass # Bad UserName = \"JohnDoe\" # Pascal case is not recommended for variables def CalculateTotalPrice(items): # Pascal case is not recommended for functions pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:1","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"2. Constants Use UPPER_SNAKE_CASE for constants. Constants are typically defined at the module level and are intended to remain unchanged. # Good MAX_CONNECTIONS = 100 DEFAULT_TIMEOUT = 30 # Bad maxConnections = 100 # Not in uppercase default_timeout = 30 # Not in uppercase ","date":"0001-01-01","objectID":"/drafts/pep8/:2:2","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"3. Classes Use PascalCase (also known as CamelCase) for class names. Class names should be nouns and should clearly describe the object they represent. # Good class UserProfile: pass class DatabaseConnection: pass # Bad class user_profile: # Snake case is not recommended for classes pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:3","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"4. Methods Methods follow the same naming convention as functions: snake_case. Method names should be verbs or verb phrases that describe the action they perform. # Good class User: def get_name(self): pass def update_profile(self, new_data): pass # Bad class User: def GetName(self): # Pascal case is not recommended for methods pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:4","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"5. Modules and Packages Use lowercase names for modules and packages. Keep names short and descriptive. Underscores are acceptable, especially if they improve readability. Avoid underscores in module names if possible, but they are acceptable for readability. # Good import utilities from data_processing import analyzer # Bad import Utilities # Uppercase is not recommended from DataProcessing import Analyzer # Pascal case is not recommended ","date":"0001-01-01","objectID":"/drafts/pep8/:2:5","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"6. Private and Protected Members Use a single leading underscore (_) for non-public methods and variables. Use a double leading underscore (__) for name mangling (to make an attribute private to its class). class MyClass: def __init__(self): self._protected_variable = 42 # Protected self.__private_variable = 100 # Private def _protected_method(self): pass def __private_method(self): pass ","date":"0001-01-01","objectID":"/drafts/pep8/:2:6","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"7. Avoid Single-Letter Names Avoid using single-letter variable names except for trivial loop counters or mathematical variables. # Good for index in range(10): print(index) # Bad for i in range(10): # Not descriptive print(i) ","date":"0001-01-01","objectID":"/drafts/pep8/:2:7","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"8. Avoid Reserved Keywords Do not use Python reserved keywords (e.g., class, def, import) as variable or function names. # Bad class = \"MyClass\" # This will raise a SyntaxError ","date":"0001-01-01","objectID":"/drafts/pep8/:2:8","tags":null,"title":"","uri":"/drafts/pep8/"},{"categories":null,"content":"Best Practices for Naming Be Descriptive: Choose names that clearly describe the purpose of the variable, function, or class. Keep It Short but Meaningful: Avoid overly long names, but ensure they are descriptive enough. Be Consistent: Stick to the same naming conventions throughout your codebase. Avoid Ambiguity: Use names that are unambiguous and easy to understand. Happy coding! ","date":"0001-01-01","objectID":"/drafts/pep8/:3:0","tags":null,"title":"","uri":"/drafts/pep8/"}]