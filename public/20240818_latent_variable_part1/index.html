<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Introduction to Latent Variable Modeling (Part 1) - Han&#39;s XYZ</title><meta name="Description" content="Latent variable tutorial"><meta property="og:url" content="http://localhost:1313/20240818_latent_variable_part1/">
  <meta property="og:site_name" content="Han&#39;s XYZ">
  <meta property="og:title" content="Introduction to Latent Variable Modeling (Part 1)">
  <meta property="og:description" content="Latent variable tutorial">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-18T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-20T16:00:56+09:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Latent Variable">
    <meta property="article:tag" content="K-Means Clustering">
    <meta property="article:tag" content="Clustering">
    <meta property="article:tag" content="Gmm">
    <meta property="article:tag" content="Gaussian Mixture Modeling">
    <meta property="og:image" content="http://localhost:1313/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/logo.png">
  <meta name="twitter:title" content="Introduction to Latent Variable Modeling (Part 1)">
  <meta name="twitter:description" content="Latent variable tutorial">
<meta name="application-name" content="KeepIt">
<meta name="apple-mobile-web-app-title" content="KeepIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/20240818_latent_variable_part1/" /><link rel="prev" href="http://localhost:1313/20240815_svd/" /><link rel="next" href="http://localhost:1313/20240819_gradient_descent/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Introduction to Latent Variable Modeling (Part 1)",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/20240818_latent_variable_part1\/"
        },"genre": "posts","keywords": "machine learning, latent variable, k-means clustering, clustering, gmm, Gaussian mixture modeling","wordcount":  1821 ,
        "url": "http:\/\/localhost:1313\/20240818_latent_variable_part1\/","datePublished": "2024-08-18T00:00:00+00:00","dateModified": "2025-04-20T16:00:56+09:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Han"
            },"description": "Latent variable tutorial"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Han&#39;s XYZ"><span class="header-title-pre"><i class='fa fa-home ' aria-hidden='true'></i></span>Han&#39;s XYZ</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://github.com/Han8931" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Introduction to Latent Variable Modeling (Part 1)</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Han</a>
</span>&nbsp;<span class="post-category">included in <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Machine Learning</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-18">2024-08-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;1821 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;9 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#motivation-of-latent-variable-modeling">Motivation of Latent Variable Modeling</a></li>
    <li><a href="#k-means-clustering">K-Means Clustering</a></li>
    <li><a href="#gaussian-mixture-models">Gaussian Mixture Models</a>
      <ul>
        <li><a href="#maximum-likelihood">Maximum Likelihood</a>
          <ul>
            <li><a href="#singularity">Singularity</a></li>
            <li><a href="#identifiability">Identifiability</a></li>
          </ul>
        </li>
        <li><a href="#expectation-maximization-for-gmm">Expectation Maximization for GMM</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="latent-variable-modeling">Latent Variable Modeling</h1>
<h2 id="motivation-of-latent-variable-modeling">Motivation of Latent Variable Modeling</h2>
<p>Let&rsquo;s say we want to classify some data. If we had access to a corresponding latent variable for each observation $ \mathbf{x}_i $, modeling would be more straightforward. To illustrate this, consider the challenge of finding the latent variable (i.e., the true class of $ \mathbf{x} $). It can be expressed like $ z^* = \argmax_{z} p(\mathbf{x} | z) $. It is hard to identify the true clusters without prior knowledge about them. For example, we can cluster like Fig. (b) or (c).</p>
<p style="text-align:center;"> 
<img src="https://raw.githubusercontent.com/Han8931/han8931.github.io/main/assets/images/latent_variable.png" alt="Latent Variable" height="130">
</p>
<p>Consider modeling the complete data set $ p(\mathbf{x} | z) $ under the assumption that the observations are independent and identically distributed (i.i.d.). Based on the above Fig. (c), the joint distribution for a single observation $ (\mathbf{x}_i, \mathbf{z}_i) $ given the model parameters $ \boldsymbol{\theta} $ can be expressed:</p>
<p>\begin{align*}
p(\mathbf{x}_i, \mathbf{z}_i | \boldsymbol{\theta}) =
\begin{cases}
p(\mathcal{C}_1) p(\mathbf{x}_i | \mathcal{C}_1) &amp; \text{if } z_i = 0 \\
p(\mathcal{C}_2) p(\mathbf{x}_i | \mathcal{C}_2) &amp; \text{if } z_i = 1 \\
p(\mathcal{C}_3) p(\mathbf{x}_i | \mathcal{C}_3) &amp; \text{if } z_i = 2 \\
\end{cases}
\end{align*}
Given $ N $ observations, the joint distribution for the entire dataset $ { \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N } $ along with their corresponding latent variables $ { \mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N } $ is:</p>
<p>\begin{align*}
p(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N, \mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_N | \boldsymbol{\theta}) = \prod_{n=1}^{N} \prod_{k=1}^{K} \pi_k^{z_{nk}} \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{z_{nk}}
\end{align*}</p>
<p>Here, $ \pi_k = p(\mathcal{C}_k) $ represents the prior probability of the $ k $-th component, and $ p(\mathbf{x}_n | \mathcal{C}_k) = \mathcal{N}(\mathbf{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) $ denotes the Gaussian distribution associated with component $ \mathcal{C}_k $. Also, $z_{nk}\in{0,1}$ and $\sum_k z_{nk}=1$.</p>
<p>However, in practice, the latent variables $ \mathbf{z}_k $ are often not directly observable, which complicates the modeling process.</p>
<p>In the following sections, we present various methods for identifying and handling these latent variables to improve the classification and modeling of data.</p>
<h2 id="k-means-clustering">K-Means Clustering</h2>
<p>Suppose that we have a data set $\mathbf{X} = {\mathbf{x}_1,\dots, \mathbf{x}_n}$ consisting of $N$ observations of a random $D$-dimensional variable $\mathbf{x}\in \mathbb{R}^{D}$. Our goal is to partition the data into $K$ of clusters.  Intuitively, a cluster can be thought as <strong>a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster</strong>.</p>
<p>This notion can be formalized by introducing a set of $D$-dimensional vectors $\boldsymbol{\mu}_k$, which represents the centers of the clusters. Our goal is to find an assignment of data points to clusters, as well as a set of vectors ${\boldsymbol{\mu}_k}$. Objective function of $K$-means clustering (<em>distortion measure</em>) can be defined as follows:
$$J =  \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\lVert\boldsymbol{x}_n-\boldsymbol{\mu}_k\rVert^2$$
, where $r_{nk}\in{0,1}$ is a binary indicator variable which represents the <strong>membership of data</strong> $\mathbf{x}_n$. It can be expressed as follows:
% The $r_{nk}$ can be optimized in a closed-form solution as follows:
\begin{align*}
r_{nk}=\begin{cases}
1 &amp; \text{if } k=\argmin_{j} \lVert\boldsymbol{x}_n-\boldsymbol{\mu}_j\rVert^2\\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}
Our goal is to find values for the $\boldsymbol{\mu}_k$ and the $r_{nk}$ that minimize $J$.</p>
<p>We can minimize $J$ through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the $\boldsymbol{\mu}_k$ and the $r_{nk}$. First we choose some initial values for the $\boldsymbol{\mu}_k$. Then, in the first phase, we minimize $J$ with respect to the $r_{nk}$, keeping the $\boldsymbol{\mu}_k$ fixed. In the second phase we minimize $J$ with respect to the $\boldsymbol{\mu}_k$, keeping $r_{nk}$ fixed. This two-stage optimization is then repeated until convergence.</p>
<p>Now consider the optimization of the $\boldsymbol{\mu}_k$ with the $r_{nk}$ held fixed. The objective function $J$ is a quadratic function of $\boldsymbol{\mu}_k$, and it can be minimized by setting its derivative with respect to $\boldsymbol{\mu}_k$ to zero giving
\begin{align*}
2\sum_{n=1}^{N}r_{nk}(\boldsymbol{x}_n-\boldsymbol{\mu}_k) = 0.
\end{align*}
We can arrange as
\begin{align*}
\boldsymbol{\mu}_k = \frac{\sum_n r_{nk}\boldsymbol{x}_n}{\sum_n r_{nk}}.
\end{align*}</p>
<p>The denominator of $\boldsymbol{\mu}_k$ is equal to the number of points assigned to cluster $k$. The mean of cluster $k$ is essentially the same as the mean of data points $\mathbf{x}_n$ assigned to cluster $k$. For this reason, the procedure is known as the $K$-<em>means clustering algorithm</em>.</p>
<p>The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments. These two phases reduce the value of the objective function $J$, so the convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of $J$.</p>
<p>We can also sequentially update the $\mu_k$ as follows:
\begin{align*}
\mu_{k+1} = \mu_{k} + \eta(\mathbf{x}_k-\mu_{k})
\end{align*}</p>
<p>There are some properties to note:</p>
<ul>
<li>It is a hard clustering algorithm ($\leftrightarrow$ soft clustering)</li>
<li>It is sensitive to the initialization of centroid.</li>
<li>The number of clusters is uncertain.</li>
<li>Sensitive to distance metrics (e.g., Euclidean?)</li>
</ul>
<h2 id="gaussian-mixture-models">Gaussian Mixture Models</h2>
<p>$K$-means clustering is a form of hard clustering, where each data point is assigned to exactly one cluster. However, in some cases, soft clustering—where data points can belong to multiple clusters with varying degrees of membership—provides a better model in practice. A Gaussian Mixture Model (GMM) assumes a linear superposition of Gaussian components, offering a richer class of density models than a single Gaussian distribution.</p>
<p>In essence, rather than assuming that all data points are generated by a single Gaussian distribution, we assume that the data is generated by a mixture of $ K $ different Gaussian distributions, where each Gaussian represents a different component in the mixture.</p>
<p>For a single sample, the Gaussian Mixture Model can be expressed as a weighted sum of these individual Gaussian distributions:
\begin{align*}
p(\mathbf{x}) &amp;= \sum_\mathbf{z} p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) \\
&amp;= \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}
Here, $ \mathbf{x} $ is a data point, $ \pi_k $ represents the mixing coefficients, $ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) $ is a Gaussian distribution with mean $ \boldsymbol{\mu}_k $ and covariance $ \boldsymbol{\Sigma}_k $, and $ K $ is the number of Gaussian components.</p>
<p>A key quantity in GMMs is the conditional probability of $ \mathbf{z} $ given $ \mathbf{x} $, denoted as $ p(z_k = 1 | \mathbf{x}) $ or $ \gamma(z_k) $. This is also known as the responsibility or assignment probability, which represents the probability that a given data point $ \mathbf{x} $ belongs to component $ k $ of the mixture. Essentially, this can be thought of as the <strong>classification result</strong> for $ \mathbf{x} $.</p>
<p>This responsibility is updated using Bayes&rsquo; Theorem, and can be expressed as:
\begin{align*}
\gamma(z_k) \equiv p(z_k=1|\mathbf{x}) &amp; \equiv \frac{p(z_k=1)p(\mathbf{x}|z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(\mathbf{x}|z_j=1)} \\
&amp; = \frac{\pi_k\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
\end{align*}</p>
<p>In this expression, $ \pi_k $ is the prior probability (or mixing coefficient) for component $ k $, and $ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) $ is the likelihood of the data point $ \mathbf{x} $ under the Gaussian distribution corresponding to component $ k $. The denominator is a normalization factor that ensures the responsibilities sum to 1 across all components for a given data point.</p>
<p>This framework allows for a soft classification of data points, where each point is associated with a probability of belonging to each cluster, rather than being strictly assigned to a single cluster as in $K$-means.</p>
<h3 id="maximum-likelihood">Maximum Likelihood</h3>
<p>Suppose we have a data set of observations $\mathbf{X}=\{ \mathbf{x}_1,\dots,\mathbf{x}_n \}^{T}\in\mathbb{R}^{N\times D}$ and we want to model the data distribution $p(\mathbf{X})$ using GMM. Assuming the data is independent and identically distributed (i.i.d.), the likelihood of the entire dataset can be expressed as:
\begin{align*}
p(\mathbf{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma}) = \prod_{n=1}^{N}\Bigg(\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\Bigg).
\end{align*}
To simplify the optimization process, we consider the log-likelihood function, which is given by:
\begin{align*}
\ln p(\mathbf{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma}) &amp;= \sum_{n=1}^{N}\ln \Bigg(\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\Bigg)
\end{align*}
To solve the Maximum Likelihood Estimation (MLE) for Gaussian Mixture Models (GMMs), we typically consider the iterative <em>Expectation-Maximization</em> (EM) algorithm due to the non-convex nature of the problem. Before discussing how to maximize the likelihood, it is important to emphasize two significant issues that arise in GMMs: <em>singularities</em> and <em>identifiability</em>.</p>
<h4 id="singularity">Singularity</h4>
<p>A major challenge in applying the maximum likelihood framework to Gaussian Mixture Models is the presence of singularities. This problem arises because the likelihood function can become unbounded under certain conditions, leading to an ill-posed optimization problem.</p>
<p>For simplicity, consider a Gaussian mixture model where each component has a covariance matrix of the form $ \Sigma_k = \sigma^2_k I $, where $ I $ is the identity matrix. Suppose one of the mixture components, say the $ j $-th component, has its mean $ \boldsymbol{\mu}_j $ exactly equal to one of the data points $ \mathbf{x}_n $, so that $ \boldsymbol{\mu}_j = \mathbf{x}_n $ for some value of $ n $. The contribution of this data point to the likelihood function would then be:
\begin{align*}
\mathcal{N}(\mathbf{x}_n | \mathbf{x}_n, \sigma^2_j I) = \frac{1}{\sqrt{2\pi} \sigma_j} \cdot \exp^0
\end{align*}
As $ \sigma_j $ approaches 0, this term goes to infinity, causing the log-likelihood function to also diverge to infinity. Therefore, maximizing the log-likelihood function becomes an ill-posed problem because such singularities can always be present. These singularities occur whenever one of the Gaussian components <strong>collapses</strong> onto a specific data point, leading to a covariance matrix with a determinant approaching zero. This issue did not arise with a single Gaussian distribution because the variance cannot be zero by definition.</p>
<h4 id="identifiability">Identifiability</h4>
<p>Another issue in finding MLE solutions for GMMs is related to identifiability. For any given maximum likelihood solution, a GMM with $ K $ components has a total of $ K! $ equivalent solutions. This arises from the fact that the $ K! $ different ways of permuting the $ K $ sets of parameters (means, covariances, and mixing coefficients) yield the same likelihood.</p>
<p>In other words, for any point in the parameter space that represents a maximum likelihood solution, there are $ K! - 1 $ additional points that produce exactly the same probability distribution. This lack of identifiability means that the solution is not unique, complicating both the interpretation of the model and the optimization process.</p>
<h3 id="expectation-maximization-for-gmm">Expectation Maximization for GMM</h3>
<p>The goal of the Expectation-Maximization (EM) algorithm is to find maximum likelihood solutions for models that involve latent variables.</p>
<ul>
<li>Suppose that directly optimizing the likelihood $p(\mathbf{X}|\boldsymbol{\theta})$ is difficult.</li>
<li>However, it is easier to optimize the complete-data likelihood function $p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\theta})$ as as discussed in the previous sections.</li>
<li>In such cases, we can use the <strong>EM algorithm</strong>. EM algorithm is a general technique for finding maximum likelihood solutions in latent variable models.</li>
</ul>
<p>Let&rsquo;s begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. By setting the derivatives of $\ln p(\mathbf{X}|\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Sigma})$ with respect to the means $\boldsymbol{\mu}_k$ of the Gaussian components to zero, we obtain
\begin{align*}
0 = -\sum_{n=1}^N\frac{\pi_k\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}\boldsymbol{\Sigma}_k(\mathbf{x}_n-\boldsymbol{\mu}_k)
\end{align*}
Multiplying by $\boldsymbol{\Sigma}_k^{-1}$ (which we assume to be non-singular) and rearranging we obtain
\begin{align*}
\boldsymbol{\mu}_k = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\mathbf{x}_n,
\end{align*}
where we have defined
\begin{align*}
N_k = \sum_{n=1}^{N}\gamma(z_{nk}).
\end{align*}
We can interpret $N_k$ as the effective number of points assigned to cluster $k$. We can obtain the MLE solutions for other variables similarly.</p>
<p style="text-align:center;"> 
<img src="https://raw.githubusercontent.com/Han8931/han8931.github.io/main/assets/images/gmm_em.png" alt="EM Algorithm for GMM" height="480">
</p> 
<p><strong>References</strong>:</p>
<ul>
<li>Christoper M. Bishop, Pattern Recognition and Machine Learning, 2006</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-04-20&nbsp;<a class="git-hash" href="https://github.com/Han8931/commit/7b251f771bafef4b78fcca36c0ac7da6fa6ab581" target="_blank" title="commit by Han(tabularasa8931@gmail.com) 7b251f771bafef4b78fcca36c0ac7da6fa6ab581: Update">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>7b251f7</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/20240818_latent_variable_part1/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/latent-variable/">Latent Variable</a>,&nbsp;<a href="/tags/k-means-clustering/">K-Means Clustering</a>,&nbsp;<a href="/tags/clustering/">Clustering</a>,&nbsp;<a href="/tags/gmm/">Gmm</a>,&nbsp;<a href="/tags/gaussian-mixture-modeling/">Gaussian Mixture Modeling</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/20240815_svd/" class="prev" rel="prev" title="Gentle Introduction to Singular Value Decomposition"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Gentle Introduction to Singular Value Decomposition</a>
            <a href="/20240819_gradient_descent/" class="next" rel="next" title="Direction of Gradient Descent Update">Direction of Gradient Descent Update<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.147.0">Hugo</a> | Theme - <a href="https://github.com/Fastbyte01/KeepIt" target="_blank" rel="noopener noreffer" title="KeepIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> KeepIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2024 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Han</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"A9NUSQZEO5","algoliaIndex":"github","algoliaSearchKey":"e255482bc340762a0da27f50eddd2765","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
